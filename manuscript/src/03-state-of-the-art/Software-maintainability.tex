\section{Software Maintainability} \label{chapter3:software-maintainability}

\cit{It is becoming increasingly important to the data-processing industry to be able to produce more programming systems and produce them with fewer errors, at a faster rate, and in a way that modifications can be accomplished easily and quickly.}{W. P. Stevens, G. J. Myers, L. L. Constantine \cite{Stevens1974}}.

In order to improve and maintain a software system, it is important to holds in mind the mental representation behind its implementation.
Architects, and mechanical engineers draw codified plans to share their mental representations with peers and building teams.
software design is an exception in that the implementation is both the shared mental representation, and the actual product.
The mental representation is often lost in technical details and optimizations for the actual product.
This problem becomes even more critical as the system grows in size.
Therefore, it is crucial to decompose the system into smaller subsystem easier to grasp individually.
This section shows the theories, programming languages and frameworks helping this decomposition.

\subsection{Modularity} \label{chapter3:software-maintainability:modularity}

The modularity of a software implementation is about enclosing the subproblems and bringing the relevant interfaces to allow these part to be composed.
It allows greater design to emerge from the composition of smaller components.
Such modularity helps organizing the implementation to reflect the underlying mental organization.
The modularity in software design improves the maintainability of an implementation, as presented in the following schema.
It allows to limit the understanding required to contribute to a module \cite{Stevens1974}.
And it reduces development time by allowing several developers to simultaneously implement different modules \cite{Wong2009,Cataldo2006}.

\begin{center}
\includegraphics[width=0.6\textwidth]{../ressources/state-of-the-art-1.pdf}
\end{center}

The section \ref{chapter3:software-maintainability:modularity:design-choices} is about the decomposition of a problem into subproblems.
Then, the section \ref{chapter3:software-maintainability:modularity:programming-models} is about the bringing the interfaces allowing their composition.
Finally, the section \ref{chapter3:software-maintainability:modularity:limitations} presents the consequences of this decomposition on performance.


\subsubsection{Design Choices} \label{chapter3:software-maintainability:modularity:design-choices}

\nt{TODO this introduction is not very clear}
In the decomposition of a large problem into smaller subproblem, there is two design choices.
The first one is the granularity, and organization of the subproblems within the system decomposition.
The second one is the organization of the implementation within the subproblems to improve maintainability.

\paragraph{System Decomposition}

\illustration{spaghetti programming}

Dijkstra firstly developed the concept of Structured Programming \cite{Dijkstra1970}, which later led to modular programming.
% It is defined as \textit{the systematic use of abstraction to control a mass of details, and also a means of documentation which aids program design} \cite{Knuth1974}.
Structured programming is about drawing clear interfaces around a piece of implementation so that the execution is clearly enclosed inside.
At a fine level, it helps avoid spaghetti code \cite{Dijkstra1968a}, and at a coarser level, it structures the implementation \cite{Dijkstra1968} into modules, or layers.
The next paragraph explains further the criteria to draw the borders around modules.

\illustration{lasagna programming}

\paragraph{Decomposition Criteria}

The criteria to decompose the system into well defined modules are coupling and cohesion \cite{Stevens1974}.
The coupling defines the strength of the interdependence between modules, while cohesion defines how strongly the features inside a module are related.
Low coupling between modules and high cohesion inside modules helps logically organize, and understand the implementation.
Hence, it improve its maintainability.
The next paragraph present the approach to build module helping with the evolution of the implementation.

\paragraph{Development Evolution}

To improve maintainability of implementation, the modular organization should isolate the evolution of a module from impacting the rest of the implementation.
The Information Hiding Principle \cite{Parnas1972}, and the Separation of Concerns \cite{Tarr1999,Hursch1995} are two similar approach to do so.
The information hiding principle advocates to encapsulate a specific design choice in each module.
The Separation of Concerns advocates each module to be responsible for one and only one specific concern.
Examples of separation of concerns are the separation of the form and the content in HTML / CSS, or the OSI model for the network stack.

\subsubsection{Programming Models} \label{chapter3:software-design:programming-models}

% Programming languages are designed for developers to follow the best practices mentioned above.
Programming languages used in the industry were designed following programming models favoring the use of the best practices mentioned above.
This section presents two programming models : object oriented programming and functional programming.

\paragraph{Object Oriented Programming}

% The following list defines Object-Oriented Programming (OOP).
% \begin{enumerate}
% \item Everything is an object.
% \item Communication is performed by objects communicating with each other, requesting that objects perform actions. Objects communicate by sending and receiving messages. A message is a request for action, bundled with whatever objects may be necessary to complete the task.
% \item Objects have their own memory, which consists of other objects.
% \item Every object is an instance of a class. A class simply represents a grouping of similar objects, such as integers or lists.
% \item The class is the repository for behavior associated with an object. That is, all objects that are instances of the same class can perform the same actions.
% \end{enumerate}

Alan Kay, who coined the term, states that Object Oriented Programming (OOP) is about message-passing, encapsulation and late binding.
\nt{emphasize on message-passing, encapsulation and late binding}
(There is no academic reference for that, only a public mail exchange\ftnt{http://userpage.fu-berlin.de/~ram/pub/pub\_jf47ht81Ht/doc\_kay\_oop\_en}.)
This original definition is an evolution upon modular programming.
It helps encapsulate both the data, and the functions to process this data in an isolated, loosely coupled module.
The very first OOP language was Smalltalk \cite{Goldberg1984}.
It defined the core concept of OOP.
It is inspired by LISP and by the definition of the Actor Model, which we will define in the next section.

% Illustration of multiple cells, as Alan Kay thought of biology when developing the object-oriented concepts.
% http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en
% I thought of objects being like biological cells [...] able to communicate with messages ...

\nt{TODO don't be too categorical, just say that theorical OOP is different from industrial OOP in the implementation, or something like that}
Object-Oriented Programming abandoned late-binding and adopted a stricter approach with the concepts of class, inheritance and polymorphism.
The major languages of the software industry feature this stricter Object-Oriented approach.
We can cite C++ and Java as the emblematic figures of OOP \cite{Gosling2000,Stroustrup1986}.

\nt{TODO the next paragraph should be a transition to FP, not saying that OOP is bullshit}
Though, the field test seems to have had reason of this stricter version.
The trends in programming language seems to digress from the pure Object-Oriented approach to evolve toward a more dynamic approach, closer to Functional Programming.
Indeed Javascript, Ruby and Python adopt functional features such as dynamic typing and higher-order functions \cite{Ecma1999}\ftnt{https://www.ruby-lang.org/en/about/}.

% \paragraph{Object Calisthenics}

% Object calisthenics are defined as the chapter 6 of \textit{The Thoughtworks Anthology} \cite{Bay2008}.
% It is an exercise for developers presented as a list of 9 rules to follow to enforce maintainability and readability on source code \ftnt{http://www.cs.helsinki.fi/u/luontola/tdd-2009/ext/ObjectCalisthenics.pdf}.

% Some of these rules are direct implementations of the more general concept of separation of concerns, and information hiding.
% As an example, rule 7 \textit{Keep all entities small} advocate that entities should have a concise concern.
% Other rules are just syntactic guides to improve readability and comprehensibility.


% See Object calisthenics 
% - http://williamdurand.fr/2013/06/03/object-calisthenics/
% - http://www.cs.helsinki.fi/u/luontola/tdd-2009/ext/ObjectCalisthenics.pdf



\paragraph{Functional Programming} \label{chapter3:software-design:programming-models:functional-programming}

% \cit{All problems in computer science can be solved by another level of indirection}{Butler Lampson}

The formal definition of Functional Programming resides in manipulating only mathematical expressions - instead of operation statements - and forbidding state mutability.
However, the functional programming concepts implemented in programming language are more mitigated, and resides in higher-order functions and lazy evaluation.
Two features that major programming languages now commonly present.

\nt{TODO what about these}
Haskell \cite{Hudak1992}
Miranda \cite{Turner1986}
Standard ML \cite{Milner1997}
Scheme \cite{Rees1986}

Languages providing higher-order functions allows to manipulate functions like any other primary value : to store them in variables, or to pass them as arguments.
Higher-order functions replace the needs for most modern object oriented programming design patterns \ftnt{http://stackoverflow.com/a/5797892/933670}.

Most languages use closures to implement lexical scope with higher-order functions \cite{Sussman1998}.
A closure is the association of a function and the lexical context from its creation.
It allows this function to access variable from this context, even when invoked outside the scope of this context.
For example when passed as an argument to another module.

\nt{Transition to next two sections}
It loosen the couple between modules, and helps define more generic and reusable modules.
However, it increase their dependencies during the execution.
Indeed, by exchanging closures, two modules intricately share their contexts of execution.


+ A function is side-effect free if it is referentially transparent.


\subsubsection{Advantages of Functional Programming for Modularity}
\nt{Not a section, integrate with the previous section}

Higher-order functions and lazy evaluation help loosen the couple between modules, and improve their re-usability.
\textit{In fine}, it helps developers to write applications that are more maintainable, and favorable to evolution \cite{Hughes1989}, and \cite{Turner1981}.

\textbf{++}

\paragraph{Higher-Order Programming}

How to glue the modules together ?
Structured programming define the glue between the blocks of code.
A block can not be entered, or exited except at special interfaces that make the glue between the modules.

Similarly, higher-order programming provide new kind of glue between modules.



It allows Inversion of control

\textbf{++}


\paragraph{Lazy Evaluation}

And dataflow programming is similar to lazy evaluation.
Remember that lazy evaluation is only an extreme case of a buffered streaming execution.
See your notes on Computer Models and Techniques stuffs.

Lazy evaluation allows as well a new kind of glue between modules.

According to \cite{Hughes1989}, \textit{Abelson and Sussman stress that streams (lazy lists) are a powerful tool for structuring programs} \cite{Sussman1983}.

Lazy evaluation is somehow equivalent to stream, or data flow

Pipeline parallelism is relevant for multi-pass algorithms \cite{Conway1963}, and it is particularly efficient for stream processing applications as we saw in section \ref{chapter3:software-efficiency:dataflow-pipeline}.
For these applications, sequentiality is no longer relevant, as the different stages of the execution are repeated for each message in the stream.
Only causality is necessary, and it opens a possible pipeline parallelism.

\textbf{++}

\subsubsection{Performance Limitations} \ref{chapter3:software-maintainability:modularity:limitations}

Functional programming greatly improves the resilience of implementation to the evolution of their specification.
However, it requires a global memory to share the context of execution among modules.
As shown in the previous chapter, sharing memory makes parallelism difficult.
At the regard of this insight, the concern of evolution and the concern of performance seem hardly compatible.
This section explore in further details the limitation of functional programming and so on.

Mutability vs Immutability ?

-> Mutability should win.

With mutability and higher-order, you get closures.
With closures, you get a very tangled web of mutable memory references.
It the worst when you need to parallelize.
For example, some argued to remove the mutability in memory references \cite{Gordon2012} \cite{Matsakis2012a}.

Higher-order programming is very good for modularity, but it inherently blocks the parallelization of the memory.


Immutability is the ... ? \cite{Gordon2012}

Isolation > Immutability > Synchronization
http://joeduffyblog.com/2010/07/11/thoughts-on-immutability-and-concurrency/
This scale is explained in the section concurrent computing

\textbf{++}

Amdahl \cite{Amdahl1967} and later Ghunter \cite{Gunther1993} theorized the speedup gains with parallelism for a sequential program.
They concludes that sharing resources protected by mutual exclusion eventually decreases performances when increasing parallelism \cite{Gustafson1988,Gunther1996,Nelson1996,Gunther2002}.
The concurrent process sharing resources need to be scheduled sequentially, and not in parallel, as the contention of locking negatively impact the performance.
To increase the parallelism and performance, it implies to reduce the shared resources between concurrent processes.
\nt{TODO link the previous paragraph with the isolation / immutability / synchronization scale in concurrent programming}
% The execution regions requiring the same resource needs to execute sequentially.
% This wait impacts performances negatively because of contention.
% Therefore, to increase parallelism one needs to increase the number of independent processes, and to ensure their communicate to be solely by asynchronous messages without waiting.

In this article, D. Parnas opposes the organization of modules following the information hiding principle from the one following a pipeline approach to parallelize the execution.
The former organization supports the development evolution, while the latter is more favorable to the performance of parallel execution.
This opposition shows that a program cannot trivially follow an organization that support both development evolution, and performance.
However, D. Parnas advocates the use of an assembler to conciliate the two approaches.

\paragraph{}

The next section shows improvement for performance and parallelism.
Section \ref{chapter3:software-performance} shows the techniques for parallelism.

\subsection{Performance Improvements}

\begin{center}
\includegraphics[width=0.6\textwidth]{../ressources/state-of-the-art-2.pdf}
\end{center}

\subsubsection{Concurrent Programming}

\cit{TODO Building concurrent programming is like building a steam engine through a keyhole}

Another approach is to provide directly concurrent mechanisms to the developer.
\comment{TODO Event-loop / event-based programming / concurrent programming / multi-thread}

+ Fibers \cite{Adya2002} + Capricio - User cooperative threads (also known as fibers / green threads)
+ Vert.X - node like + thread / worker capabilities

+ TAME - event-based solution without stack ripping in C (it is like closure, but for C)
    + AMPED Asymetric multi-process event-driven
    +> Flash
    +> Nginx
    +> Ninja

- Continuation-passing style parallelization
  - The interprocedural analysis and automatic parallelization of Scheme programs - http://link.springer.com/article/10.1007/BF01808954#page-1


+ Combining events and threads for scalable network services, Linguistic symbiosis between event loop actors and threads, Combining data flow and control flow computing)


Design methodology:
+> Leda - PCAM
+ PCAM - Partition -> Communicate -> Agglomerate -> Map

Again, isolation > immutability > synchronization
Isolation is independent processes
Immutability is event-loop ? (or is it synchronization ?)
Synchronization is multi-thread, lock-free

+ include alternative programming models CUDA / OpenCL + DSMS
  + OpenMP
  + OpenCL
  + CUDA
  + Cg: A system for programming graphics hardware in a C-like language
  + Brook for GPUs: Stream Computing on Graphics Hardware
  + Liquid Metal by IBM - unified language for FPGAs, GPUs and such ...

  + Commutativity analysis: a new analysis technique for parallelizing compilers
  + the scalable commutativity rule ...
  + Making asynchronous parallelism safe for the world


It is easy to understand the parallelism in a cooking recipe because the interdependencies between operations are trivial.
It seems obvious that melting chocolate is independent from whipping up egg whites.
% Because chocolate and egg whites are different ingredients.
This distinction between chocolate and egg whites is trivial.
% ... comes from the modifications to the state.
While the distinctions within the state of an application are more intricate.
This makes concurrent application more difficult to design and implement.

\subparagraph{State Coordination}

% The interdependencies between the tasks impose the coordination of the global application state.
The global state of an application impose the coordination between the tasks.
This coordination happens either by sending messages, or by modifying a shared memory.
\nt{The following sentence needs to be rewritten to include both message passing and shared memory. Because state coordination limits parallelism, and scalability, it is a better solution to use message passing}
If the tasks are independent enough, the coordinations can be done with message passing.
Each task sends messages to indicate the modifications of the state with consequences outside its scope.
% They pass the states from one task to another so as to always have an exclusive access on the state.
% As example, applications built around a pipeline architecture define independent tasks arranged to be executed one after the other.
% The tasks pass the result of their computation to the next.
% These tasks never share a state.
However, if the tasks are too dependent, the overhead of message passing tends to impact performances.
% If the tasks need concurrent accesses to a state, they cannot efficiently pass the state from one to the other repeatedly.
They need to share and coordinate their accesses to the state.
Each access needs to be exclusive to avoid corruption.
I address in the next paragraphs the different scheduling strategies, and how they assure this exclusivity.

\subparagraph{Task Scheduling}

There are two scheduling strategies to execute tasks sequentially on a single processing unit : preemptive scheduling and cooperative scheduling.
The coordination is different with the two scheduling strategies.

\illustration{feu rouge et rond point}

Preemptive scheduling is used to assure fairness between the tasks, such as in a multi-tasking operating system.
% in most execution environment in conjunction with multi-threading.
The scheduler allows a limited time of execution for each task, before preempting it.
% It is a fair and pessimistic scheduling, as it grant the same amount of computing time to each task.
However, as the preemption happens unexpectedly, the developer needs to assure exclusivity by locking the shared state before access.
Locking is known to be hard to manage by developers, and to impact performances negatively.
Because it is not ideal both for development scalability and performance scalability, it is set aside for the remaining of this chapter.
% This scheduling strategy should be avoided except when true concurrency is needed in concert with true shared state.
% Shared state could probably always be emulated with isolated memory and message passing.

On the other hand, in cooperative scheduling, a task is allowed to run until it yields the execution back to the scheduler.
Each task is an atomic execution : it has an exclusive access on the memory.
% It gives back to the developer the control over the preemption.
As the developer doesn't need to explicitly assure exclusivity, it is easier to write concurrent programs efficiently with this scheduling strategy.
% Indeed, I presented in the previous section the popularity of Javascript, which is often implemented on top of this scheduling strategy (DOM, Node.js).

\subparagraph{Invariance}

\nt{TODO This section is not clear}

% The challenge introduced above is to assure to the developer an exclusive access to the state of its application.
In concurrent computation, it is important to assure the invariance of a state during its manipulation.
This assurance is given by the exclusive access of an atomic execution on the state.
% I call invariance the assurance given that the state accessible from a task will remain unchanged during its access to avoid corruption, and more generally to allow the developer to perform atomic modifications on the state.
It allows the developer to group operations inside this atomic execution, so as to avoid corruption of the state.
% so as to perform all the operations without interference from concurrent executions.
% The same concept is found in transactional memory.

When the tasks remains isolated and communicate by message passing, there is no risk of corrupted state.
The invariance is assured by the isolation of the state specified by the developer, and by the atomicity of the message processing.
On the other hand, in a cooperative scheduling application, each task is atomic, so the developer always has an exclusive access to the global state.
This atomicity assures the invariance.
% The invariance is assured by the atomicity of each task.
% , because any region in the memory can be accessed only by one task at a time.

% Between these two invariances, the locking mechanisms seems to be a promising compromise.
% The developer defines only the shared states, and these are locked only when needed.
% However, it increases the complexity of the possible locked combination, leading to unpredictable situations, such as deadlock, and so on.
% The locking mechanisms are known to be difficult to manage, and sub-optimal.
% Indeed, they are eventually as efficient as a queue to share resources.

% For the rest of this thesis, I focus only on the invariances provided by the multi-process paradigm and the cooperative scheduling.
% They two invariance are similar, because the developer defines sequence of instructions with atomic access to the memory.
% And in both paradigms, these sequences communicate by sending messages to each other.
The difference is that in the message passing paradigm, the developer defines the state isolation inducing the execution isolation, while with cooperative scheduling, the developer defines only the execution isolation.
% This difference seems to be crucial in the adoption by the developer community.
It is difficult for developer to isolate state, but it provides good performances through parallelism.
While it is easier to assure atomic execution with cooperative scheduling, but it is unable to provide parallelism.
On one hand, the state is distributed and isolated to improve performance scalability.
On the other hand, the code is organized logically to improve maintainability.
The impact of these two organizations on performance scalability and development scalability is at the heart of this thesis.


\paragraph{Synchronization}

\nt{Too raw, not enough synthesis in this section}

+ The purpose of explicit synchronization is to manage the timing of side-effects in the presence of parallelism. 

- transactional memory
  - http://www.morganclaypool.com/doi/abs/10.2200/s00272ed1v01y201006cac011
  - http://delivery.acm.org/10.1145/1370000/1364800/p80-larus.pdf?ip=160.92.8.31&id=1364800&acc=OPEN&key=4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35.6D218144511F3437&CFID=529578904&CFTOKEN=25104956&__acm__=1437488908_6eeb5d92ab07593b1aa92d3ac4461c0b
  - ...

These programming models allowed parallel execution on several processing units, so there is a need to shared resources among processing units, like a common memory store, or network interface.
Multiprogramming was used to allow different programs to be executed concurrently in isolated processes, and to share resources \cite{Dijkstra1968}.
To synchronize the different processes over these resources, and avoid conflicting accesses, it is crucial to assure the mutual exclusion.
For this purpose, Djikstra introduced the Semaphore \cite{Dijkstra}.
Similar works include guarded commands \cite{Dijkstra1975}, guarded region \cite{Hansen1978a} and monitors \cite{Hoare1974}.
They are all kinds of locks to assure mutual exclusion.

% Following this work, he also introduced guarded commands \cite{Dijkstra1975} and Hansen introduced guarded region \cite{Hansen1978a}.
% Both assure the execution of a set of instructions to be exclusive to only one process.

% Hoare introduced the monitor following the work of Hansen \cite{Hoare1974}.
% A monitor is an extension of a class, it regroups data and procedures, except that it assures its procedures to be entered only once at a time.
% With this restrictions, it guards against race condition on the access of a shared resource.
% Modula \cite{Wirth1977} and Concurrent Pascal \cite{Hansen1975} uses Monitors.

\subparagraph{Multi-Threading}

As we saw earlier, a common memory storage helps to follow the best practice, and is easier to develop with.
These lock mechanisms were used in Multi-Threading to provide this common memory storage for concurrent programming.
% Multi-threading programming make use of synchronization within isolated processes.
Threads are light processes sharing the same memory execution context within an isolated process.
It seems to be an easy solution to parallelize sequential execution on parallel execution units with a common memory store.
But because of the preemptive scheduling, threads require to lock each and every shared memory cell.
It is known that this heavy need for synchronization leads to bad performances, and is difficult to develop with \cite{Adya2002}.

\subparagraph{Lock-Free Data-Structures}

An interesting alternative to locks are the wait-free and lock-free data-structures \cite{Lamport1977,Herlihy1988,Herlihy1990,Herlihy1991,Anderson1990}.
They are based on clever use of atomic read and write operations on a shared memory to provide concurrent safe version of common data-structures algorithms.
Therefore no locking is necessary for the algorithm to be highly concurrent, while conserving a common memory store
However, even if they are theoretically infinitely scalable, they are hard to come with, and are not fit for every problem.
% Lock-free algorithm are highly concurrent, as they can be replicated, however, they are limited, and really hard to develop.
% \url{https://en.wikipedia.org/wiki/Non-blocking_algorithm}

% Reference papers :
% Concurrent reading and writing \cite{Lamport1977}
% Impossibility and universality results for wait-free synchronization \cite{Herlihy1988}
% A methodology for implementing highly concurrent data structures \cite{Herlihy1990}
% Wait-free synchronization \cite{Herlihy1991}

% Book :
% The virtue of Patience: Concurrent Programming With And Without Waiting \cite{Anderson1990}


\subparagraph{Data-stream management systems}

% The processing of large volume of data was historically handled by Database management systems.
% These systems naturally evolved to manage data-streams as well.

Database Management Systems (DBMS) historically processed large volume of data, and they naturally evolved into Data-stream Management System (DSMS) to processed data streams as well.
They concurrently run SQL-like requests on continuous data streams.
The computation of these requests spread over a distributed architecture.
Among the early works, we can cite NiagaraCQ \cite{Chen2000,Naughton2001}, Aurora \cite{Abadi2003,Abadi2003a,Balakrishnan2004} which evolved into Borealis \cite{Abadi2005}, AQuery \cite{Lerner2003}, STREAM \cite{Arasu2003,Arasu2005} and TelegraphCQ \cite{Krishnamurthy2003,Chandrasekaran2003}.
More recently, we can cite DryadLINQ \cite{Isard2007,Yu2009}, Timestream \cite{Qian2013} and Shark \cite{Xin2013}.

However, these solutions implies to understand two paradigms of language, the SQL paradigm, and the imperative paradigm.
% Even if SQL is a turing-complete language, it is rather difficult to write a complex application only with a SQL-like language.
The difference between these two paradigms creates a rupture in the design of the system.
% Even if the design follows an imperative structure,
The SQL parts difficulty merge with the imperative structure.
% It is harder to separate concerns. \nt{need reference, plus rewrite this paragraph}.
This rupture impacts the maintainability of the system as it is not straightforward to reorganize the logic between the two paradigms.


  + Grape / Timestream - distributed SQL (roughly)
  + CQL
  +> STREAM (uses CQL)
  + StreaQuel
  +> TelegraphCQ
  + AQuery

% SQL-like
%   AQuery \cite{Lerner2003}
%   STREAM (uses CQL) \cite{Arasu2003,Arasu2005}
%   TelegraphCQ (uses StreaQuel) \cite{Krishnamurthy2003,Chandrasekaran2003}
%   Grape / Timestream - distributed SQL (roughly) \cite{Qian2013}
%   Shark        Stateless dataflow \cite{Xin2013}

%   DryadLINQ    Stateless dataflow \cite{Isard2007,Yu2009}

% \subsubsection{Batched dataflow}

% Map/Reduce
%   MapReduce    Stateless dataflow \cite{Dean2008}
%   Hadoop       Stateless dataflow 
%   Incoop       Incremental dataflow \cite{Bhatotia2011}

% Functional
%   Comet        Batched dataflow \cite{He2010}
%   D-Streams    Batched dataflow \cite{Zaharia2012}
%   Spark        Stateless dataflow \cite{Zaharia,Zaharia2010}
%   Nectar       Incremental dataflow \cite{Gunda2010}




\subsubsection{Compilation}

\nt{The following paragraph is not very clear.}
% Instead of trying to find a fitting between the two organization,
Another approach to conciliate performance and maintainability, is to transform the source from one organization into the other.
\textit{It is a mistake to attempt high concurrency without help from the compiler} \cite{Behren2003}.
When showing the incompatibility between the two organization, D. Parnas already advocated conciling the two methods using an assembler to transform the development organization into the execution organization \cite{Parnas1972}.
I present in the this section the state of the art in compilation-based parallelization.

% Imperative Stream Processing
%   Piccolo                                  Parallel in-memory \cite{Power2010}
%   CIEL                                    Stateless dataflow \cite{Murray2011}
%   Statdeful Dataflow Graph (SDG)          Stateful dataflow  \cite{Fernandez2014a}

\paragraph{Parallelism Extraction}

Generally, there is three type of parallelism, data, task and pipeline parallelism.
Some works explore the extraction of the three types indistincly \cite{Li2012}.
Other works focused on the task parallelism \cite{Rinard1996}.
However, huge works has been done on the data parallelization, to parallelize the loops inside a sequential program \cite{Mauras1989,Amarasinghe1995,Yuki2013,Banerjee2013,Radoi2014}

Indeed, the loops represent most of the execution time in scientific applications, so an important speedup is expected from this data parallelization.
\nt{As said in limitation : isolation > immutability > synchronization}

C. Hermann proposed the parallelization of loop in a functional language with higher-order programming and immutable data \cite{Herrmann2000}.
\nt{And so what ?}

However, there is few works to parallelize higher-order programming languages, with mutable data.
\nt{}
Closures often complicates the dependencies between iterations.

To conserve higher-order programming, N. Matsakis proposed to forbid the mutation of the parent closure of a loop, so that the iterations can be executed in parallel while accessing the immutable closure\cite{Matsakis2012a}.

\paragraph{}

\nt{For next paragraph, see causality in next section}
All these approaches are based on synchronous execution and Amdahl's law states that even if a slight portion of execution is sequential, the expected speedup is limited \cite{Amdahl1967,Clements2013a}.
Another approach to break free from the sequential structure is to split the sequential execution into following, parallelizable tasks to form a pipeline \cite{Kamruzzaman2013,Fernandez2014a}.
This thesis focus solely on pipeline parallelism.

\paragraph{Static analysis}

In order to extract parallelism, compilers analyze the source code of applications.
The compiler analyzes the control flow to detect the dependencies between statements to parallelize them.
As these dependencies are linked to memory access, it is important to have a good memory representation.
The point-to analysis, presented by L. Andersen \cite{Andersen1994} is a common approach to extract the memory representation.
It analyzes the modification of pointers through the control flow, to help extract properties from programs.
It is used in security to assure the safeness of an implementation, for example in Javascript \cite{Chudnov2015}.
% However, these techniques are not precise enough to rely only on them for the parallelization.

\paragraph{Annotations}

Extracting parallel dataflow from an imperative, sequential implementation is a hard problem \cite{Johnston2004a}.
Some works proposed to rely on annotations from the developer to help the extraction \cite{Vandierendonck2010a,Fernandez2014a}.
% Some works asked the developers to annotate their code so as help the compiler extract parallelism
% It is an intermediate solution with the solution presented in the previous section.
However, it still requires developers indicate the independence of the memory or the execution.
In this regard, this solution is similar to concurrent programming present in \ref{chapter3:concurrent-programming}, and are unable to fix the rupture between performance and maintainability.

All the solution presented throughout this chapter are elitist, as they tend to rely on the developer to reconcile the two organizations.
They are not satisfactory as they are too hardly accessible for most developers.
It finally results in frail implementations, that require great efforts of development to assure their performance in the first place, and then to maintain.



\nt{include the following sentence}
CUDA, OpenCL are data parallel API to allow imperative code to run onto accelerators such as GPUs or FPGAs \cite{Stone2010}.



\subsubsection{Limitations on Accessibility}

Concurrent programming is difficult.
Compilation is not yet solid enough.

\paragraph{Transition on parallel programming}

The definition of separation of concerns given in this section is orthogonal to the original meaning coined by Dijkstra \cite{Dijkstra1982}.
It is interesting to note this difference, as it is related directly to this thesis.
% Initially, it meant the ability to reason independently about different concern about a software system.
The initial definition was about analyzing independently how a system meets different concerns.
Dijkstra gives the example of analyzing independently correctness and efficiency.
It is impossible to encapsulate correctness, or efficiency in a module, they concern the whole system.
In this respect, this thesis is oriented towards dissociating the concern of development evolution and of performance.
That is to be able to reason on the maintainability of a program, independently than of its performance, and vice versa.
% This seems challenging as D. Parnas opposed these two concerns.
It is the challenge presented by D. Parnas when he opposed the two concerns in \cite{Parnas1972}.

This thesis investigates further this opposition to dissociate the concern of evolution and the concern of performance in the case of a web application.
The next section investigates the first concern, and presents the major programming models used to improve the evolution of an application.

\endinput

\subsubsection{Modularity based on Design Decisions}

Designing Software for ease of extension and contraction \cite{Parnas1979}

Design Rules: The Power of Modularity Volume 1 \cite{Baldwin1999}
A reference book, but I can't get it.


Promises 
\cite{Liskov1988}


What makes a great software engineer? \cite{Li2015}

About great software development:
Productivity : Sackman et. al 68, Gugerty & Olson 86
Collaboration, meaningful contribution : Kelly 99, Begel & Simon 06, Hewner & Guzdial 10
Communicate and acquire understanding : LaToza 06, Ko 06
Technical Knowledge : 
Open minded : McConnell 04, Bryant 13




Modularity :
- encapsulation : a module contains the data, as well as the functions to manipulate this data
- separation of concerns : each module should have a clear scope of action, and this scope should not overlap with the scope of other modules
- loose coupling : each module should require no, or as little knowledge as possible about the definition of other modules






Continuations and coroutines \cite{Haynes1984}
-> THIS

Parallel closures, a new twist on an old idea \cite{Matsakis2012a}

Continuation of work on SEDA \cite{Salmito2014}



From control flow to dataflow \cite{Beck1991}


-> THIS, to read
Automatic Extraction of Coarse-Grained Data-Flow Threads from Imperative Programs \cite{Li2012}
In this paper all parallelism are extracted (data, task and pipeline).


Commutativity analysis: A new analysis framework for parallelizing compilers \cite{Rinard1996}
In this paper, they analyze commutative operations to parallelize them.
It is novel because it isn't about parallelizing loops.
However, it is not exactly pipeline parallelism either.


Interesting articles :

http://comjnl.oxfordjournals.org/content/early/2015/09/15/comjnl.bxv077.abstract


-> THIS, to read
Load balanced pipeline parallelism \cite{Kamruzzaman2013}


??? The Paralax infrastructure \cite{Vandierendonck2010a}

??? Blazes: Coordination analysis for distributed programs \cite{Alvaro2014}

Making state explicit ... \cite{Fernandez2014a}
http://2015.splashcon.org/event/splash2015-splash-i-lindsey-kuper-talk


Introducing 'Bones': a parallelizing source-to-source compiler based on algorithmic skeletons \cite{Nugteren2012}


Recent paper about Javascrypt static analysis \cite{Chudnov2015}



Loop nesting optimization
- systolic arrays
- polyhedral compilers
- Simplifying reductions 


\cite{Mendis2015}