\section{Parallel execution} \label{chapter3:parallel-execution}

Programming started with a very sequential nature, as we saw in the last chapter.
But it eventually evolved toward concurrency to make advantage of the parallel architecture.

The first models of computation, like the Turing machine and lambda-calculus, were inherently sequential and based on a global state.
A formalism was lacking to represent concurrent computations.
We present the most important works on formalisms for parallel computation.
They first tackled the problems of determinacy, communication and state synchronization.
The answer to this problems seems to lie in a formalism based on a network of concurrent processes, asynchronously communicating via messages.
We present the works on the programming models based on this formalism.
Recently, with the need of performance from the web to process stream of requests, we see huge improvements in the field of distributed stream processing.

\subsection{Concurrency Theory} \label{chapter3:parallel-execution:concurrency-theory}

The mathematical models are a ground for all following work on concurrent programming, we briefly explain them in the next paragraphs.
There is two main formal models for concurrent computations.
The Actor Model of C. Hewitt, the Pi-calculus of R. Milner and the Communicating Sequential Processes of T. Hoare.
Based on these definition, we explain the importance of determinism, and the reason that asynchronous message-passing prevailed.


% TODO illustration of cells, and draw an analogy between cells and actor model.
% Or something the actor models is based upon.

\subsubsection{Models}

\paragraph{Actor Model}

The Actor model allows to express the computation as a set of communicating actors \cite{Hewitt1973a, Hewitt1977, Clinger1981}.
In reaction to a received message, an actor can create actors, send messages, and choose how to respond to the next message.
All actors are executed concurrently, and communicate asynchronously.

The Actor model was presented as a highly parallel programming model, but intended for Artificial Intelligence purposes.
Its success spread way out of this first scope, and it became a general reference on message passing parallel programming.
For example, the Scala language advertises its use of an actor approach of concurrency.

% More recent work of C. Hewitt on Actors is about ... \nt{TODO} \cite{Hewitt2007,Hewitt2007a}.

The Actor model uses an asynchronous message-passing communication paradigm.
The communication between two actors, the sender and the receiver, is a stream of discrete messages.
% The sender names the receiver actor when sending messages to be the recipient of these messages.
It is asynchronous because, contrary to invocation, the sender doesn't wait for the result of the initiated communication.

\paragraph{$\Pi$-calculus}

R. Milner presented a process calculus to describe concurrent computation : the Calculus of Communicating Systems (CCS) \cite{Milner1975, Milner1980}.
It is an algebraic notation to express identified processes communicating through synchronous labeled channels.
% In CCS, process compose concurrently, communications are synchronous, and the topology is static.
The Pi-calculus improved upon this earlier work to allow processes to be communicated as values, hence to become mobile \cite{Engberg1986,Milner1992a,Milner1992}.
Similarly to Actors, in Pi-calculus processes can dynamically modify the topology.
However, contrary to the Actor model, communications in Pi-calculus are based on simultaneous execution of complementary actions, they are synchronous.


% Actors can create actors, pi-caclulys processes can replicate, and send processes through channel.
% Processes create a new processes on each instruction to continue the execution.

% Pi-calculus resembles to the actor model, but its algebraic nature led to a critical difference with the latter.
% Indeed, processes in the Pi-calculus communicate indirectly, through labeled ports, whereas actors communicate directly by naming the recipient actors.
% This difference allows multiple processes to listen in turns to the same channel, whereas the recipient of a message cannot change.

% I think this difference lead the Pi-calculus to be composable, whereas message-passing is not.
% Message-passing is not composable, whereas invocation is.
% The Actor model is not an ideal programming model, as non-composability makes difficult to reuse or extends existing components.
% A way to compose actors, is to send to an actor the name of the actor to respond to.
% It is similar in essence to the continuation concept.



\subsubsection{Determinism and Non-determinism}

% All these early work adopted concurrent composition by default, instead of sequential composition, to adapt to the very concurrent nature of real parallel machines.
% However, sequential programming is still the default.
% Concurrent composition is yet still to be widely accepted, as stated by Reed \cite{Reed2012}.
% \comment{TODO rewrite this paragraph}

The Actor Model uses asynchronous communications, while $\pi$-calculus uses synchronous communications.
Synchronous communications are deterministic.
The message sent needs to be received to continue the execution on both ends.
Because the concurrent executions and the communications in such system are both deterministic, the result of the concurrent system is assured to be deterministic.
Determinism is a wanted property to assure the correctness of the execution.

On the other hand, asynchronous communications are non-deterministic.
The message sent can take an infinite time to be received.
Therefore, the result of the concurrent system is not assured to be deterministic.

But the communication in reality are subject to various fault and attacks, called Byzantin fault \cite{Lamport1982}.
It makes the real communications means unable to provide the determinism required by the deterministic models.
The Actor model, on the other hands, was explicitly designed to take physical limitations in account \cite{Hewitt1977a}.
Eventually, All these works evolved to adopt asynchronous communications.
Indeed, it is not realistic to build a distributed system based on synchronous communications.

Moreover, the total ordering of messages is only local to an actor, while between actors, messages are causally ordered.
As Lamport showed \cite{Lamport1978}, and Reed related later \cite{Reed2012}, causal order is sufficient to build a correct distributed system.
The non-determinism in communications is hidden by the organization of the system.
The execution will either terminate correctly, or not terminate at all.




% Asynchronous communications are less expressive than synchronous ones \cite{PALAMIDESSI2003}.

% Pi-calculus is a synchronous paradigm which contains an asynchronous fragment.\cite{PALAMIDESSI2003}
% (Boudol, G. (1992). Asynchrony and the π-calculus (note). Rapport de Recherche  1702, INRIA, Sophia-Antipolis,
% Honda, K. and Tokoro, M. (1991).  An object calculus for asynchronous communication. In America, P., editor, Proceedings of the European Conference on Object-Oriented Programming (ECOOP), volume 512 of Lecture Notes in Computer Science, pages 133–147. Springer-Verlag)


% The asynchronous pi-calculus defined by Honda and Tokoro in 1991 led to Pict, a programming language\cite{Pierce2000}.




% There was firstly theories, and models for concurrent computation.
% The main problem was determinism.
% In a sequential machine, the non-determinism of the physical world is hidden by the sequentiality of the machine.
% However, in concurrent computation, the order of communication cannot be assured the way the order of statements is assured in a sequential machine.
% We observe local non-determinism.
% However, to conserve an apparent determinism, causal ordering is sufficient.



\subsection{Message passing concurrency} \label{chapter3:parallel-execution:message-passing}


The theory advocates asynchronous message-passing, but it doesn't precise the granularity of the actors.
In the Actor Model, everything is an actor, even the simplest types, like numbers.
In practice this level of asynchronous communication is unachievable due to overhead.
In practice, most implementations feature independent synchronous processes communicating by messages.

\subsubsection{Programming models}

Conway defines coroutines as an autonomous program which communicate with adjacent modules as if they were input and output subroutines.\cite{Conway1963}
It seems to be the first definition of a parallel pipeline.
\textit{When coroutines A and B are connected so that A sends items to B, B runs for a while until it encounters a a read command, which means it needs something from A. The control is then transfered to A until it wants to write, whereupon the control is returned to B at the point where it left off.}

Hoare presented the Communicating Sequential Processes (CSP) \cite{Hoare1978, Brookes1984}.
These processes are executed concurrently, and communicates events via named channels.
The evolutions of this model were influenced by, and influenced the work of Milner that led to $\pi$-calculus.

Similarly, Kahn developed the Kahn Networks \cite{Kahn1974, Kahn1976}, following the work of Conway on coroutines.
They are explicitly parallel coroutines separated by bounded FIFO streams for communication.

These programming models are highly similar, and differs only in details irrelevant for this thesis.
However, it is interesting to note that they don't correctly inherit from the Actor Model, as it is generally impossible to dynamically modify the topology of the application.
Coroutines and processes are defined statically in the source of the application.
We shall come back to this limitation later.

\subsubsection{Scalability law}

These programming model were applied to run programs concurrently in machines providing a single processor, or shared resources among processors, like a common memory store, or network interface.
To manage these resources, and avoid conflicting accesses, it is crucial to assure the mutual exclusion.
For this purpose, Djikstra introduced the Semaphore \cite{Dijkstra}.

Following this work, he also introduced guarded commands \cite{Dijkstra1975} and Hansen introduced guarded region \cite{Hansen1978a}.
Both assure the execution of a set of instructions to be exclusive to only one process.

Hoare introduced the monitor following the work of Hansen \cite{Hoare1974}.
A monitor is an extension of a class, it regroups data and procedures, except that it assures its procedures to be entered only once at a time.
With this restrictions, it guards against race condition on the access of a shared resource.
Modula \cite{Wirth1977} and Concurrent Pascal \cite{Hansen1975} uses Monitors.

\paragraph{Multi-threading}

Multi-threading programming extensively uses these concepts, because of the preemptive scheduling and the common storage.
When executed in a parallel architecture, we say it follows the Multiple Instruction Multiple Data (MIMD) paradigm.
Preemptive scheduling is known to lead to bad performances, and difficulties in the development \cite{Adya2002}.
\nt{This paragraph is too short, and not well structured. Introduce Fibers ?}

\paragraph{PGAS}

Another approach to parallelism is the Single Program Multiple Data (SPMD) paradigm.
As an example, is the Partitioned Global Address Space (PGAS) model.
It has the advantage of providing to the developer a uniform access to a distributed memory space.
Each computing node executes the same program, and provide its local memory to be shared with all the other nodes.
The PGAS programming model assure the remote accesses and synchronization of memory across nodes, and enforces locality of reference, to reduce the communication overhead.
% This model is a SPMD : Single Program Multiple Data.
Known implementation of the PGAS model are 
Chapel\cite{Chamberlain2007},
X10 \cite{Charles2005}.
Unified Parallel C \cite{El-Ghazawi2006},
CoArray Fortran \cite{Numrich1998},
OpenSHMEM \cite{Chapman2010}.

Amdahl warned against sequential portion of a program impacting the performance gained with parallelism \cite{Amdahl1967}.
Ghunter extended Amdahl's law to show that sharing resources, even protected, leads to bad, and even decreases performances with increasing parallelism \cite{Gustafson1988,Gunther1993,Gunther1996,Nelson1996,Gunther2002}.
If portions of program in Multi-threading and PGAS are sequential, that is, if they share resources, it implies to defer the execution until the resource becomes available.
This wait impacts performances negatively.
Therefore, isolating the parallel processes in such way to limit the communication to the minimum is a better way to achieve efficient parallelism.
It is important the processes to be independent, and communicate solely by messages.


\subsubsection{Programming languages}

% Scala / Akka / Erlang

Some programming languages features message-passing and isolation of actors directly.
To some extent, these languages succeeded in industrial contexts.
However, they largely remain elitist solutions for specific problems more than a general, and accessible tool.

Scala is an attempt at unifying the object model and functional programming \cite{Odersky2004}.
It proposes an actor approach in its design.
Akka\ftnt{http://akka.io/} is a framework based on Scala, to build higly scalable and resilient applications.

Erlang is a functional concurrent language designed by Ericsson to operate telecommunication devices \cite{JoeArmstrong}.
\nt{TODO extends these paragraphs}

\subsection{Stream Processing} \label{chapter3:parallel-execution:stream-processing}

All the solutions previously presented are generally designed to build distributed systems.
We focus on real-time applications as defined by \cite{Hansen1978}. Such applications nowadays process high volumes streams of requests, and are web often oriented.

\textit{
From a language designer's point of view, real-time
programs have these characteristics:
\begin{enumerate}
\item A real-time program interacts with an environ-
ment in which many things happen simultaneously at
high speeds.
\item A real-time program must respond to a variety
of nondeterministic requests from its environment. The
program cannot predict the order in which these requests
will be made but must respond to them within certain
time limits. Otherwise, input data may be lost or output
data may lose their significance.
\item A real-time program controls a computer with a
fixed configuration of processors and peripherals and
performs (in most cases) a fLxed number of concurrent
tasks in its environment.
\item A real-time program never terminates but contin-
ues to serve its environment as long as the computer
works. (The occasional need to stop a real-time program,
say at the end of an experiment, can be handled by ad
hoc mechanisms, such as turning the machine off or
loading another program into it.)
\end{enumerate}
}







SEDA \cite{Welsh2001}

StreaMIT \cite{Thies2002}

SQL-like
  Grape / Timestream - distributed SQL (roughly) \cite{Qian2013}
  CQL \cite{Arasu2005}
  STREAM (uses CQL) \cite{Arasu2003}
  StreaQuel is the language implemented in Telegraph CQ 
  TelegraphCQ \cite{Krishnamurthy2003,Chandrasekaran2003}
  AQuery \cite{Lerner2003}

Map/Reduce
  MapReduce    Stateless dataflow \cite{Dean2008}
  Hadoop       Stateless dataflow 
  Incoop       Incremental dataflow \cite{Bhatotia2011}

Functional
  DryadLINQ    Stateless dataflow \cite{Isard2007,Yu2009}
  Spark        Stateless dataflow \cite{Zaharia,Zaharia2010}
  Nectar       Incremental dataflow \cite{Gunda2010}
  Comet        Batched dataflow \cite{He2010}
  D-Streams    Batched dataflow \cite{Zaharia2012}

Dataflow
  CBP          Incremental dataflow \cite{Logothetis2010}
  Naiad        Batched dataflow \cite{Murray2013}
  Storm        Continuous dataflow \cite{Toshniwal2014}
  S4           Continuous dataflow \cite{Neumeyer2010}
  SEEP         Continuous dataflow \cite{Fernandez2013}

Imperative
  CIEL         Stateless dataflow \cite{Murray2011}
  SDG          Stateful dataflow 
  Piccolo      Parallel in-memory \cite{Power2010}



\newcommand{\SPEentry}[4]{%
  \textbf{#1} \newline%
  #2 &%
  #3 &%
  #4 \\%
  \\%
}

\begin{center}
  \begin{longtable}{p{7cm} l p{8cm}}
    Nom & Année & Description \\
    \hline

    \SPEentry{NiagaraCQ
    }{        
    }{         2000
    }{        Un des plus ancien projet (à ma connaissance), Continuous Query (CQ), et scalable (à quel point, je ne sais pas)
    }

    \SPEentry{Aurora
    }{          Data Stream Management System
    }{          2003
    }{          Permet de visualiser et traiter des flux de données venant de sources diverses, d'une façon très similaire à un DataBase Management System. S'adresse à des applications de monitoring
    }

    \SPEentry{STREAM
    }{        Data Stream Management System
    }{         2003
    }{        idem à aurora
    }

    \SPEentry{TelegraphCQ
    }{        Data Stream Management System ?
    }{         2003
    }{        Permet d'executer des requetes continus sur un flux de donnée.
    }

    \SPEentry{Nile
    }{        
    }{         2004
    }{        Nile est la partie Stream Processing de STREAM.
    }

    \SPEentry{Borealis
    }{        Distributed Stream Processing Engines
    }{         2005
    }{        Hérite des fonctionnalités de processing d'Aurora, et des fonctionnalités de distributivité de Medusa
    }

    \SPEentry{SPC Stream Processing Core (IBM)
    }{        Distributed Stream Processing Middleware
    }{         2006
    }{        Le premier à proposer un modèle de souscription pour les applications de data-mining, et d'apporter les opérateurs non relationnel (?)
    }

    \SPEentry{Dryad
    }{        Distributed Execution Engine
    }{         2007
    }{        Distributed Execution Engine for coarse-grained Data-parallel applications. Dryad est le premier qui semble s'interesser plus au découpage d'une application et de sa distribution que d'utiliser des flux. Les flux ne sont qu'une conséquence architectural. les flux en question sont des fichiers, des sockets ou des fichiers.
    }

    \SPEentry{DryadLINQ (Microsoft)
    }{        Distributed Data-Parallel Computing
    }{         2008
    }{        DryadLINQ transforme un programme sequentiel écrit en .NET permettant de manipuler des données à l'aide d'opérations spécifiques (LINQ), en un programme executable par Dryad, et donc distribuable sur un cluster de machine, simplement en parallélisant les opérations qui peuvent l'être.
    }

    \SPEentry{SPADE / System S (IBM)
    }{        Distributed Stream Processing Engines
    }{         2008
    }{        System S is a large-scale, distributed data stream processing middleware under development at IBM T. J.Watson Research Center, TODO System S semble être une boite à outils, SPADE n'est qu'une interface graphique, SPC est un outils de distribution de calcul sur des flux.
    }

    \SPEentry{Comet
    }{        Batched Stream Processing
    }{         2010
    }{        Comet, extension de DraydLINQ, apporte l'implémentation d'un paradigme nommé Batched Stream Processing, batch computation on bulked-appended data stream, autrement dit : traitement en lot sur des flux de données actualisé par paquets. La definition semble pointer directement vers les outils actuels, Storm, TimeStream, MillWheel etc ...
    }

    \SPEentry{CBP Continus Bulk Processing
    }{        Continus Bulk Processing 
    }{         2010
    }{        Se compare à MapReduce ou Dryadd (pas de flux, juste de la distribution), en disant apporter des états dans le traitement, typiquement, modifier la topologie en fonction des résultats précédents.
    % TODO ce papier est interessant parce qu'il analyse la place et l'importance des états dans une topologie de flux, à lire.
    }

    \SPEentry{S4 (Yahoo)
    }{        
    }{         2010
    }{        Admet la similarité avec Actor Model, apporte une certaine fiabilité (fault-tolerant), peut être le premier système industrialisable.
    }

    \SPEentry{Percolator (Google)
    }{        
    }{         2010
    }{        TODO Semble très similaire à Yahoo S4, à investiger. L'idée de Percolator, c'est de mettre à jour une grosse base de données de pages, à partir d'un flux de données provenant de crawlers. Tâche qu'aucun des outils de l'époque ne savait faire (soit traiter des flux de données, soit traiter des grosses bases de données)
    }

    \SPEentry{SPARK
    }{        
    }{         2010
    }{        fault-tolerant : resilient distributed datasets (RDD), Spark n'est pas orienté flux, mais vise plutôt à remplacer Hadoop ou map reduce dans certains cas : les applications qui réutilisent un set de données pour plusieurs opérations en parallèles. SPARK offre une interface similaire à celle de DryadLINQ.
    Il est implémenté en Scala.
    }

    \SPEentry{Ciel
    }{        
    }{         2011
    }{        un moteur d'execution pour programme dataflow distribué. prétend apporter les data-dependent control-flow decision permettant d'executer des algorythme récursifs et itératifs (se compare à MapReduce et Dryadd)
    }

    \SPEentry{Storm / Trident (Twitter)
    }{        
    }{         2011
    }{        Similaire à ses prédecesseurs, pas d'article scientifique.
    }

    \SPEentry{D-Stream (extension de Spark)
    }{        
    }{         2012
    }{        étend Spark (donc pas de flux à la base) semble proposer de meilleurs mécanisme de vérification de l'intégrité et de récupération, tout en étant aussi haut niveau que les autres. (les concurent sont probablement Percolator et S4, peut être Storm.)
    }

    \SPEentry{Naiad (Microsoft)
    }{        
    }{         2012
    }{        est un ensemble d'extension de langage (TODO quel langage ? probablement .NET) (avec runtime) permettant des opérations de parallelisation sur la donnée. apporte le differential dataflow : en utilisant un ordre partiel entre les tuples, plutôt qu'un ordre absolu, les calculs sont plus souple.
    }

    \SPEentry{Sonora (Microsoft)
    }{        
    }{         2012
    }{        Sonora est un projet qui semble similaire à TimeStream, mais en beaucoup moins abouti. Ils se comparent encore à MapReduce et Dryad, quand d'autres outils ont été publié depuis et adresses les même problèmatiques.
    }

    \SPEentry{TimeStream (Microsoft)
    }{        
    }{         2013
    }{        Différent de Storm ou S4, il se veut dans la lignée des DSMS, et permet d'executer des programmes sous la forme de requetes sur un flux infini de données. Au lieu d'établir à la main la topologie (comme pour Storm ou S4), le programmeur écrit simplement une requète qui va ensuite être transformé en graphe.
    }

    \SPEentry{MillWheel (Google)
    }{        
    }{         2013
    }{        apporte à Storm, S4 ou Sonora une plus grande généralisation, et le exactly-once processing and fault-tolerant persistent state.
    }

  \end{longtable}
\end{center}                  





\endinput



% TODO read Communication and Concurrency by Milner 1989
% And A theory of synchrony and asynchrony by He 1990

% \nt{We need to talk about state synchronization outside of message-passing.}


% \subsection{Programming models}

% \subsubsection{Distributed Processes}

% Distributed processes: a concurrent programming concept \cite{Hansen1978}
% This paper defines real-time applications.
% It proposes a unification between monitor and processes to apply on distributed storage.
% The distributed processes are basically event-loops with cooperative scheduling.
% However, the communication is done via procedure calling, not message-passing.
% A process defines common procedures, callable by other processes.
% Processes have access to guarded commands, and guarded regions.
% A guarded region (cycle, when) can make the execution wait, the guarded command (if, do) not.
% (That is how is )

% Mutual exclusion (Guarded region) \cite{Hansen1978a} \cite{Hoare2002}
% Guarded commands \cite{Dijkstra1975}

% \subsubsection{Communication Sequential Processes}

% CSP is similar to Distributed Processes, it seems :
% \textit{Both proposals [DP and CSP] attack the problem of concurrency without shared variables and recognize the need for nondeterminacy within a single process.}
% That is, they acknowledged that non-determinacy is a requirement to have good performances (to rewrite), and that with causal ordering (synchronization with guarded region) the application can be made somehow deterministic.

% C. A. R. Hoare presented Communicating Sequential Processes (CSP) \cite{Hoare1978, Brookes1984}.
% In CSP, processes are executed concurrently, and communicates events via channels.
% The evolutions of this model were influenced by, and influenced the work of R. Milner, hence CCS and CSP are highly similar.
% With the evolutions appeared the problems of determinism in distributed systems\nt{TODO}.\cite{Brookes1984}






% \subsubsection{Coroutines}

% Conway defines coroutines as subroutines executing all at the same level.\cite{Conway1963}

% \textit{A coroutine is an autonomous program which communicate with adjacent modules as if they were input and output subroutines.}

% It defines the separability of a program, and advocates that coroutines are separate program and communicate by sending discrete items.
% The first definition of coroutine, explicitly specify that coroutines are best used when they can be executed, and communicate asynchronously.

% There is three conditions for coroutine
% \textit{(1) The only communication between modules is in the form of discrete items of information; (2) the flow of each of this items is along fixed, one-way paths; (3) the entire program can be laid out so that the input is at the left extreme, the output is at the right extreme, and everywhere in between all information items flowing between modules have a component of motion to the right.}
% \textit{When coroutines A and B are connected so that A sends items to B, B runs for a while until it encounters a a read command, which means it needs something from A. The control is then transfered to A until it wants to write, whereupon the control is returned to B at the point where it left off.}

% Coroutines seems to be the first definition of a concurrent pipeline.
% Coroutines can be executed simultaneously if parallel hardware is available.


% \subsubsection{Modules}

% Multiprogramming designs the ability to program concurrent activities on multiple processing units.

% Modula: A language for modular multiprogramming \cite{Wirth1977}

% Modula is a descendent of Pascal with the addition of modules, processes and signals.


\subsubsection{Promises and Futures}

This \cite{Jr1977} is the reference paper for futures.

\subsubsection{Functional Reactive Programming}
\nt{I don't know exactly what to do with this.
It is not exactly aimed at concurrency, but it is definitly not oriented on improving software growth}

Maybe I can put it with coroutine, as it seems quite similar.

But it should probably go into the reconciliation section.

\subsubsection{Flow programming}
Morrison
Noflo

\subsubsection{Data flow}

I saw this reference earlier, but don't remember where. \cite{Kahn1974}
I might relate it if I read it.


\subsubsection{Task-based parallelism}
X10 (is an APGAS), OCR, Habanero, Legion, Charm++, HPX

\subsubsection{Message-based parralelism}
Scala, Akka, Play




\subsubsection{Directive-based languages}
OpenMP, OpenACC



\subsection{Frameworks, Languages and runtimes}




\paragraph{Stream Processing Engines}
  \begin{refsection}

  Les Stream Processing Engines répondent aux problématiques de traitements d'importants volumes de données avec une faible latence.

  Ces outils permettent de modéliser le traitement des flux de données sous forme d'un graph composé de nœuds et de connexions.
  Les nœuds sont implémenté de manière classique, ce sont des programmes qui vont être exécuté pour chaque paquet d'information.
  L'outil se charge du transit de la donnée d'un noeud à l'autre en suivant les connections entre ces noeuds.
  Cette cartographie permet d'organiser un réseaux de machines disponible pour traiter les flux entrants.

  L'example des trendings topics de twitter illustre bien l'utilisation d'un tel outils.
  Imaginons le flux des tweets en entrée, et un réseau de noeux permettant de compter les tag de ces twitt afin de reporter les tag les plus courant.
  Un premier noeud va découper chaque twitt reçu en mots, ces mots vont ensuite être comptabilisé dans une base de donnée, les tags comptabilisant le plus d'occurences deviennent les trending topics de twitter (après être débarrassé des mots dénués de sens tel que 'et', 'ou', 'le' etc ...).

  Afin d'avoir une vision d'ensemble de ces projets, il peut être interessant de commencer par lire \cite{Stonebraker2005}.
  Ce papier présente 8 caractéristique qu'un outil devrait avoir lorsqu'il traite d'important volume de données. Il marque également la transition entre les outils anciennement utilisé (DBMSs et Rules engines) et les outils actuellement utilisé (Stream Processing Engines).

  Les premiers Stream Processing Engines présenté dans ce papiers sont : Aurora \cite{Abadi2003} devenu Borealis \cite{Abadi2005}, TelegraphCQ \cite{Chandrasekaran2003} et STREAM \cite{Arvind2003}.

  Depuis, d'autres projets sont apparus dans la littérature scientifique :
  \begin{itemize}
    \item Dryadd \cite{Isard2007} est un moteur d'execution décentralisé pour créer des applications distribué afin de traiter des données en parallèle. Il est élastique et fiable.
    \item S4 \cite{Neumeyer2010} propose une plateforme permettant de créer des applications distribué, élastique et fiable composé de flux de données.
    \item D-Stream \cite{Zaharia2012} propose une interface plus haut niveau et intuitive que les travaux précédents, ainsi qu'un système de récupération plus poussé (nommé parallel recomputation of lost state).
    \item TimeStream \cite{Qian2013} propose certaines amélioration dans la reprise sur panne dans un mécanisme appelé resilient substitution.
  \end{itemize}
  On compte également Twitter Storm \cite{Marz2011} et Apache Flume \cite{Apache2011}, deux projets open-source.


  Pour aller plus loin, il peut être interessant de lire ces papiers :
  SPC \cite{Amini2006}, MapReduce Online \cite{Condie2010}, in-situ MapReduce \cite{Logothetis2011}, Incoop \cite{Bhatotia2011}, Naiad \cite{McSherry}, Google Percolator \cite{Peng2010}, CBP \cite{Logothetis2010}, Comet \cite{He2010}, MegaPipe \cite{Han2012}.

  \printbibliography[heading=subbibliography]
  \end{refsection}



\paragraph{Extrait de l'article de MillWheel concernant les différents Stream Processing Engines}

Other streaming systems do not provide this combination of fault tolerance, versatility, and scalability. Spark Streaming [34] and Sonora [32] do excellent jobs of efficient checkpointing, but limit the space of operators that are available to user code. S4 [26] does not provide fully fault-tolerant persistent state, while Storm's[23] exactly-once mechanism for record delivery, Trident [22], requires strict transaction ordering to operate. Attempts to extend the batch-processing model of MapReduce and Hadoop [4] to provide low-latency systems result in compromised flexibility, such as the operator-specific dependence on Replicated Distributed Datasets [33] in Spark Streaming. Streaming SQL systems [1] [2] [5] [6] [21] [24] provide succinct and simple solutions to many streaming problems, but intuitive state abstractions and complex application logic (e.g. matrix multiplication) are more naturally expressed using the operational flow of an imperative language rather than a declarative language like SQL.



\section{Tableau regroupant les différents Stream Processing Engines}

\newcommand{\SPEentry}[4]{%
  \textbf{#1} \newline%
  #2 &%
  #3 &%
  #4 \\%
  \\%
}

\begin{center}
  \begin{longtable}{p{7cm} l p{8cm}}
    Nom & Année & Description \\
    \hline

    \SPEentry{NiagaraCQ
    }{        
    }{         2000
    }{        Un des plus ancien projet (à ma connaissance), Continuous Query (CQ), et scalable (à quel point, je ne sais pas)
    }

    \SPEentry{Aurora
    }{          Data Stream Management System
    }{          2003
    }{          Permet de visualiser et traiter des flux de données venant de sources diverses, d'une façon très similaire à un DataBase Management System. S'adresse à des applications de monitoring
    }

    \SPEentry{STREAM
    }{        Data Stream Management System
    }{         2003
    }{        idem à aurora
    }

    \SPEentry{TelegraphCQ
    }{        Data Stream Management System ?
    }{         2003
    }{        Permet d'executer des requetes continus sur un flux de donnée.
    }

    \SPEentry{Nile
    }{        
    }{         2004
    }{        Nile est la partie Stream Processing de STREAM.
    }

    \SPEentry{Borealis
    }{        Distributed Stream Processing Engines
    }{         2005
    }{        Hérite des fonctionnalités de processing d'Aurora, et des fonctionnalités de distributivité de Medusa
    }

    \SPEentry{SPC Stream Processing Core (IBM)
    }{        Distributed Stream Processing Middleware
    }{         2006
    }{        Le premier à proposer un modèle de souscription pour les applications de data-mining, et d'apporter les opérateurs non relationnel (?)
    }

    \SPEentry{Dryad
    }{        Distributed Execution Engine
    }{         2007
    }{        Distributed Execution Engine for coarse-grained Data-parallel applications. Dryad est le premier qui semble s'interesser plus au découpage d'une application et de sa distribution que d'utiliser des flux. Les flux ne sont qu'une conséquence architectural. les flux en question sont des fichiers, des sockets ou des fichiers.
    }

    \SPEentry{DryadLINQ (Microsoft)
    }{        Distributed Data-Parallel Computing
    }{         2008
    }{        DryadLINQ transforme un programme sequentiel écrit en .NET permettant de manipuler des données à l'aide d'opérations spécifiques (LINQ), en un programme executable par Dryad, et donc distribuable sur un cluster de machine, simplement en parallélisant les opérations qui peuvent l'être.
    }

    \SPEentry{SPADE / System S (IBM)
    }{        Distributed Stream Processing Engines
    }{         2008
    }{        System S is a large-scale, distributed data stream processing middleware under development at IBM T. J.Watson Research Center, TODO System S semble être une boite à outils, SPADE n'est qu'une interface graphique, SPC est un outils de distribution de calcul sur des flux.
    }

    \SPEentry{Comet
    }{        Batched Stream Processing
    }{         2010
    }{        Comet, extension de DraydLINQ, apporte l'implémentation d'un paradigme nommé Batched Stream Processing, batch computation on bulked-appended data stream, autrement dit : traitement en lot sur des flux de données actualisé par paquets. La definition semble pointer directement vers les outils actuels, Storm, TimeStream, MillWheel etc ...
    }

    \SPEentry{CBP Continus Bulk Processing
    }{        Continus Bulk Processing 
    }{         2010
    }{        Se compare à MapReduce ou Dryadd (pas de flux, juste de la distribution), en disant apporter des états dans le traitement, typiquement, modifier la topologie en fonction des résultats précédents.
    % TODO ce papier est interessant parce qu'il analyse la place et l'importance des états dans une topologie de flux, à lire.
    }

    \SPEentry{S4 (Yahoo)
    }{        
    }{         2010
    }{        Admet la similarité avec Actor Model, apporte une certaine fiabilité (fault-tolerant), peut être le premier système industrialisable.
    }

    \SPEentry{Percolator (Google)
    }{        
    }{         2010
    }{        TODO Semble très similaire à Yahoo S4, à investiger. L'idée de Percolator, c'est de mettre à jour une grosse base de données de pages, à partir d'un flux de données provenant de crawlers. Tâche qu'aucun des outils de l'époque ne savait faire (soit traiter des flux de données, soit traiter des grosses bases de données)
    }

    \SPEentry{SPARK
    }{        
    }{         2010
    }{        fault-tolerant : resilient distributed datasets (RDD), Spark n'est pas orienté flux, mais vise plutôt à remplacer Hadoop ou map reduce dans certains cas : les applications qui réutilisent un set de données pour plusieurs opérations en parallèles. SPARK offre une interface similaire à celle de DryadLINQ.
    Il est implémenté en Scala.
    }

    \SPEentry{Ciel
    }{        
    }{         2011
    }{        un moteur d'execution pour programme dataflow distribué. prétend apporter les data-dependent control-flow decision permettant d'executer des algorythme récursifs et itératifs (se compare à MapReduce et Dryadd)
    }

    \SPEentry{Storm / Trident (Twitter)
    }{        
    }{         2011
    }{        Similaire à ses prédecesseurs, pas d'article scientifique.
    }

    \SPEentry{D-Stream (extension de Spark)
    }{        
    }{         2012
    }{        étend Spark (donc pas de flux à la base) semble proposer de meilleurs mécanisme de vérification de l'intégrité et de récupération, tout en étant aussi haut niveau que les autres. (les concurent sont probablement Percolator et S4, peut être Storm.)
    }

    \SPEentry{Naiad (Microsoft)
    }{        
    }{         2012
    }{        est un ensemble d'extension de langage (TODO quel langage ? probablement .NET) (avec runtime) permettant des opérations de parallelisation sur la donnée. apporte le differential dataflow : en utilisant un ordre partiel entre les tuples, plutôt qu'un ordre absolu, les calculs sont plus souple.
    }

    \SPEentry{Sonora (Microsoft)
    }{        
    }{         2012
    }{        Sonora est un projet qui semble similaire à TimeStream, mais en beaucoup moins abouti. Ils se comparent encore à MapReduce et Dryad, quand d'autres outils ont été publié depuis et adresses les même problèmatiques.
    }

    \SPEentry{TimeStream (Microsoft)
    }{        
    }{         2013
    }{        Différent de Storm ou S4, il se veut dans la lignée des DSMS, et permet d'executer des programmes sous la forme de requetes sur un flux infini de données. Au lieu d'établir à la main la topologie (comme pour Storm ou S4), le programmeur écrit simplement une requète qui va ensuite être transformé en graphe.
    }

    \SPEentry{MillWheel (Google)
    }{        
    }{         2013
    }{        apporte à Storm, S4 ou Sonora une plus grande généralisation, et le exactly-once processing and fault-tolerant persistent state.
    }

  \end{longtable}
\end{center}                  

  2003    Aurora                  Data Stream Management System          Permet de visualiser et traiter des flux de données venant de sources diverses, d'une façon très similaire à un DataBase Management System. S'adresse à des applications de monitoring.

  2003    STREAM                   Data Stream Management System           idem à aurora

  2003    TelegraphCQ                Data Stream Management System ?          Permet d'executer des requetes continus sur un flux de donnée.

  2005    Borealis                Distributed Stream Processing Engines        Hérite des fonctionnalités de processing d'Aurora, et des fonctionnalités de distributivité de Medusa

  2006    SPC Stream Processing Core (IBM)    Distributed Stream Processing Middleware    Le premier à proposer un modèle de souscription pour les applications de data-mining, et d'apporter les opérateurs non relationnel (?)

  2007    Dryad                   Distributed Execution Engine             Distributed Execution Engine for coarse-grained Data-parallel applications. Dryad est le premier qui semble s'interesser plus au découpage d'une application et de sa distribution que d'utiliser des flux. Les flux ne sont qu'une conséquence architectural. les flux en question sont des fichiers, des sockets ou des fichiers.

  2008     DryadLINQ (Microsoft)          Distributed Data-Parallel Computing        DryadLINQ transforme un programme sequentiel écrit en .NET permettant de manipuler des données à l'aide d'opérations spécifiques (LINQ), en un programme executable par Dryad, et donc distribuable sur un cluster de machine, simplement en parallélisant les opérations qui peuvent l'être.

  2008    SPADE / System S (IBM)                                  System S is a large-scale, distributed data stream processing middleware under development at IBM T. J.Watson Research Center, TODO System S semble être une boite à outils, SPADE n'est qu'une interface graphique, SPC est un outils de distribution de calcul sur des flux.

  2010    Comet                   Batched Stream Processing             Comet, extension de DraydLINQ, apporte l'implémentation d'un paradigme nommé Batched Stream Processing, batch computation on bulked-appended data stream, autrement dit : traitement en lot sur des flux de données actualisé par paquets. La definition semble pointer directement vers les outils actuels, Storm, TimeStream, MillWheel etc ...

  2010     CBP Continus Bulk Processing       Continus Bulk Processing             Se compare à MapReduce ou Dryadd (pas de flux, juste de la distribution), en disant apporter des états dans le traitement, typiquement, modifier la topologie en fonction des résultats précédents.
  TODO ce papier est interessant parce qu'il analyse la place et l'importance des états dans une topologie de flux, à lire.

  2010    S4 (Yahoo)                                        Admet la similarité avec Actor Model, apporte une certaine fiabilité (fault-tolerant), peut être le premier système industrialisable.

  2010     Percolator (Google)                                    TODO Semble très similaire à Yahoo S4, à investiger. L'idée de Percolator, c'est de mettre à jour une grosse base de données de pages, à partir d'un flux de données provenant de crawlers. Tâche qu'aucun des outils de l'époque ne savait faire (soit traiter des flux de données, soit traiter des grosses bases de données)

  2010    Spark                                          fault-tolerant : resilient distributed datasets (RDD), Spark n'est pas orienté flux, mais vise plutôt à remplacer Hadoop ou map reduce dans certains cas : les applications qui réutilisent un set de données pour plusieurs opérations en parallèles.

  2011     Ciel                                          un moteur d'execution pour programme dataflow distribué. prétend apporter les data-dependent control-flow decision permettant d'executer des algorythme récursifs et itératifs (se compare à MapReduce et Dryadd)

  2011    Storm / Trident (Twitter)                                Similaire à ses prédecesseurs, pas d'article scientifique.

  2012    D-Stream (extension de Spark)                              étend Spark (donc pas de flux à la base) semble proposer de meilleurs mécanisme de vérification de l'intégrité et de récupération, tout en étant aussi haut niveau que les autres. (les concurent sont probablement Percolator et S4, peut être Storm.)

  2012     Naiad (Microsoft)                                  est un ensemble d'extension de langage (TODO quel langage ? probablement .NET) (avec runtime) permettant des opérations de parallelisation sur la donnée. apporte le differential dataflow : en utilisant un ordre partiel entre les tuples, plutôt qu'un ordre absolu, les calculs sont plus souple.

  2012    Sonora (Microsoft)                                    Sonora est un projet qui semble similaire à TimeStream, mais en beaucoup moins abouti. Ils se comparent encore à MapReduce et Dryad, quand d'autres outils ont été publié depuis et adresses les même problèmatiques.
  La programmation est basé sur des requêtes, écrites en MONAD (plus ou moins), de manière similaire à Bacon.JS

  2013    TimeStream (Microsoft)                                  Différent de Storm ou S4, il se veut dans la lignée des DSMS, et permet d'executer des programmes sous la forme de requetes sur un flux infini de données. Au lieu d'établir à la main la topologie (comme pour Storm ou S4), le programmeur écrit simplement une requète qui va ensuite être transformé en graphe.

  2013    MillWheel (Google)                                    apporte à Storm, S4 ou Sonora une plus grande généralisation, et le exactly-once processing and fault-tolerant persistent state  


  StreamInsight

  2000     NiagaraCQ  Un des plus ancien (avant Aurora), Continuous Query (CQ), et scalable (à quel point, je ne sais pas), publié en 2000 (3 ans avant Aurora)

  2004     Nile     Nile est la partie Stream Processing de STREAM.

StreamInsight permet de construire des Complex Event Processing applications, et est un Data Stream Management System selon [CEDR]


LINQ 
  est un framework de .NET / C\# permettant de créer des requetes directement dans le langage.
  Par exemple : 
    var truc = from nom in table where condition ...

  Lorsque LINQ est utilisé pour créer un CEP, il s'agit alors d'un programme en .NET utilisant des requetes LINQ. Ces requetes sont alors transformé en DAG (directed acyclic graph).
  C'est le cas notamment de DryadLINQ, et de TimeStream


Pour bien comprendre les principes derrières les Data Stream Management System, il peut être intérressant de lire le papier de STREAM.
Les papiers de STREAM et Aurora sembleavoir été publié presque en même temps, aucun des deux de cite l'autre.
Aurora n'est pas distribué, mais traite des flux d'informations.
Medusa offre à Aurora la possibilité de se distribuer sur plusieurs machines.
TelegraphCQ propose d'executer des requetes continus sur un flux de données, il semble répondre à la définition de Data Stream Management System bien que le terme ne soit pas employé.


[CEDR] "Consistent Streaming Through Time: A Vision for Event Stream Processing"




\section{Différence entre les SPE orienté topologies, et orienté requête}

  \subsection{Storm, exemples de code de topologie}
  \begin{code}[Java]
    package storm.starter;

    import backtype.storm.Config;
    import backtype.storm.LocalCluster;
    import backtype.storm.StormSubmitter;
    import backtype.storm.task.ShellBolt;
    import backtype.storm.topology.BasicOutputCollector;
    import backtype.storm.topology.IRichBolt;
    import backtype.storm.topology.OutputFieldsDeclarer;
    import backtype.storm.topology.TopologyBuilder;
    import backtype.storm.topology.base.BaseBasicBolt;
    import backtype.storm.tuple.Fields;
    import backtype.storm.tuple.Tuple;
    import backtype.storm.tuple.Values;
    import storm.starter.spout.RandomSentenceSpout;

    import java.util.HashMap;
    import java.util.Map;

    /**
    * This topology demonstrates Storm's stream groupings and multilang capabilities.
    */
    public class WordCountTopology {
      public static class SplitSentence extends ShellBolt implements IRichBolt {

        public SplitSentence() {
          super("python", "splitsentence.py");
        }

        @Override
        public void declareOutputFields(OutputFieldsDeclarer declarer) {
          declarer.declare(new Fields("word"));
        }

        @Override
        public Map<String, Object> getComponentConfiguration() {
          return null;
        }
      }

      public static class WordCount extends BaseBasicBolt {
        Map<String, Integer> counts = new HashMap<String, Integer>();

        @Override
        public void execute(Tuple tuple, BasicOutputCollector collector) {
          String word = tuple.getString(0);
          Integer count = counts.get(word);
          if (count == null)
            count = 0;
          count++;
          counts.put(word, count);
          collector.emit(new Values(word, count));
        }

        @Override
        public void declareOutputFields(OutputFieldsDeclarer declarer) {
          declarer.declare(new Fields("word", "count"));
        }
      }

      public static void main(String[] args) throws Exception {

        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout("spout", new RandomSentenceSpout(), 5);

        builder.setBolt("split", new SplitSentence(), 8).shuffleGrouping("spout");
        builder.setBolt("count", new WordCount(), 12).fieldsGrouping("split", new Fields("word"));

        Config conf = new Config();
        conf.setDebug(true);


        if (args != null && args.length > 0) {
          conf.setNumWorkers(3);

          StormSubmitter.submitTopology(args[0], conf, builder.createTopology());
        }
        else {
          conf.setMaxTaskParallelism(3);

          LocalCluster cluster = new LocalCluster();
          cluster.submitTopology("word-count", conf, builder.createTopology());

          Thread.sleep(10000);

          cluster.shutdown();
        }
      }
    }
  \end{code}

  \subsection{DryadLINQ exemple de code}
  \begin{code}[C]
    public static IQueryable <Pair> Histogram(IQueryable<string> input, int k) {
      IQueryable<string> words = input.SelectMany(x => x.Split(' '));
      IQueryable<IGrouping<string, string>> groups = words.GroupBy(x => x);
      IQueryable<Pair> counts = groups.Select(x => new Pair(x.Key, x.Count()));
      IQueryable<Pair> ordered = counts.OrderByDescending(x => x.count);
      IQueryable<Pair> top = ordered.Take(k);
      return top;
    }
  \end{code}

  \subsection{TimeStream exemple de code}
  \begin{code}[Java]
    // Compute sentiment changes:
    var scores = from w in topicTweets.TumblingWindow(wSize)
                 select w.Average(t => Sentiment(t));
    var change = from ss in scores.CountWindow(2)
                 where ss.IsChanged()
                 select ww.Events.Last();

    // Compute top word changes:
    var words = from t in topicTweets.TumblingWindow(wSize)
                 select Aggregate(WordCount(t));
    var delta = from ww in words.CountWindow(2)
                select Delta(ww);

    // Relate sentiment changes to word changes:
    from c in change
    from r in delta
    select ChangeWithReason(c, r)
  \end{code}
  \vspace{2mm}
  \begin{code}[Java]
    from c in input.HashPartition(c => c.FromCluster, 40)
    group c by new { c.FromMachine, c.ToMachine } into ctmp
    from w in ctmp.TumblingWindow(10000)
    select new QueryResult() {
      FromMachine = ctmp.Key.FromMachine,
      ToMachine = ctmp.Key.ToMachine,
      Latency = w.Average(a => a.Latency)
    }
  \end{code}
  The input is a continuous stream of measurements from the end machines. The payload of an event contains the la- tency of a data transfer from FromMachine to ToMachine. The query groups the latency events by each pair of ma- chines. For each pair, it computes the average latency in every tumbling window of 10 seconds.

  \subsection{SPARK exemple de code}
  \begin{code}[Java]
    val file = spark.textFile("hdfs://...")
    val errs = file.filter(_.contains("ERROR"))
    val ones = errs.map(_ => 1)
    val count = ones.reduce(_+_)
  \end{code}


  MapReduce Like : map, reduce, filter, join ...

  SQL Like : from, select, where, OrderByDescending, Take, GroupBy, SelectMany ...


\begin{longtable}{c | c | c}
                    & \textbf{Serveur} ?     & \textbf{Client} ? \\
\hline
\textbf{Topologie}   & Storm etc...           & FBP (NoFlo etc...) \\
\hline
\textbf{Requètes}   & TimeStream etc...     & FRP (Bacon.js etc...) \\
\end{longtable}


\section{API}

  \subsection{Storm/Trident}

    \subsubsection{Storm}

      Shuffle grouping
        Tuples are randomly distributed across the bolt's tasks in a way such that each bolt is guaranteed to get an equal number of tuples.
      Fields grouping
        The stream is partitioned by the fields specified in the grouping. For example, if the stream is grouped by the "user-id" field, tuples with the same "user-id" will always go to the same task, but tuples with different "user-id"'s may go to different tasks.
      All grouping
        The stream is replicated across all the bolt's tasks. Use this grouping with care.
      Global grouping
        The entire stream goes to a single one of the bolt's tasks. Specifically, it goes to the task with the lowest id.
      None grouping
        This grouping specifies that you don't care how the stream is grouped. Currently, none groupings are equivalent to shuffle groupings. Eventually though, Storm will push down bolts with none groupings to execute in the same thread as the bolt or spout they subscribe from (when possible).
      Direct grouping
        This is a special kind of grouping. A stream grouped this way means that the producer of the tuple decides which task of the consumer will receive this tuple. Direct groupings can only be declared on streams that have been declared as direct streams. Tuples emitted to a direct stream must be emitted using one of the emitDirect methods. A bolt can get the task ids of its consumers by either using the provided TopologyContext or by keeping track of the output of the emit method in OutputCollector (which returns the task ids that the tuple was sent to).
      Local or shuffle grouping
        If the target bolt has one or more tasks in the same worker process, tuples will be shuffled to just those in-process tasks. Otherwise, this acts like a normal shuffle grouping.


    \subsubsection{Trident}

      Une partition = l'ensemble des tuples géré par une instance / une tache d'un bolt.

      A.each(a, f, b)
        execute la fonction f pour chaque tuple de A, f utilise les champs défini par a, et ajoute les champs défini par b.
      A.each(a, filter)
        determine si un tuple doit être gardé ou non, en fonction de la fonction filtre et des champs défini par b.
      A.project(a)
        filtre les champs des tuples en ne gardant que les champs spécifié par a

      Partition Aggregation
        A.PartitionAggregate(a, f, b)
          Pour chaque partition, les tuples sont réduits selon la fonction f qui prend utilise les champs défini par a, et renvoi les champs défini par b.
          À la fin, chaque partition ne contient donc plus qu'un seul tuple.

          La classe f peut être implémenté à partir de :
          CombinerAggregator
            execute init sur chaque tuple, puis combine les tuples entre avec la fonction combine jusqu'à épuisement, si il n'y à aucun tuple execute la fonction zero.
          ReducerAggregator
            produit une valeur initiale avec init, puis itère sur cette valeur en la combinant avec les tuples en utilisant la fonction reduce.
          Aggregator
            produit une valeur initiale avec init, puis itère en combinant cette valeur avec les tuples en utilisant la fonction aggregate, enfin appel la fonction complete.

            Il est possible de chainer les aggregateurs.

      Stream Aggregation
        aggregation / persistentAggregation
        indépendament pour chaque batch / globalement, pour tous les batch, le résultat est stoqué dans une source of state (?)

      Repartitioning
        shuffle
          Use random round robin algorithm to evenly redistribute tuples across all target partitions
        broadcast
          Every tuple is replicated to all target partitions. This can useful during DRPC - for example, if you need to do a stateQuery on every partition of data.
        partitionBy
          partitionBy takes in a set of fields and does semantic partitioning based on that set of fields. The fields are hashed and modded by the number of target partitions to select the target partition. partitionBy guarantees that the same set of fields always goes to the same target partition.
        global
          All tuples are sent to the same partition. The same partition is chosen for all batches in the stream.
        batchGlobal
          All tuples in the batch are sent to the same partition. Different batches in the stream may go to different partitions.
        partition
          This method takes in a custom partitioning function that implements backtype.storm.grouping.CustomStreamGrouping

      groupBy a
        réparti les tuples dans des partitions en regroupant les tuples dont les champs défini par a égaux.

      merge(A, B, C)
        Simplement rassemble tous les tuples de différents stream en un seul
        Les champs du nouveau stream seront nommé d'après le premier stream

      join(A, a, B, b, c)
        Par Batch ! rassemble les tuples des stream A et B en utilisant comme clé de jointure les champs défini par a et b.
        Les champs des nouveaux tuples sont défini dans c.


  \subsection{Functional Reactive Programming / Bacon.js}

    A.map(f) 
      transforme chaque événements de A par la fonction f
    A.filter(f)
      filtre les événements selon la valeur de retour de f
    A.merge(B)
      créé un stream composé des événements de A et B.
    A.combine(B, f)
      créé un flux composé du retour de la fonction f appliqué aux 2 derniers événement de A et B.
    A.sampledBy(B, f)
      créé un flux composé du retour de la fonction f appliqué aux 2 derniers événement de A et B seulement à chaque occurrence d'un événement de B.

    A.flatMap(f)
      Pour chaque événement de A, créé un flux à partir de la fonction f et de la valeur.
      flatMap retourne un flux composé de chacune des événements des flux créé.
    A.flatMapLatest(f)
      Pareille que flatMap, mais pour chaque nouvel événement de A, l'ancien flux terminé, et un nouveau flux est créé avec la nouvelle valeur.

    A.scan(p, f)
      Correspond aux reduce de MapReduce.
      Créé un flux d'évenement nommé ici B.
      Pour chaque événement de A, f prend 2 arguments : la dernière valeur de A et la dernière valeur de B, la valeur que retourne f est poussé sur le flux B.
    A.fold(p, f) / A.reduce(p, f)
      Comme scan, mais ne renvoie que la dernière valeur.
    A.diff(p, f)
      retourne la différence calculé par f entre les deux dernière événements de A.
      La première valeur est p.

    A.take(n)
      seulement les n premiers événement de A.
    A.takeWhile(f)
      créé un flux avec les événement de A tant que f renvoie vrai.
    A.takeUntil(B)
      créé un flux avec les événement de A jusqu'à ce que B envoie un événement Next
    A.skip(n)
      Ignore les n premiers événement du flux A.
    A.delay(n)
      delai de n ms
    A.throttle(n)
      s'assure qu'il y ai au minimum n ms entre 2 événements de A
    A.debounce(n) / AdebounceImmediate.(n)
      renvoie le dernier/premier événement dans une fenêtre de n ms minimum.

    A.map(f) <=> f.sampledBy(A)


    ...

  \subsection{LINQ}

    Select (Map)
      The Select operator performs a projection on the collection to select interesting aspects of the elements. The user supplies an arbitrary function, as a delegate or lambda expression, which projects the data members.

    Where (Filter)
      The Where operator allows the definition of a set of predicate rules that are evaluated for each object in the collection, while objects that do not match the rule are filtered away. The predicate is supplied to the operator as a delegate.

    SelectMany (Bind)
      For a user-provided mapping from collection elements to collections, semantically two steps are performed. First, every element is mapped to its corresponding collection. Second, the result of the first step is flattened by one level. Note: Select and Where are both implementable in terms of SelectMany, as long as singleton and empty collections are available. The translation rules mentioned above still make it mandatory for a LINQ provider to provide the other two operators.

    Sum / Min / Max / Average
      These operators optionally take a lambda that retrieves a certain numeric value from each element in the collection and uses it to find the sum, minimum, maximum or average values of all the elements in the collection, respectively. Overloaded versions take no lambda and act as if the identity is given as the lambda.

    Aggregate (Fold)
      A generalized Sum / Min / Max. This operator takes a lambda that specifies how two values are combined to form an intermediate or the final result. Optionally, a starting value can be supplied, enabling the result type of the aggregation to be arbitrary. Furthermore, a finalization function, taking the aggregation result to yet another value, can be supplied.

    Join / GroupJoin
      The Join operator performs an inner join on two collections, based on matching keys for objects in each collection. It takes two functions as delegates, one for each collection, that it executes on each object in the collection to extract the key from the object. It also takes another delegate in which the user specifies which data elements, from the two matched elements, should be used to create the resultant object. The GroupJoin operator performs a group join. Like the Select operator, the results of a join are instantiations of a different class, with all the data members of both the types of the source objects, or a subset of them.

    Take / TakeWhile
      The Take operator selects the first n objects from a collection, while the TakeWhile operator, which takes a predicate, selects those objects that match the predicate.

    Skip / SkipWhile
      The Skip and SkipWhile operators are complements of Take and TakeWhile - they skip the first n objects from a collection, or those objects that match a predicate (for the case of SkipWhile).

    OfType
      The OfType operator is used to select the elements of a certain type.

    Concat
      The Concat operator concatenates two collections.

    OrderBy / ThenBy
      The OrderBy operator is used to specify the primary sort ordering of the elements in a collection according to some key. The default ordering is in ascending order, to reverse the order, the OrderByDescending operator is to be used. ThenBy and ThenByDescending specifies subsequent ordering of the elements. The function to extract the key value from the object is specified by the user as a delegate.

    Reverse
      The Reverse operator reverses a collection.

    GroupBy
      The GroupBy operator takes a delegate that extracts a key value and returns a collection of IGrouping<Key, Values> objects, for each distinct key value. The IGrouping objects can then be used to enumerate all the objects for a particular key value.

    Distinct
      The Distinct operator removes duplicate instances of a key value from a collection. The function to retrieve the key value is to be supplied as a delegate.

    Union / Intersect / Except
      These operators are used to perform a union, intersection and difference operation on two sequences, respectively.

    SequenceEqual
      The SequenceEqual operator determines whether all elements in two collections are equal and in the same order.

    First / FirstOrDefault / Last / LastOrDefault
      These operators take a predicate. The First operator returns the first element for which the predicate yields true or throws an exception, if nothing matches. The FirstOrDefault operator is like the First operator except that it returns the default value for the element type (usually a null reference) in case nothing matches the predicate. The last operator retrieves the last element to match the predicate, or throws an exception in case nothing matches. The LastOrDefault returns the default element value, if nothing matches.

    Single
      The Single operator takes a predicate and returns the element that matches the predicate. An exception is thrown, if none or more than one element match the predicate.

    ElementAt
      The ElementAt operator retrieves the element at a given index in the collection.

    Any / All / Contains
      The Any operator checks, if there are any elements in the collection matching the predicate. It does not select the element, but returns true for a match. The All operator checks, if all elements match the predicate. The Contains operator checks, if the collection contains a given value.

    Count
      The Count operator counts the number of elements in the given collection. 


  \subsection{Comparaison : Bacon.js vs. LINQ vs MapReduce}

    \subsubsection{Cas spécial : From}
      Le mot clé From existe en LINQ / SQL mais n'a pas d'équivalence directe en Bacon.js ou MapReduce.
      Il s'agit d'une selection non necessaire dans ces langage car la selection se fait par l'intermédiaire de l'opérateur '.' .

    \subsubsection{Map (select)}
      En Bacon.js / FRP
        var numsPlusOne =
          numbers.map(function(n){
            return n + 1;
          });

      En LINQ / SQL (select)
        var numsPlusOne = 
          from n in numbers 
          select n + 1; 

      Map permet d'appliquer une fonction à un ensemble d'éléments.

    \subsubsection{Cas spécial : SampledBy}

      SampledBy est l'inverse de map : A.map(f) <=> f.sampledBy(A). 

      A.map(f) renvoie une valeur modifié par f à chaque occurence de A.
      f.sampledBy(A) renvoie une valeur modifié par f à chaque occurence de A.

      Cette distinction ne semble nécessaire que à cause de la composante temporel de Bacon.js

    \subsubsection{Filter (where) / OfType}
      En Bacon.js / FRP
        var lowNums = 
          numbers.filter(function(n){
            if (n < 5)
              return n;
          })

      En LINQ / SQL (where)
        var lowNums = 
          from n in numbers 
          where n < 5 
          select n;

      Filter est très similaire à Map.
      Filter permet de sélectionner les éléments à inclure dans l'ensemble d'arrivé à l'aide d'une fonction.
      La différentiation entre Filter et Map n'à de sens que si Filter n'accepte que des conditions, et Map n'accepte que des expressions impératives comme c'est le cas en SQL.
      En revanche, ce n'est pas le cas pour les langages fonctionnel en revanche.

    \subsubsection{Single}

      Single permet de s'assurer qu'un ensemble ne possède qu'un et un seul élément validant un certain prédicat.
      Si il n'y à aucun ou plusieurs éléments validant ce prédicat, Single renvoie une exception.

      Un comportement similaire peut être implémenté en Bacon.js, à la différence qu'une exception n'aurais pas de sens du fait de la nature temporel des ensemble en Bacon.js.

    \subsubsection{GroupBy}

      L'opération Filter est en réalité un cas simplifié de GroupBy.
      GroupBy permet de regrouper les éléments d'une liste en catégorie selon une certaine clé (calculé ou translaté d'une table).
      Filter permet donc d'opérer un GroupBy avec 2 groupes : rejeté et accepté; avec pour clé, la condition du filtre.

      Il est donc possible de créer un GroupBy avec plusieurs filtres en cascades.

      var group1 = source.filter(condition1);
      var group2 = source.filter(condition2);
      var group3 = source.filter(condition3);
      var group...

    \subsubsection{merge Concat ?Union }
      En Bacon.js
        var AllNumbers = numbersA.merge(numbersB);

        combine les éléments des flux numbersA et numbersB en un flux unique AllNumbers.

      En LINQ / SQL union
        var uniqueNumbers = numbersA.Concat(numbersB);

      Rassemble tous les éléments de deux ensembles dans un ensemble unique.

      Pour l'union, cette équivalence n'est pas directe, en LINQ, les opérateurs d'ensemble ont un sens puisque applicable sur des ensembles (des listes par exemple);
      En revanche, en Bacon.js, il s'agit d'une Map, un ensemble de couple clé / valeur, avec pour clé, l'instant d'occurrence de l'événement, donc pour que deux flux produisent une intersection, il faudrait que chacun des deux flux produisent en même temps la même valeur, ce cas est suffisamment peu courant pour qu'il n'existe pas d'opérateur dédié. (En revanche, on pourrais simuler un tel filtre avec un combine (pour chaque nouvel valeur, vérifier que ce n'est pas un doublon de la valeur immédiatement précédente))

    \subsubsection{combine}

      En Bacon.js combine prend les derniers éléments de 2 flux, les applique à une fonction afin de générer une valeur à pousser sur le flux de résultant.

      En LINQ combine prend applique une fonction sur les éléments de 2 liste à la même place.


    \subsubsection{scan / fold / reduce / diff / aggregate}
      En Bacon.js
        var product = doucles.scan(1, function(n, m) {
          return n * m;
        });

      En LINQ / SQL Aggregate
        double[] doubles = { 1.7, 2.3, 1.9, 4.1, 2.9 };
        double product = doubles.Aggregate((runningProduct, nextFactor) => runningProduct * nextFactor); 

       TODO ? 

      \paragraph{Sum / min / max / average / Count}
        Ces mots clé du langage SQL ne sont que des agrégateurs spécifique pré-implémenté.
        Ils est possible de les implémenter en Bacon.js à l'aide d'un aggrégateur tel scan.


    \subsubsection{SelectMany Bind}
      var pairs = 
        from a in numbersA 
        from b in numbersB 
        where a < b 
        select new { a, b };

        Créé des paires en combinant les deux listes.

        N'existe pas en Bacon.js ? TODO

    \subsubsection{FlatMap}

      Le mot clé FlatMap pourrait être ce qui se rapproche le plus du SelectMany, cependant, les 2 restent très différents.
      SelectMany va schématiquement créé un tableau dont chaque case est un couple possible entre 2 valeurs issus des listes.

      A.flatMap(f) créé pour chaque élément de A composé des valeurs de retour de la fonction f appliqué à un élément.
      Si dans la fonction f, on utilise un flux B, correspondant à la seconde liste du SelectManu, alors schématiquement, FlatMap créé le même tableau SI tous les événements de A apparaissent avant les événements de B.
      Sinon, certaines cases du tableau seront vides (typiquement, avec B les colonnes, et A les lignes, les cases dans le coin en bas à gauche ne pourront pas être rempli).

    \subsection{FlatMapLatest}

      FlapMapLatest est identique à FlatMap, à la différence que seul le dernier flux issu d'un événement de A est écouté.

    \subsubsection{Take / TakeWhile / Skip / SkipWhile}

      Il s'agit de cas particulier de Map/Filter.

      Take et Skip permettent de filtrer suivant la position des éléments dans l'ensemble.
      TakeWhile et SkipWhile permettent de filtrer suivant la position et une fonction précisant d'autres conditions.
      (SkipWhile n'est pas implémenté directement en Bacon.js, mais il est possible de l'implémenter avec un map ou un filter).

      Bacon.js apporte le mot clé takeUntil qui permet d'utiliser la première occurence d'un flux comme condition du take.

    \subsubsection{delay / throttle / debounce / DebounceImmediate}
      Ces mot-clés sont complétement ancré dans la temporalité de Bacon.js, ils n'ont pas de sens en LINQ / SQL.

    \subsubsection{First / FirstOrDefault / Last / LastOrDefault}
      Ces mots clés permettent de sélectionner le premier ou dernier élément satisfaisant une certaine condition.
      Ils n'existent pas en Bacon.js car ils sont la composition de plusieurs notion élémentaire.
      First = .filter(condition).take(1);
      FirstOrDefault = .filter(condition).mapEnd(default).take(1);
        On ne peut être sûr d'envoyer la valeur default que lorsque le flux est clos.
      
      Last = .filter(condition)
        Le flux aura toujours la valeur du dernier élément remplissant la condition
      LastOrDefault = .Next(default).filter(condition)
        Le flux aura la valeur default jusqu'à ce qu'un autre élément remplisse la condition.

    \subsubsection{ElementAt / Reverse}
      Ces mots clés n'existe pas en Bacon.js, puisque Bacon.js ne manipule pas des ensembles bornés.
      TODO, ElementAt pourrais être implémenté par un .take(n).skip(n-1) mais : 1- ça conduiras sûrement à des problèmes de performances, 2- ça n'as pas de sens en FRP

    \subsubsection{Join / GroupJoin}
      Ces mots clés n'existe pas en Bacon.js, puisque Bacon.js ne manipule pas des ensembles bornés.

    \subsubsection{OrderBy / ThenBy}
      Ces mots clés n'existe pas en Bacon.js, puisque Bacon.js ne manipule pas des ensembles bornés.

    \subsubsection{SequenceEqual}
      Permet de comparer des listes élément par élément.
      N'existe pas en Bacon.js, mais peut être implémenté à l'aide d'un sampledBy

    \subsubsection{Any / All / Contains}
      Permet de filtrer en s'assurant que au moins un / tous les éléments remplissent une condition.
      (Cette condition peut utiliser Contains)

  Bacon.js
  -  map(f)
  -  filter(f)
  -  merge(B)
  -  combine(B, f)
  -  sampledBy(B, f)

  -  flatMap(f)
  -  flatMapLatest(f)

  -  scan(p, f)
  -  fold(p, f) / reduce(p, f)
  -  diff(p, f)

  -  take(n) / takeWhile(f) / takeUntil(B)
  -  skip(n)
  -  delay(n)
  -  throttle(n)
  -  debounce(n) / AdebounceImmediate.(n)

  LINQ
  -  From
  -  Select (Map)
  -  Where (Filter)
  -  SelectMany (Bind)
  -  Aggregate (Fold) 
  -  Sum / Min / Max / Average
  -  Join / GroupJoin
  -  Take / TakeWhile
  -  Skip / SkipWhile
  -  OfType
  -  Concat
  -  OrderBy / ThenBy
  -  Reverse
  -  GroupBy
  -  Distinct / Union / Intersect / Except
  -  SequenceEqual
  -  First / FirstOrDefault / Last / LastOrDefault
  -  Single
  -  ElementAt
  -  Any / All / Contains
  -  Count

  MapReduce
    map
    reduce
    collect
    partitioner ... (TODO)
    grouping ... (TODO)