\section{Parallel execution} \label{chapter3:parallel-execution}

Programming started with a very sequential nature, as we saw in the last chapter.
But it eventually evolved toward concurrency to make advantage of the parallel architecture.

The first models of computation, like the Turing machine and lambda-calculus, were inherently sequential and based on a global state.
A formalism was lacking to represent concurrent computations.
We present the most important works on formalisms for parallel computation.
They first tackled the problems of determinacy, communication and state synchronization.
The answer to this problems seems to lie in a formalism based on a network of concurrent processes, asynchronously communicating via messages.
We present the works on the programming models based on this formalism.
Recently, with the need of performance from the web to process stream of requests, we see huge improvements in the field of distributed stream processing.

\subsection{Concurrency Theory} \label{chapter3:parallel-execution:concurrency-theory}

The mathematical models are a ground for all following work on concurrent programming, we briefly explain them in the next paragraphs.
There is two main formal models for concurrent computations.
The Actor Model of C. Hewitt, the Pi-calculus of R. Milner and the Communicating Sequential Processes of T. Hoare.
Based on these definition, we explain the importance of determinism, and the reason that asynchronous message-passing prevailed.


% TODO illustration of cells, and draw an analogy between cells and actor model.
% Or something the actor models is based upon.

\subsubsection{Models}

\paragraph{Actor Model}

The Actor model allows to express the computation as a set of communicating actors \cite{Hewitt1973a, Hewitt1977, Clinger1981}.
In reaction to a received message, an actor can create actors, send messages, and choose how to respond to the next message.
All actors are executed concurrently, and communicate asynchronously.

The Actor model was presented as a highly parallel programming model, but intended for Artificial Intelligence purposes.
Its success spread way out of this first scope, and it became a general reference on message passing parallel programming.
For example, the Scala language advertises its use of an actor approach of concurrency.

% More recent work of C. Hewitt on Actors is about ... \nt{TODO} \cite{Hewitt2007,Hewitt2007a}.

The Actor model uses an asynchronous message-passing communication paradigm.
The communication between two actors, the sender and the receiver, is a stream of discrete messages.
% The sender names the receiver actor when sending messages to be the recipient of these messages.
It is asynchronous because, contrary to invocation, the sender doesn't wait for the result of the initiated communication.

\paragraph{$\Pi$-calculus}

R. Milner presented a process calculus to describe concurrent computation : the Calculus of Communicating Systems (CCS) \cite{Milner1975, Milner1980}.
It is an algebraic notation to express identified processes communicating through synchronous labeled channels.
% In CCS, process compose concurrently, communications are synchronous, and the topology is static.
The Pi-calculus improved upon this earlier work to allow processes to be communicated as values, hence to become mobile \cite{Engberg1986,Milner1992a,Milner1992}.
Similarly to Actors, in Pi-calculus processes can dynamically modify the topology.
However, contrary to the Actor model, communications in Pi-calculus are based on simultaneous execution of complementary actions, they are synchronous.


% Actors can create actors, pi-caclulys processes can replicate, and send processes through channel.
% Processes create a new processes on each instruction to continue the execution.

% Pi-calculus resembles to the actor model, but its algebraic nature led to a critical difference with the latter.
% Indeed, processes in the Pi-calculus communicate indirectly, through labeled ports, whereas actors communicate directly by naming the recipient actors.
% This difference allows multiple processes to listen in turns to the same channel, whereas the recipient of a message cannot change.

% I think this difference lead the Pi-calculus to be composable, whereas message-passing is not.
% Message-passing is not composable, whereas invocation is.
% The Actor model is not an ideal programming model, as non-composability makes difficult to reuse or extends existing components.
% A way to compose actors, is to send to an actor the name of the actor to respond to.
% It is similar in essence to the continuation concept.



\subsubsection{Determinism and Non-determinism}

% All these early work adopted concurrent composition by default, instead of sequential composition, to adapt to the very concurrent nature of real parallel machines.
% However, sequential programming is still the default.
% Concurrent composition is yet still to be widely accepted, as stated by Reed \cite{Reed2012}.
% \comment{TODO rewrite this paragraph}

The Actor Model uses asynchronous communications, while $\pi$-calculus uses synchronous communications.
Synchronous communications are deterministic.
The message sent needs to be received to continue the execution on both ends.
Because the concurrent executions and the communications in such system are both deterministic, the result of the concurrent system is assured to be deterministic.
Determinism is a wanted property to assure the correctness of the execution.

On the other hand, asynchronous communications are non-deterministic.
The message sent can take an infinite time to be received.
Therefore, the result of the concurrent system is not assured to be deterministic.

But the communication in reality are subject to various fault and attacks, called Byzantin fault \cite{Lamport1982}.
It makes the real communications means unable to provide the determinism required by the deterministic models.
The Actor model, on the other hands, was explicitly designed to take physical limitations in account \cite{Hewitt1977a}.
Eventually, All these works evolved to adopt asynchronous communications.
Indeed, it is not realistic to build a distributed system based on synchronous communications.

Moreover, the total ordering of messages is only local to an actor, while between actors, messages are causally ordered.
As Lamport showed \cite{Lamport1978}, and Reed related later \cite{Reed2012}, causal order is sufficient to build a correct distributed system.
The non-determinism in communications is hidden by the organization of the system.
The execution will either terminate correctly, or not terminate at all.




% Asynchronous communications are less expressive than synchronous ones \cite{PALAMIDESSI2003}.

% Pi-calculus is a synchronous paradigm which contains an asynchronous fragment.\cite{PALAMIDESSI2003}
% (Boudol, G. (1992). Asynchrony and the π-calculus (note). Rapport de Recherche  1702, INRIA, Sophia-Antipolis,
% Honda, K. and Tokoro, M. (1991).  An object calculus for asynchronous communication. In America, P., editor, Proceedings of the European Conference on Object-Oriented Programming (ECOOP), volume 512 of Lecture Notes in Computer Science, pages 133–147. Springer-Verlag)


% The asynchronous pi-calculus defined by Honda and Tokoro in 1991 led to Pict, a programming language\cite{Pierce2000}.




% There was firstly theories, and models for concurrent computation.
% The main problem was determinism.
% In a sequential machine, the non-determinism of the physical world is hidden by the sequentiality of the machine.
% However, in concurrent computation, the order of communication cannot be assured the way the order of statements is assured in a sequential machine.
% We observe local non-determinism.
% However, to conserve an apparent determinism, causal ordering is sufficient.



\subsection{Message passing concurrency} \label{chapter3:parallel-execution:message-passing}


The theory advocates asynchronous message-passing, but it doesn't precise the granularity of the actors.
In the Actor Model, everything is an actor, even the simplest types, like numbers.
In practice this level of asynchronous communication is unachievable due to overhead.
In practice, most implementations feature independent synchronous processes communicating by messages.

\subsubsection{Programming models}

Conway defines coroutines as an autonomous program which communicate with adjacent modules as if they were input and output subroutines.\cite{Conway1963}
It seems to be the first definition of a parallel pipeline.
\textit{When coroutines A and B are connected so that A sends items to B, B runs for a while until it encounters a a read command, which means it needs something from A. The control is then transfered to A until it wants to write, whereupon the control is returned to B at the point where it left off.}

Hoare presented the Communicating Sequential Processes (CSP) \cite{Hoare1978, Brookes1984}.
These processes are executed concurrently, and communicates events via named channels.
The evolutions of this model were influenced by, and influenced the work of Milner that led to $\pi$-calculus.

Similarly, Kahn developed the Kahn Networks \cite{Kahn1974, Kahn1976}, following the work of Conway on coroutines.
They are explicitly parallel coroutines separated by bounded FIFO streams for communication.

These programming models are highly similar, and differs only in details irrelevant for this thesis.
However, it is interesting to note that they don't correctly inherit from the Actor Model, as it is generally impossible to dynamically modify the topology of the application.
Coroutines and processes are defined statically in the source of the application.
We shall come back to this limitation later.

\subsubsection{Scalability law}

These programming model were applied to run programs concurrently in machines providing a single processor, or shared resources among processors, like a common memory store, or network interface.
To manage these resources, and avoid conflicting accesses, it is crucial to assure the mutual exclusion.
For this purpose, Djikstra invented the Semaphore \cite{Dijkstra}.

Following this work, he also introduced guarded commands \cite{Dijkstra1975} and Hansen introduced guarded region \cite{Hansen1978a}.
Both assure the execution of a set of instructions to be exclusive to only one process.

Hoare introduced the monitor following the work of Hansen \cite{Hoare1974}.
A monitor is an extension of a class, it regroups data and procedures, except that it assures its procedures to be entered only once at a time.
With this restrictions, it guards against race condition on the access of a shared resource.
Modula \cite{Wirth1977} and Concurrent Pascal \cite{Hansen1975} uses Monitors.

Multi-threading programming extensively uses these concepts, because of the preemptive scheduling and the common storage.
It is known to lead to bad performances, and difficulties in the development \cite{Adya2002}.

Amdahl warned against sequential portion of a program impacting the performance gained with parallelism \cite{Amdahl1967}.
Ghunter extended Amdahl's law to show that sharing resources, even protected, leads to bad, and even decreases performances with increasing parallelism \cite{Gustafson1988,Gunther1993,Gunther1996,Nelson1996,Gunther2002}.
Therefore, the only way to do an efficient concurrent application is to isolate the parallel processes in such way to limit the communication to the minimum.
It is important the processes to be independent, and communicate solely by messages.

\subsubsection{Programming languages}

% Scala / Akka / Erlang

Some programming languages features message-passing and isolation of actors directly.
To some extent, these languages succeeded in industrial contexts.
However, they largely remain elitist solutions for specific problems more than a general, and accessible tool.

Scala is an attempt at unifying the object model and functional programming \cite{Odersky2004}.
It proposes an actor approach in its design.
Akka\ftnt{http://akka.io/} is a framework based on Scala, to build higly scalable and resilient applications.

Erlang is a concurrent language designed by Ericsson to operate telecommunication devices \cite{JoeArmstrong}.


All these solutions are generally oriented to design distributed systems.
We focus more precisely on real-time web applications that process high volumes of requests.



\subsection{Stream Processing} \label{chapter3:parallel-execution:stream-processing}

Nowadays, we see more industrial platforms for stream processing.
% See all the good stuffs at the end of this file.










\endinput



TODO read Communication and Concurrency by Milner 1989
And A theory of synchrony and asynchrony by He 1990

\nt{We need to talk about state synchronization outside of message-passing.}


\subsection{Programming models}

\subsubsection{Distributed Processes}

Distributed processes: a concurrent programming concept \cite{Hansen1978}
This paper defines real-time applications.
It proposes a unification between monitor and processes to apply on distributed storage.
The distributed processes are basically event-loops with cooperative scheduling.
However, the communication is done via procedure calling, not message-passing.
A process defines common procedures, callable by other processes.
Processes have access to guarded commands, and guarded regions.
A guarded region (cycle, when) can make the execution wait, the guarded command (if, do) not.
(That is how is )

Mutual exclusion (Guarded region) \cite{Hansen1978a} \cite{Hoare2002}
Guarded commands \cite{Dijkstra1975}

\subsubsection{Communication Sequential Processes}

CSP is similar to Distributed Processes, it seems :
\textit{Both proposals [DP and CSP] attack the problem of concurrency without shared variables and recognize the need for nondeterminacy within a single process.}
That is, they acknowledged that non-determinacy is a requirement to have good performances (to rewrite), and that with causal ordering (synchronization with guarded region) the application can be made somehow deterministic.

C. A. R. Hoare presented Communicating Sequential Processes (CSP) \cite{Hoare1978, Brookes1984}.
In CSP, processes are executed concurrently, and communicates events via channels.
The evolutions of this model were influenced by, and influenced the work of R. Milner, hence CCS and CSP are highly similar.
With the evolutions appeared the problems of determinism in distributed systems\nt{TODO}.\cite{Brookes1984}






% \subsubsection{Coroutines}

% Conway defines coroutines as subroutines executing all at the same level.\cite{Conway1963}

% \textit{A coroutine is an autonomous program which communicate with adjacent modules as if they were input and output subroutines.}

% It defines the separability of a program, and advocates that coroutines are separate program and communicate by sending discrete items.
% The first definition of coroutine, explicitly specify that coroutines are best used when they can be executed, and communicate asynchronously.

% There is three conditions for coroutine
% \textit{(1) The only communication between modules is in the form of discrete items of information; (2) the flow of each of this items is along fixed, one-way paths; (3) the entire program can be laid out so that the input is at the left extreme, the output is at the right extreme, and everywhere in between all information items flowing between modules have a component of motion to the right.}
% \textit{When coroutines A and B are connected so that A sends items to B, B runs for a while until it encounters a a read command, which means it needs something from A. The control is then transfered to A until it wants to write, whereupon the control is returned to B at the point where it left off.}

% Coroutines seems to be the first definition of a concurrent pipeline.
% Coroutines can be executed simultaneously if parallel hardware is available.


% \subsubsection{Modules}

% Multiprogramming designs the ability to program concurrent activities on multiple processing units.

% Modula: A language for modular multiprogramming \cite{Wirth1977}

% Modula is a descendent of Pascal with the addition of modules, processes and signals.


\subsubsection{Promises and Futures}

This \cite{Jr1977} is the reference paper for futures.

\subsubsection{Functional Reactive Programming}
\nt{I don't know exactly what to do with this.
It is not exactly aimed at concurrency, but it is definitly not oriented on improving software growth}

Maybe I can put it with coroutine, as it seems quite similar.

But it should probably go into the reconciliation section.

\subsubsection{Flow programming}
Morrison
Noflo

\subsubsection{Data flow}

I saw this reference earlier, but don't remember where. \cite{Kahn1974}
I might relate it if I read it.

\subsubsection{SIMD / SPMD / MIMD / MPMD}

\subsubsection{Partitioned Global Address Space (PGAS)}
OpenSHMEM, UPC, CAF, Chapel


\subsubsection{Task-based parallelism}
X10 (is an APGAS), OCR, Habanero, Legion, Charm++, HPX

\subsubsection{Message-based parralelism}
Scala, Akka, Play

\subsubsection{Directive-based languages}
OpenMP, OpenACC





\subsection{Frameworks, Languages and runtimes}

\subsubsection{Stream Processing}

SEDA

StreaMIT

CANS Cluster-based scalable network services

SQL-like
  Grape / Timestream - distributed SQL (roughly)
  CQL
  STREAM (uses CQL)
  StreaQuel
  TelegraphCQ
  AQuery

Map/Reduce
  MapReduce    Stateless dataflow
  Hadoop       Stateless dataflow
  Incoop       Incremental dataflow

Functional
  DryadLINQ    Stateless dataflow
  Spark        Stateless dataflow
  Nectar       Incremental dataflow
  Comet        Batched dataflow
  D-Streams    Batched dataflow

Dataflow
  CBP          Incremental dataflow
  Naiad        Batched dataflow
  Storm, S4    Continuous dataflow
  SEEP         Continuous dataflow

Imperative
  CIEL         Stateless dataflow
  SDG          Stateful dataflow
  Piccolo      Parallel in-memory