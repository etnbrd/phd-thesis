\section{Parallel execution} \label{chapter3:parallel-execution}

Programming started with a very sequential nature, as Moore's law \cite{Moore1965} was wrongly interpreted as an exponential evolution in the sequential performance of the processing unit.
But it eventually evolved toward concurrency to make advantage of parallel architectures.

The first models of computation, like the Turing machine and lambda-calculus, were sequential and based on a global memory state.
A formalism was lacking to represent concurrent computations.
We present the most important works on formalisms for parallel computation.
They first tackled the problems of determinacy, communication and state synchronization.
The answer to this problems seems to lie in a formalism based on a network of concurrent processes, asynchronously communicating via messages.
We present the works on the programming models based on this formalism.
Recently, with the need of performance from the web to process stream of requests, we see huge improvements in the field of distributed stream processing.

\subsection{Concurrency Theory} \label{chapter3:parallel-execution:concurrency-theory}

The mathematical models are a ground for all following work on concurrent programming, we briefly explain them in the next paragraphs.
There are two main formal models for concurrent computations.
The Actor Model of C. Hewitt, the Pi-calculus of R. Milner.
Based on these definition, we explain the importance of determinism, and the reason that made asynchronous message-passing prevail.

% TODO illustration of cells, and draw an analogy between cells and actor model.
% Or something the actor models is based upon.

\subsubsection{Models}

\paragraph{Actor Model}

The Actor model allows to express the computation as a set of communicating actors \cite{Hewitt1973a, Hewitt1977, Clinger1981}.
In reaction to a received message, an actor can create actors, send messages, and choose how to respond to the next message.
All actors are executed concurrently, and communicate asynchronously.
% The Actor model uses an asynchronous message-passing communication paradigm.
% The communication between two actors, the sender and the receiver, is a stream of discrete messages.
% The sender names the receiver actor when sending messages to be the recipient of these messages.
Asynchronous communication means that the sender continues its execution immediately after sending the message, before receiving the result of the initiated communication.

The Actor model was presented as a highly parallel programming model, but intended for Artificial Intelligence purposes.
Its success spread way out of this first scope, and it became a general reference and influence.
% For example, the Scala programming language features an actor approach to concurrency.

% More recent work of C. Hewitt on Actors is about ... \nt{TODO} \cite{Hewitt2007,Hewitt2007a}.

\paragraph{$\Pi$-calculus}

R. Milner presented a process calculus to describe concurrent computation : the Calculus of Communicating Systems (CCS) \cite{Milner1975, Milner1980}.
It is an algebraic notation to express identified processes communicating through synchronous labeled channels.
% In CCS, process compose concurrently, communications are synchronous, and the topology is static.
The $\pi$-calculus improved upon this earlier work to allow processes to be communicated as values, hence to become mobile \cite{Engberg1986,Milner1992a,Milner1992}.
Therefore, similarly to Actors, in Pi-calculus processes can dynamically modify the topology.
However, contrary to the Actor model, communications in Pi-calculus are based on simultaneous execution of complementary actions, they are synchronous.


% Actors can create actors, pi-caclulys processes can replicate, and send processes through channel.
% Processes create a new processes on each instruction to continue the execution.

% Pi-calculus resembles to the actor model, but its algebraic nature led to a critical difference with the latter.
% Indeed, processes in the Pi-calculus communicate indirectly, through labeled ports, whereas actors communicate directly by naming the recipient actors.
% This difference allows multiple processes to listen in turns to the same channel, whereas the recipient of a message cannot change.

% I think this difference lead the Pi-calculus to be composable, whereas message-passing is not.
% Message-passing is not composable, whereas invocation is.
% The Actor model is not an ideal programming model, as non-composability makes difficult to reuse or extends existing components.
% A way to compose actors, is to send to an actor the name of the actor to respond to.
% It is similar in essence to the continuation concept.



\subsubsection{Determinism and Non-determinism}

% All these early work adopted concurrent composition by default, instead of sequential composition, to adapt to the very concurrent nature of real parallel machines.
% However, sequential programming is still the default.
% Concurrent composition is yet still to be widely accepted, as stated by Reed \cite{Reed2012}.
% \comment{TODO rewrite this paragraph}

The Actor Model uses asynchronous communications, while $\pi$-calculus uses synchronous communications.
% Synchronous communications are deterministic.
% The message sent needs to be received to continue the execution on both ends.
Because the concurrent executions and the communications in such system are both deterministic, the result of the concurrent system is assured to be deterministic.
Determinism is a wanted property to assure the correctness of the execution.

On the other hand, asynchronous communications are non-deterministic.
The message sent can take an infinite time to be received.
Therefore, the result of the concurrent system is not assured to be deterministic.

But the communication in reality are subject to various fault and attacks \cite{Lamport1982}.
And the wait required by synchronous communication negatively impact performances of the system because of the difference between communication latency, and execution latency.
The Actor model was explicitly designed to take these physical limitations in account \cite{Hewitt1977a}.

Moreover, the total ordering of messages is only local to an actor, while between actors, messages are causally ordered.
As Lamport showed \cite{Lamport1978}, and Reed related later \cite{Reed2012}, causal order is sufficient to build a correct distributed system.
The non-determinism in the asynchronous communications is hidden by the organization of the system.
The execution will either terminate correctly, or not terminate at all.

Eventually, following works adopted asynchronous communications.
Indeed, it is not realistic to build a distributed system based on synchronous communications.




% Asynchronous communications are less expressive than synchronous ones \cite{PALAMIDESSI2003}.

% Pi-calculus is a synchronous paradigm which contains an asynchronous fragment.\cite{PALAMIDESSI2003}
% (Boudol, G. (1992). Asynchrony and the π-calculus (note). Rapport de Recherche  1702, INRIA, Sophia-Antipolis,
% Honda, K. and Tokoro, M. (1991).  An object calculus for asynchronous communication. In America, P., editor, Proceedings of the European Conference on Object-Oriented Programming (ECOOP), volume 512 of Lecture Notes in Computer Science, pages 133–147. Springer-Verlag)


% The asynchronous pi-calculus defined by Honda and Tokoro in 1991 led to Pict, a programming language\cite{Pierce2000}.




% There was firstly theories, and models for concurrent computation.
% The main problem was determinism.
% In a sequential machine, the non-determinism of the physical world is hidden by the sequentiality of the machine.
% However, in concurrent computation, the order of communication cannot be assured the way the order of statements is assured in a sequential machine.
% We observe local non-determinism.
% However, to conserve an apparent determinism, causal ordering is sufficient.



\subsection{Message passing concurrency} \label{chapter3:parallel-execution:message-passing}


The theory advocates asynchronous message-passing, but it doesn't precise the granularity of the actors.
In the Actor Model, everything is an actor, even the simplest types, like numbers.
In practice this level of asynchronous communication is unachievable due to communication overhead.
In practice, most implementations feature independent processes communicating by messages.

\subsubsection{Programming models}

Conway defines coroutines as an autonomous program which communicate with adjacent modules as if they were input and output subroutines.\cite{Conway1963}
It seems to be the first definition of a parallel pipeline.
\textit{When coroutines A and B are connected so that A sends items to B, B runs for a while until it encounters a a read command, which means it needs something from A. The control is then transfered to A until it wants to write, whereupon the control is returned to B at the point where it left off.}

Hoare presented the Communicating Sequential Processes (CSP) \cite{Hoare1978, Brookes1984}.
These processes are executed concurrently, and communicates events via named channels.
The evolutions of this model were influenced by, and influenced the work of Milner that led to $\pi$-calculus.

Similarly, Kahn developed the Kahn Networks \cite{Kahn1974, Kahn1976}, following the work of Conway on coroutines.
They are explicitly parallel coroutines separated by bounded FIFO streams for communication.

These programming models are highly similar, and differs only in details irrelevant for this thesis.
However, it is interesting to note that they don't correctly inherit from the Actor Model, as it is generally impossible to dynamically modify the topology of the application.
Coroutines and processes are defined statically in the source of the application.
We shall come back to this limitation later.

\subsubsection{Scalability law}

These programming model were applied to run programs concurrently in machines providing a single processor, or shared resources among processors, like a common memory store, or network interface.
To manage these resources, and avoid conflicting accesses, it is crucial to assure the mutual exclusion.
For this purpose, Djikstra introduced the Semaphore \cite{Dijkstra}.

Following this work, he also introduced guarded commands \cite{Dijkstra1975} and Hansen introduced guarded region \cite{Hansen1978a}.
Both assure the execution of a set of instructions to be exclusive to only one process.

Hoare introduced the monitor following the work of Hansen \cite{Hoare1974}.
A monitor is an extension of a class, it regroups data and procedures, except that it assures its procedures to be entered only once at a time.
With this restrictions, it guards against race condition on the access of a shared resource.
Modula \cite{Wirth1977} and Concurrent Pascal \cite{Hansen1975} uses Monitors.

\paragraph{Multi-threading}

Multi-threading programming extensively uses these concepts, because of the preemptive scheduling and the common storage.
When executed in a parallel architecture, we say it follows the Multiple Instruction Multiple Data (MIMD) paradigm.
Preemptive scheduling is known to lead to bad performances, and difficulties in the development \cite{Adya2002}.
\nt{This paragraph is too short, and not well structured. Introduce Fibers ?}

\paragraph{PGAS}

Another approach to parallelism is the Single Program Multiple Data (SPMD) paradigm.
As an example, is the Partitioned Global Address Space (PGAS) model.
It has the advantage of providing to the developer a uniform access to a distributed memory space.
Each computing node executes the same program, and provide its local memory to be shared with all the other nodes.
The PGAS programming model assure the remote accesses and synchronization of memory across nodes, and enforces locality of reference, to reduce the communication overhead.
% This model is a SPMD : Single Program Multiple Data.
Known implementation of the PGAS model are 
Chapel\cite{Chamberlain2007},
X10 \cite{Charles2005}.
Unified Parallel C \cite{El-Ghazawi2006},
CoArray Fortran \cite{Numrich1998},
OpenSHMEM \cite{Chapman2010}.

Amdahl warned against sequential portion of a program impacting the performance gained with parallelism \cite{Amdahl1967}.
Ghunter extended Amdahl's law to show that sharing resources, even protected, leads to bad, and even decreases performances with increasing parallelism \cite{Gustafson1988,Gunther1993,Gunther1996,Nelson1996,Gunther2002}.
If portions of program in Multi-threading and PGAS are sequential, that is, if they share resources, it implies to defer the execution until the resource becomes available.
This wait impacts performances negatively.
Therefore, isolating the parallel processes in such way to limit the communication to the minimum is a better way to achieve efficient parallelism.
It is important the processes to be independent, and communicate solely by messages.


\subsubsection{Programming languages}

% Scala / Akka / Erlang

Some programming languages features message-passing and isolation of actors directly.
To some extent, these languages succeeded in industrial contexts.
However, they largely remain elitist solutions for specific problems more than a general, and accessible tool.

Scala is an attempt at unifying the object model and functional programming \cite{Odersky2004}.
It proposes an actor approach in its design.
Akka\ftnt{http://akka.io/} is a framework based on Scala, to build higly scalable and resilient applications.

Erlang is a functional concurrent language designed by Ericsson to operate telecommunication devices \cite{JoeArmstrong}.
\nt{TODO extends these paragraphs}

\subsection{Stream Processing Systems} \label{chapter3:parallel-execution:stream-processing}

All the solutions previously presented are generally designed to build distributed systems.
We focus on real-time applications as defined by \cite{Hansen1978}.
A real-time application must respond to a variety of simultaneous requests within a certain time.
Otherwise, input data may be lost or output data may lose their significance.
Such applications nowadays are often connected to the internet, which implies to process high volumes streams of requests with a constrained latency.
Moreover, because these systems are key to business, their reliability is of critical importance.
These requirements are challenging to meet in the design of such system.
We present the state of the art.


% \textit{
% From a language designer's point of view, real-time
% programs have these characteristics:
% \begin{enumerate}
% \item A real-time program interacts with an environ-
% ment in which many things happen simultaneously at
% high speeds.
% \item A real-time program must respond to a variety
% of nondeterministic requests from its environment. The
% program cannot predict the order in which these requests
% will be made but must respond to them within certain
% time limits. Otherwise, input data may be lost or output
% data may lose their significance.
% \item A real-time program controls a computer with a
% fixed configuration of processors and peripherals and
% performs (in most cases) a fLxed number of concurrent
% tasks in its environment.
% \item A real-time program never terminates but contin-
% ues to serve its environment as long as the computer
% works. (The occasional need to stop a real-time program,
% say at the end of an experiment, can be handled by ad
% hoc mechanisms, such as turning the machine off or
% loading another program into it.)
% \end{enumerate}
% }

\subsubsection{Data-stream management systems}

The processing of large volume of data is usually handled by Database management systems.
It is naturally that these systems evolved to manage data-streams as well.
These systems continuously run on data streams request written in an SQL-like language.
Among the early works, we can cite NiagaraCQ \cite{Chen2000,Naughton2001}, Aurora \cite{Abadi2003,Abadi2003a,Balakrishnan2004} which evolved into Borealis \cite{Abadi2005}, AQuery \cite{Lerner2003}, STREAM \cite{Arasu2003,Arasu2005} and TelegraphCQ \cite{Krishnamurthy2003,Chandrasekaran2003}.
More recently, we can cite DryadLINQ \cite{Isard2007,Yu2009}, Timestream \cite{Qian2013} and Shark \cite{Xin2013}.


% SQL-like
%   AQuery \cite{Lerner2003}
%   STREAM (uses CQL) \cite{Arasu2003,Arasu2005}
%   TelegraphCQ (uses StreaQuel) \cite{Krishnamurthy2003,Chandrasekaran2003}
%   Grape / Timestream - distributed SQL (roughly) \cite{Qian2013}
%   Shark        Stateless dataflow \cite{Xin2013}

%   DryadLINQ    Stateless dataflow \cite{Isard2007,Yu2009}

% \subsubsection{Batched dataflow}

% Map/Reduce
%   MapReduce    Stateless dataflow \cite{Dean2008}
%   Hadoop       Stateless dataflow 
%   Incoop       Incremental dataflow \cite{Bhatotia2011}

% Functional
%   Comet        Batched dataflow \cite{He2010}
%   D-Streams    Batched dataflow \cite{Zaharia2012}
%   Spark        Stateless dataflow \cite{Zaharia,Zaharia2010}
%   Nectar       Incremental dataflow \cite{Gunda2010}



\subsubsection{Dataflow pipeline}

SEDA is a precursor in the design of pipeline-based architecture for real-time applications for the internet \cite{Welsh2001}.
It organizes an application as a network of event-driven stages connected by explicit queues.
Several projects followed and adapted the principles in this work.

StreaMIT is a language to help the programming of large streaming application \cite{Thies2002}.

Storm \cite{Toshniwal2014} is designed by and used at Twitter to process the flow of tweets, and calculate metrics such as the trending topics.
It is only one example of industrial practical application, among many others.
We can cite CBP \cite{Logothetis2010} and S4 \cite{Neumeyer2010}, that were designed at Yahoo, Millwheel \cite{Akidau2013} designed at Google and Naiad \cite{Murray2013} designed at Microsoft.

% Dataflow
%   CBP          Incremental dataflow \cite{Logothetis2010}
%   S4           Continuous dataflow \cite{Neumeyer2010}
%   Storm        Continuous dataflow \cite{Toshniwal2014}
%   Millwheel    Continuous dataflow \cite{Akidau2013}
%   SEEP         Continuous dataflow \cite{Fernandez2013}
%   Naiad        Batched dataflow \cite{Murray2013}

