\section{Parallel execution} \label{chapter3:parallel-execution}

Programming started with a very sequential nature, as we saw in the last chapter.
But it eventually evolved toward concurrency to make advantage of the parallel architecture.

The first models of computation, like the Turing machine and lambda-calculus, were inherently sequential and based on a global state.
A formalism was lacking to represent concurrent computations.
We present the most important works on formalisms for parallel computation.
They first tackled the problems of determinacy, communication and state synchronization.
The answer to this problems seems to lie in a formalism based on a network of concurrent processes, asynchronously communicating via messages.
We present the works on the programming models based on this formalism.
Recently, with the need of performance from the web to process stream of requests, we see huge improvements in the field of distributed stream processing.

\subsection{Concurrency Theory} \label{chapter3:parallel-execution:concurrency-theory}

The mathematical models are a ground for all following work on concurrent programming, we briefly explain them in the next paragraphs.
There is two main formal models for concurrent computations.
The Actor Model of C. Hewitt, the Pi-calculus of R. Milner and the Communicating Sequential Processes of T. Hoare.
Based on these definition, we explain the importance of determinism, and the reason that asynchronous message-passing prevailed.


\subsubsection{Models}

\paragraph{Actor Model}

The Actor model allows to express the computation as a set of communicating actors \cite{Hewitt1973a, Hewitt1977, Clinger1981}.
In reaction to a received message, an actor can create actors, send messages, and choose how to respond to the next message.
All actors are executed concurrently, and communicate asynchronously.

The Actor model was presented as a highly parallel programming model, but intended for Artificial Intelligence purposes.
Its success spread way out of this first scope, and it became a general reference on message passing parallel programming.
For example, the Scala language advertises its use of an actor approach of concurrency.

% More recent work of C. Hewitt on Actors is about ... \nt{TODO} \cite{Hewitt2007,Hewitt2007a}.

The Actor model uses an asynchronous message-passing communication paradigm.
The communication between two actors, the sender and the receiver, is a stream of discrete messages.
% The sender names the receiver actor when sending messages to be the recipient of these messages.
It is asynchronous because, contrary to invocation, the sender doesn't wait for the result of the initiated communication.

\paragraph{$\Pi$-calculus}

R. Milner presented a process calculus to describe concurrent computation : the Calculus of Communicating Systems (CCS) \cite{Milner1975, Milner1980}.
It is an algebraic notation to express identified processes communicating through synchronous labeled channels.
% In CCS, process compose concurrently, communications are synchronous, and the topology is static.
The Pi-calculus improved upon this earlier work to allow processes to be communicated as values, hence to become mobile \cite{Engberg1986,Milner1992a,Milner1992}.
Similarly to Actors, in Pi-calculus processes can dynamically modify the topology.
However, contrary to the Actor model, communications in Pi-calculus are based on simultaneous execution of complementary actions, they are synchronous.


% Actors can create actors, pi-caclulys processes can replicate, and send processes through channel.
% Processes create a new processes on each instruction to continue the execution.

% Pi-calculus resembles to the actor model, but its algebraic nature led to a critical difference with the latter.
% Indeed, processes in the Pi-calculus communicate indirectly, through labeled ports, whereas actors communicate directly by naming the recipient actors.
% This difference allows multiple processes to listen in turns to the same channel, whereas the recipient of a message cannot change.

% I think this difference lead the Pi-calculus to be composable, whereas message-passing is not.
% Message-passing is not composable, whereas invocation is.
% The Actor model is not an ideal programming model, as non-composability makes difficult to reuse or extends existing components.
% A way to compose actors, is to send to an actor the name of the actor to respond to.
% It is similar in essence to the continuation concept.



\subsubsection{Determinism and Non-determinism}

% All these early work adopted concurrent composition by default, instead of sequential composition, to adapt to the very concurrent nature of real parallel machines.
% However, sequential programming is still the default.
% Concurrent composition is yet still to be widely accepted, as stated by Reed \cite{Reed2012}.
% \comment{TODO rewrite this paragraph}

The Actor Model uses asynchronous communications, while $\pi$-calculus uses synchronous communications.
Synchronous communications are deterministic.
The message sent needs to be received to continue the execution on both ends.
Because the concurrent executions and the communications in such system are both deterministic, the result of the concurrent system is assured to be deterministic.
Determinism is a wanted property to assure the correctness of the execution.

On the other hand, asynchronous communications are non-deterministic.
The message sent can take an infinite time to be received.
Therefore, the result of the concurrent system is not assured to be deterministic.

But the communication in reality are subject to various fault and attacks, called Byzantin fault \cite{Lamport1982}.
It makes the real communications means unable to provide the determinism required by the deterministic models.
The Actor model, on the other hands, was explicitly designed to take physical limitations in account \cite{Hewitt1977a}.
Eventually, All these works evolved to adopt asynchronous communications.
Indeed, it is not realistic to build a distributed system based on synchronous communications.

Moreover, the total ordering of messages is only local to an actor, while between actors, messages are causally ordered.
As Lamport showed \cite{Lamport1978}, and Reed related later \cite{Reed2012}, causal order is sufficient to build a correct distributed system.
The non-determinism in communications is hidden by the organization of the system.
The execution will either terminate correctly, or not terminate at all.




% Asynchronous communications are less expressive than synchronous ones \cite{PALAMIDESSI2003}.

% Pi-calculus is a synchronous paradigm which contains an asynchronous fragment.\cite{PALAMIDESSI2003}
% (Boudol, G. (1992). Asynchrony and the π-calculus (note). Rapport de Recherche  1702, INRIA, Sophia-Antipolis,
% Honda, K. and Tokoro, M. (1991).  An object calculus for asynchronous communication. In America, P., editor, Proceedings of the European Conference on Object-Oriented Programming (ECOOP), volume 512 of Lecture Notes in Computer Science, pages 133–147. Springer-Verlag)


% The asynchronous pi-calculus defined by Honda and Tokoro in 1991 led to Pict, a programming language\cite{Pierce2000}.




% There was firstly theories, and models for concurrent computation.
% The main problem was determinism.
% In a sequential machine, the non-determinism of the physical world is hidden by the sequentiality of the machine.
% However, in concurrent computation, the order of communication cannot be assured the way the order of statements is assured in a sequential machine.
% We observe local non-determinism.
% However, to conserve an apparent determinism, causal ordering is sufficient.



\subsection{Message passing concurrency} \label{chapter3:parallel-execution:message-passing}

Alternatively, as shown by Ghunter, the scalability of an application comes from its parallelism.
So the synchronization on any shared resources adds contention.
Therefore, the only way to do an efficient concurrent application is with true parallel processes communicating by messages.





Among the model to develop concurrent application with message passing there is many many many solutions.
The earliest ones are for example Distributed Processes, Kahn Networks, Coroutines.

Cite as well stuffs like Akka Scala, Erlang, and so on.

\subsection{Stream Processing} \label{chapter3:parallel-execution:stream-processing}

Nowadays, we see more industrial platforms for stream processing.
See all the good stuffs at the end of this file.










\endinput

% The following is all very interesting, but I need to integrate it into a more global argumentation.
% Now that I have more clues about everything, I can start tracing this argumentation.

\subsection{Concurrency Theory}





% For more information, see : https://en.wikipedia.org/wiki/Actor_model_and_process_calculi_history

\subsubsection{Actor Model}

% TODO illustration of cells, and draw an analogy between cells and actor model.

The Actor model allows to express the computation as a set of communicating actors \cite{Hewitt1973a, Hewitt1977, Clinger1981}.
In reaction to a received message, an actor can create actors, send messages, and choose how to respond to the next message.
All actors are executed concurrently, and communicate asynchronously.

The Actor model was presented as a highly parallel programming model, but intended for Artificial Intelligence purposes.
Its success spread way out of this first scope, and it became a general reference on message passing parallel programming.
For example, the Scala language advertises its use of an actor approach of concurrency.

% More recent work of C. Hewitt on Actors is about \nt{TODO} \cite{Hewitt2007,Hewitt2007a}.

The Actor model used the message-passing communication paradigm.
The communication between two actors, the sender and the receiver, is a stream of discrete messages.
The sender names the receiver actor when sending messages to be the recipient of these messages.

Moreover, message-passing is asynchronous, because contrary to invocation, the sender doesn't wait for the result of the initiated communication.
These asynchronous communication make the Actor Model non-deterministic.
A message can take any time to be delivered, therefore no order can be assured in the messages sent between actors.
The total ordering of messages is only local to an actor, while between actors, messages are causally ordered.
As Lamport showed \cite{Lamport1978}, and Reed related later \cite{Reed2012}, causal order is sufficient to build a correct distributed system.

\subsubsection{Pi-calculus}

R. Milner presented a process calculus to describe concurrent computation : the Calculus of Communicating Systems (CCS) \cite{Milner1975, Milner1980}.
It is an algebraic notation to express identified processes communicating through labeled channels.
In CCS, process compose concurrently, communications are synchronous, and the topology is static.
The Pi-calculus improved upon this earlier work to allow processes to be communicated as values, hence to become mobile \cite{Engberg1986,Milner1992a,Milner1992}.
Contrary to CCS, in Pi-calculus processes can replicate and send process through channel, allowing dynamic modification of the topology.

Pi-calculus resembles to the actor model, but its algebraic nature led to a critical difference with the latter.
Indeed, processes in the Pi-calculus communicate indirectly, through labeled ports, whereas actors communicate directly by naming the recipient actors.
This difference allows multiple processes to listen in turns to the same channel, whereas the recipient of a message cannot change.

I think this difference lead the Pi-calculus to be composable, whereas message-passing is not.
Message-passing is not composable, whereas invocation is.
The Actor model is not an ideal programming model, as non-composability makes difficult to reuse or extends existing components.
A way to compose actors, is to send to an actor the name of the actor to respond to.
It is similar in essence to the continuation concept.

Communications in Pi-calculus are based on handshaking : simultaneous execution of complementary actions.


Asynchronous communications are less expressive than synchronous ones \cite{PALAMIDESSI2003}.

Pi-calculus is a synchronous paradigm which contains an asynchronous fragment.\cite{PALAMIDESSI2003}
(Boudol, G. (1992). Asynchrony and the π-calculus (note). Rapport de Recherche  1702, INRIA, Sophia-Antipolis,
Honda, K. and Tokoro, M. (1991).  An object calculus for asynchronous communication. In America, P., editor, Proceedings of the European Conference on Object-Oriented Programming (ECOOP), volume 512 of Lecture Notes in Computer Science, pages 133–147. Springer-Verlag)


The asynchronous pi-calculus defined by Honda and Tokoro in 1991 led to Pict, a programming language\cite{Pierce2000}.

\subsubsection{Concurrency, asynchronism and unbounded nondeterminacy}

All these early work adopted concurrent composition by default, instead of sequential composition, to adapt to the very concurrent nature of real parallel machines.
However, sequential programming is still the default.
Concurrent composition is yet still to be widely accepted, as stated by Reed \cite{Reed2012}.
\comment{TODO rewrite this paragraph}

All these works eventually evolved to adopt asynchronous communications.
Indeed, it is not realistic to build a distributed system based on synchronous communications. \nt{TODO reference needed}
By abandoning synchronous communication, such system also needs to abandon determinism.
It becomes non-deterministic because communications can take infinite time to complete.


Input-guarded choice allows to wait on two or more input channels.
It is basically an epoll or select.
It is implemented by CSP.



% \endinput


TODO rewrite this subsection.
No subsubsection with name of model.
I want to say that non-determinacy is ineluctable, and a good thing, as it is what happen in reality.
More specifically, I want to say that causal ordering is good enough (See Lamport78 and Reed12).
I can also say more about the advantages of determinism and non-determinism, but for the moment, I don't know much about that.

I think there is several definitions for determinism.
There might be a non determinism in the order of messages, but for an external observer, the application is still deterministic because of causal ordering.

However, this determinism doesn't account for network and other Byzantines failures.
TODO explain the byzantines failures.


TODO read Communication and Concurrency by Milner 1989
And A theory of synchrony and asynchrony by He 1990


We need to wait a bit to see state synchronization outside of message-passing. \nt{TODO}


\subsection{Programming models}

\subsubsection{Distributed Processes}

Distributed processes: a concurrent programming concept \cite{Hansen1978}
This paper defines real-time applications.
It proposes a unification between monitor and processes to apply on distributed storage.
The distributed processes are basically event-loops with cooperative scheduling.
However, the communication is done via procedure calling, not message-passing.
A process defines common procedures, callable by other processes.
Processes have access to guarded commands, and guarded regions.
A guarded region (cycle, when) can make the execution wait, the guarded command (if, do) not.
(That is how is )

Mutual exclusion (Guarded region) \cite{Hansen1978a} \cite{Hoare2002}
Guarded commands \cite{Dijkstra1975}

\subsubsection{Communication Sequential Processes}

Read this : http://www.springer.com/computer/theoretical+computer+science/foundations+of+computations/book/978-3-540-25813-1

CSP is similar to Distributed Processes, it seems :
\textit{Both proposals [DP and CSP] attack the problem of concurrency without shared variables and recognize the need for nondeterminacy within a single process.}
That is, they acknowledged that non-determinacy is a requirement to have good performances (to rewrite), and that with causal ordering (synchronization with guarded region) the application can be made somehow deterministic.

C. A. R. Hoare presented Communicating Sequential Processes (CSP) \cite{Hoare1978, Brookes1984}.
In CSP, processes are executed concurrently, and communicates events via channels.
The evolutions of this model were influenced by, and influenced the work of R. Milner, hence CCS and CSP are highly similar.
With the evolutions appeared the problems of determinism in distributed systems\nt{TODO}.\cite{Brookes1984}


Modula and Concurrent Pascal uses Monitors, but they are intended for computer with a common storage, or a single processor.


Monitors: an operating system structuring concept \cite{Hoare1974}
A monitor is the association of data and procedures.
It assures that procedures are entered only one at a time (two programs cannot call the same procedure at the same time).
Local variables are not accessible from outside, and monitors procedures shall not access variables outside of the monitor.
With this restrictions, it guards against race condition on the access of a shared resource.

Monitors are somehow equivalent to semaphore, but are more expressive.
In the paper, Hoare says Monitor are to Semaphore, what coroutine or while loop are to jumps.


\subsubsection{Coroutines}

Conway defines coroutines as subroutines executing all at the same level.\cite{Conway1963}

\textit{A coroutine is an autonomous program which communicate with adjacent modules as if they were input and output subroutines.}

It defines the separability of a program, and advocates that coroutines are separate program and communicate by sending discrete items.
The first definition of coroutine, explicitly specify that coroutines are best used when they can be executed, and communicate asynchronously.

There is three conditions for coroutine
\textit{(1) The only communication between modules is in the form of discrete items of information; (2) the flow of each of this items is along fixed, one-way paths; (3) the entire program can be laid out so that the input is at the left extreme, the output is at the right extreme, and everywhere in between all information items flowing between modules have a component of motion to the right.}
\textit{When coroutines A and B are connected so that A sends items to B, B runs for a while until it encounters a a read command, which means it needs something from A. The control is then transfered to A until it wants to write, whereupon the control is returned to B at the point where it left off.}

Coroutines seems to be the first definition of a concurrent pipeline.
Coroutines can be executed simultaneously if parallel hardware is available.

\subsubsection{Kahn Networks}

Kahn Networks are basically parallel coroutines separated by bounded FIFO streams.



Following work : Coroutines and Networks of Parallel Processes\cite{Kahn1976}

The coroutines defines the first pipeline organization of a program.

\subsubsection{Modules}

Multiprogramming designs the ability to program concurrent activities on multiple processing units.

Modula: A language for modular multiprogramming \cite{Wirth1977}

Modula is a descendent of Pascal with the addition of modules, processes and signals.


\subsubsection{Promises and Futures}

This \cite{Jr1977} is the reference paper for futures.

\subsubsection{Functional Reactive Programming}
\nt{I don't know exactly what to do with this.
It is not exactly aimed at concurrency, but it is definitly not oriented on improving software growth}

Maybe I can put it with coroutine, as it seems quite similar.

But it should probably go into the reconciliation section.

\subsubsection{Flow programming}
Morrison
Noflo

\subsubsection{Data flow}

I saw this reference earlier, but don't remember where. \cite{Kahn1974}
I might relate it if I read it.

\subsubsection{SIMD / SPMD / MIMD / MPMD}

\subsubsection{Partitioned Global Address Space (PGAS)}
OpenSHMEM, UPC, CAF, Chapel


\subsubsection{Task-based parallelism}
X10 (is an APGAS), OCR, Habanero, Legion, Charm++, HPX

\subsubsection{Message-based parralelism}
Scala, Akka, Play

\subsubsection{Directive-based languages}
OpenMP, OpenACC





\subsection{Frameworks, Languages and runtimes}

\subsubsection{Stream Processing}

SEDA

StreaMIT

CANS Cluster-based scalable network services

SQL-like
  Grape / Timestream - distributed SQL (roughly)
  CQL
  STREAM (uses CQL)
  StreaQuel
  TelegraphCQ
  AQuery

Map/Reduce
  MapReduce    Stateless dataflow
  Hadoop       Stateless dataflow
  Incoop       Incremental dataflow

Functional
  DryadLINQ    Stateless dataflow
  Spark        Stateless dataflow
  Nectar       Incremental dataflow
  Comet        Batched dataflow
  D-Streams    Batched dataflow

Dataflow
  CBP          Incremental dataflow
  Naiad        Batched dataflow
  Storm, S4    Continuous dataflow
  SEEP         Continuous dataflow

Imperative
  CIEL         Stateless dataflow
  SDG          Stateful dataflow
  Piccolo      Parallel in-memory