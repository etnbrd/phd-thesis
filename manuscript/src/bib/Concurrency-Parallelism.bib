Automatically generated by Mendeley Desktop 1.16-dev1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Moore1965,
author = {Moore, G},
file = {:home/etn/Documents/PhD/Biblio/Moore - 1965 - Cramming More Components Onto Integrated Circuits.pdf:pdf},
journal = {Electronics},
number = {38},
pages = {8},
title = {{Cramming More Components Onto Integrated Circuits}},
url = {https://scholar.google.com/scholar?q=cramming+more+components+1965{\&}btnG={\&}hl=en{\&}as{\_}sdt=0{\%}2C39{\#}5},
year = {1965}
}
@article{Karp1969,
abstract = {This paper introduces a model called the parallel program schema for the representation and study of programs containing parallel sequencing. The model is related to Ianov's program schema, but extends it, both by modelling memory structure in more detail and by admitting parallel computation. The emphasis is on decision procedures, both for traditional properties, such as equivalence, and for new properties particular to parallel computation, such as determinacy and boundedness.},
author = {Karp, Richard M. and Miller, Raymond E.},
doi = {10.1016/S0022-0000(69)80011-5},
file = {:home/etn/Documents/PhD/Biblio/Karp, Miller - 1969 - Parallel program schemata.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
month = {may},
number = {2},
pages = {147--195},
title = {{Parallel program schemata}},
url = {http://www.sciencedirect.com/science/article/pii/S0022000069800115},
volume = {3},
year = {1969}
}
@article{Wirth1977,
author = {Wirth, N.},
doi = {10.1002/spe.4380070102},
file = {:home/etn/Documents/PhD/Biblio/Wirth - 1977 - Modula A language for modular multiprogramming.pdf:pdf},
issn = {00380644},
journal = {Software: Practice and Experience},
month = {jan},
number = {1},
pages = {1--35},
title = {{Modula: A language for modular multiprogramming}},
url = {http://doi.wiley.com/10.1002/spe.4380070102},
volume = {7},
year = {1977}
}
@incollection{Hoare2002,
abstract = {The objectives in the construction of a theory of parallel programming as a basis for a high-level programming language feature are: Security from error. In many of the applications of parallel programming the cost of programming error is very high, often inhibiting the use of computers in environments for which they would otherwise be highly suitable. Parallel programs are particularly prone to time-dependent errors, which either cannot be detected by program testing nor by run-time checks. It is therefore very important that a high-level language designed for this purpose should provide complete security against time-dependent errors by means of a compile-time check. Efficiency. The spread of real-time computer applications is severely limited by computing costs; and in particular by the cost of main store. If a feature to assist in parallel programming is to be added to a language used for this purpose, it must not entail any noticeable extra run-time overhead in space or speed, neither on programs which use the feature heavily, nor on programs which do not; efficient implementation should be possible on a variety of hardware designs, both simple and complex; and there should be no need for bulky or slow compilers. Conceptual simplicity.},
author = {Hoare, C. a. R.},
booktitle = {The origin of concurrent programming},
isbn = {978-1-4419-2986-0, 978-1-4757-3472-0},
issn = {<null>},
pages = {231--244},
title = {{Towards a Theory of Parallel Programming}},
url = {http://link.springer.com/chapter/10.1007/978-1-4757-3472-0{\_}6 http://link.springer.com/chapter/10.1007/978-1-4757-3472-0{\_}6$\backslash$nhttp://link.springer.com/chapter/10.1007/978-1-4757-3472-0{\_}6{\#}page-1},
year = {2002}
}
@article{Dijkstra1975,
author = {Dijkstra, Edsger W.},
doi = {10.1145/360933.360975},
file = {:home/etn/Documents/PhD/Biblio/Dijkstra - 1975 - Guarded commands, nondeterminacy and formal derivation of programs.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {case-construction,correctness proof,derivation of programs,nondeterminancy,program semantics,programming language semantics,programming languages,programming methodology,repetition,sequencing primitives,termination},
month = {aug},
number = {8},
pages = {453--457},
publisher = {ACM},
title = {{Guarded commands, nondeterminacy and formal derivation of programs}},
url = {http://dl.acm.org/citation.cfm?id=360933.360975},
volume = {18},
year = {1975}
}
@article{Flynn1972,
author = {Flynn, Michael J.},
doi = {10.1109/TC.1972.5009071},
file = {:home/etn/Documents/PhD/Biblio/Flynn - 1972 - Some Computer Organizations and Their Effectiveness.pdf:pdf},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Automata,Computer aided instruction,Computer organization,Concurrent computing,Parallel processing,Performance evaluation,Time sharing computer systems,Transmission electron microscopy,instruction stream,overlapped,parallel processors,resource hierarchy},
language = {English},
month = {sep},
number = {9},
pages = {948--960},
publisher = {IEEE},
title = {{Some Computer Organizations and Their Effectiveness}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5009071},
volume = {C-21},
year = {1972}
}
@article{Stone2010,
abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
author = {Stone, John E. and Gohara, David and Shi, Guochun},
doi = {10.1109/MCSE.2010.69},
file = {:home/etn/Documents/PhD/Biblio/Stone, Gohara, Shi - 2010 - OpenCL A Parallel Programming Standard for Heterogeneous Computing Systems.pdf:pdf},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
month = {may},
number = {3},
pages = {66--73},
publisher = {AIP Publishing},
title = {{OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems}},
url = {http://scitation.aip.org/content/aip/journal/cise/12/3/10.1109/MCSE.2010.69},
volume = {12},
year = {2010}
}
@article{Dennard2007,
abstract = {This paper considers the design, fabrication, and characterization of very small MOSFET switching devices suitable for digital integrated circuits using dimensions of the order of 1 mu. Scaling relationships are presented which show how a conventional MOSFET can be reduced in size. The performance improvement expected from using these very small devices in highly miniaturized integrated circuits is projected.},
author = {Dennard, Robert H. and Gaensslen, Fritz H. and Yu, Hwa-Nien and Rideovt, V. Leo and Bassous, Ernest and Leblanc, Andre R.},
doi = {10.1109/N-SSC.2007.4785543},
file = {:home/etn/Documents/PhD/Biblio/Dennard et al. - 2007 - Design of Ion-Implanted MOSFET's with Very Small Physical Dimensions.pdf:pdf},
isbn = {1098-4232 VO - 12},
issn = {1098-4232},
journal = {IEEE Solid-State Circuits Newsletter},
keywords = {Dennard scaling},
number = {1},
title = {{Design of Ion-Implanted MOSFET's with Very Small Physical Dimensions}},
url = {http://www.ece.ucsb.edu/courses/ECE225/225{\_}W07Banerjee/reference/Dennard.pdf},
volume = {12},
year = {2007}
}
@article{Bohr2007,
author = {Bohr, Mark},
doi = {10.1109/N-SSC.2007.4785534},
file = {:home/etn/Documents/PhD/Biblio/Bohr - 2007 - A 30 Year Retrospective on Dennard's MOSFET Scaling Paper.pdf:pdf},
issn = {1098-4232},
journal = {IEEE Solid-State Circuits Newsletter},
keywords = {Industries,Integrated circuit interconnections,Logic gates,MOSFET circuits,Silicon,Transistors,Voltage control},
language = {English},
month = {jan},
number = {1},
pages = {11--13},
publisher = {IEEE},
title = {{A 30 Year Retrospective on Dennard's MOSFET Scaling Paper}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4785534},
volume = {12},
year = {2007}
}
@book{Foster1995,
abstract = {From the Publisher: At last, a practitioner's guide to parallel programming! Students and professionals who use parallel or distributed computer systems will be able to solve real problems with Designing and Building Parallel Programs. This book provides a comprehensive introduction to parallel algorithm design, performance analysis, and program construction. It describes the tools needed to write parallel programs and provides numerous examples. A unique feature is the companion on-line version, accessible via the World Wide Web using browsers such as Mosaic. This provides a convenient hypertext version of the text with pointers to programming tools, example programs, and other resources on parallel and distributed computing.},
author = {Foster, Ian},
booktitle = {Interface},
file = {:home/etn/Documents/PhD/Biblio/Foster - 1995 - Designing and Building Parallel Programs.pdf:pdf},
isbn = {0201575949},
title = {{Designing and Building Parallel Programs}},
url = {http://www-rohan.sdsu.edu/faculty/mthomas/courses/docs/foster/Foster{\_}Designing{\_}and{\_}Building{\_}Parallel{\_}Programs.pdf},
year = {1995}
}
@article{Darema1988,
abstract = {We present a single-program-multiple-data computational model which we have implemented in the EPEX system to run in parallel mode FORTRAN scientific application programs. The computational model assumes a shared memory organization and is based on the scheme that all processes executing a program in parallel remain in existence for the entire execution; however, the tasks to be executed by each process are determined dynamically during execution by the use of appropriate synchronizing constructs that are imbedded in the program. We have demonstrated the applicability of the model in the parallelization of several applications. We discuss parallelization features of these applications and performance issues such as overhead, speedup, efficiency.},
author = {Darema, F. and George, D.A. and Norton, V.A. and Pfister, G.F.},
doi = {10.1016/0167-8191(88)90094-4},
issn = {01678191},
journal = {Parallel Computing},
keywords = {EPEX/FORTRAN,Shared memory multiprocessor,computational model,parallelization features},
month = {apr},
number = {1},
pages = {11--24},
title = {{A single-program-multiple-data computational model for EPEX/FORTRAN}},
url = {http://www.sciencedirect.com/science/article/pii/0167819188900944},
volume = {7},
year = {1988}
}
@inproceedings{Auguin1983,
author = {Auguin, Michel and Larbey, Francois},
booktitle = {Microcomputers: developments in industry, business, and education},
keywords = {SPMD},
mendeley-tags = {SPMD},
pages = {311--318},
title = {{OPSILA: an advanced SIMD for numerical analysis and signal processing}},
url = {https://scholar.google.com/scholar?q=OPSILA {\%}3A an advanced SIMD for numerical analysis and signal processing{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}0},
year = {1983}
}
@incollection{Darema2001,
abstract = {I proposed the SPMD (Single Program Multiple Data) model, in January 19841, as a means for enabling parallel execution of applications on multiprocessors, and in particular for highly parallel machines like the RP3 (the IBM Research Parallel Processor Prototype2). This talk will provide a review of the origins of the SPMD, it’s early use in enabling parallel execution of scientific applications and it’s implementation in one of the first3 parallel programming environments. In fact [3] was the first programming environment that implemented the SPMD model; other environments in the 1985 timeframe were based on the fork-and-join (F{\&}J) model.},
author = {Darema, Frederica},
booktitle = {Parallel Computing},
doi = {10.1007/3-540-45417-9{\_}1},
isbn = {978-3-540-42609-7},
pages = {1},
title = {{The SPMD Model: Past , Present and Future}},
year = {2001}
}
@article{Chan2004,
abstract = {Many parallel applications involve different independent tasks$\backslash$nwith their own data. Using the MPMD model, programmers can have$\backslash$na modular view and simplified structure of the parallel$\backslash$nprograms. Although MPI supports both SPMD and MPMD models for$\backslash$nprogramming, MPI libraries do not provide an efficient way for$\backslash$ntask communication for the MPMD model. We have developed a$\backslash$nprogramming environment, called ClusterGOP, for building and$\backslash$ndeveloping parallel applications. Based on the graph-oriented$\backslash$nprogramming (GOP) model, ClusterGOPprovides higher-level$\backslash$nabstractions for message-passing parallel programming with the$\backslash$nsupport of software tools for developing and running parallel$\backslash$napplications. In this paper, we describe how ClusterGOP supports$\backslash$nprogramming of MPMD parallel applications on top of MPI. We$\backslash$ndiscuss the issues of implementing the MPMD model in ClusterGOP$\backslash$nusing MPI and evaluate the performanceby using example$\backslash$napplications.},
author = {Chan, F and Cao, J N and Chan, A T S and Guo, M Y},
file = {:home/etn/Documents/PhD/Biblio/Chan et al. - 2004 - Programming support for MPMD parallel computing in ClusterGOP.pdf:pdf},
journal = {IEICE Transactions on Information and Systems},
number = {7},
pages = {1693--1702},
title = {{Programming support for MPMD parallel computing in ClusterGOP}},
url = {http://www.cs.sjtu.edu.cn/{~}guo-my/PDF/Journals/J43.pdf},
volume = {E87D},
year = {2004}
}
@book{Snir1996,
author = {Snir, O and Huss-Lederman, W and Dongarra, M P I},
isbn = {9780071598422},
title = {{MPI: The Complete Reference}},
url = {https://scholar.google.com/scholar?q=MPI{\%}3A the complete referen{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}3 https://scholar.google.com/scholar?q=MPI{\%}3A the complete referen{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}1},
year = {1996}
}
@article{Chang1997,
abstract = {The MPMD approach for parallel computing is attractive for programmers who seek fast development cycles, high code re-use, and modular programming, or whose applications exhibit irregular computation loads and communication patterns. RPC is widely adopted as the communication abstraction for crossing address space boundaries. However, the communication overheads of existing RPC-based systems are usually an order of magnitude higher than those found in highly tuned SPMD systems. This problem has thus far limited the appeal of high-level programming languages based on MPMD models in the parallel computing community. This paper investigates the fundamental limitations of MPMD communication using a case study of two parallel programming languages, Compositional C++ (CC++) and Split-C, that provide support for a global name space. To establish a common comparison basis, our implementation of CC++ was developed to use MRPC, a RPC system optimized for MPMD parallel computing and based on Active Messages. Basic RPC performance in CC++ is within a factor of two from those of Split-C and other messaging layers. CC++ applications perform within a factor of two to six from comparable Split-C versions, which represent an order of magnitude improvement over previous CC++ implementations. The results suggest that RPC-based communication can be used effectively in many high-performance MPMD parallel applications.},
author = {Chang, Chi-Chao and Czajkowski, G. and Eicken, T. Von and Kesselman, C.},
doi = {10.1109/SC.1997.10040},
file = {:home/etn/Documents/PhD/Biblio/Chang et al. - 1997 - Evaluating the Performance Limitations of MPMD Communication.pdf:pdf},
isbn = {0-89791-985-8},
journal = {ACM/IEEE SC 1997 Conference (SC'97)},
pages = {1--10},
title = {{Evaluating the Performance Limitations of MPMD Communication}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.8068{\&}rep=rep1{\&}type=pdf},
year = {1997}
}
@incollection{K.ManiChandy2005,
abstract = {There is a class of sparse matrix computations, such as direct solvers of systems of linear equations, that change the fill-in (nonzero entries) of the coefficient matrix, and involve row and column operations (pivoting). This paper addresses the problem of the parallelization of these sparse computations from the point of view of the parallel language and the compiler. Dynamic data structures for sparse matrix storage are analyzed, permitting to efficiently deal with fill-in and pivoting issues. Any of the data representations considered enforces the handling of indirections for data accesses, pointer referencing and dynamic data creation. All of these elements go beyond current data-parallel compilation technology. We propose a small set of new extensions to HPF-2 to parallelize these codes, supporting part of the new capabilities on a runtime library. This approach has been evaluated on a Cray T3E, implementing, in particular, the sparse LU factorization.},
author = {Chandy, K. Mani and Kesselman, Carl},
booktitle = {Languages and Compilers for Parallel Computing},
doi = {10.1007/3-540-48319-5},
file = {:home/etn/Documents/PhD/Biblio/Chandy, Kesselman - 2005 - Compositional C Compositional parallel programming.pdf:pdf},
isbn = {978-3-540-66426-0},
issn = {0302-9743},
pages = {124--144},
title = {{Compositional C++: Compositional parallel programming}},
url = {http://authors.library.caltech.edu/26756/2/92-13.pdf http://www.springerlink.com/content/dd8uk1lp4pvmclta},
volume = {757},
year = {2005}
}
@article{Grimshaw1991,
author = {Grimshaw, Andrew S.},
month = {apr},
publisher = {University of Virginia},
title = {{An Introduction to Parallel Object-Oriented Programming with Mentat}},
url = {http://dl.acm.org/citation.cfm?id=900803},
year = {1991}
}
@article{Foster1995b,
abstract = {Fortran M is a small set of extensions to Fortran 77 that supports a modular approach to the design of message-passing programs. It has the following features. (1) Modularity. Programs are constructed by using explicitly-declared communication channels to plug together program modules called processes. A process can encapsulate common data, subprocesses, and internal communication. (2) Safety. Operations on channels are restricted so as to guarantee deterministic execution, even in dynamic computations that create and delete processes and channels. Channels are typed, so a compiler can check for correct usage. (3) Architecture Independence. The mapping of processes to processors can be specified with respect to a virtual computer with size and shape different from that of the target computer. Mapping is specified by annotations that influence performance but not correctness. (4) Efficiency. Fortran M can be compiled efficiently for uniprocessors, sharedmemory computers, distributed-memory computers, and networks of workstations. Because message passing is incorporated into the language, a compiler can optimize communication as well as computation.},
author = {Foster, I.T. and Chandy, K M},
doi = {10.1006/jpdc.1995.1044},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = {apr},
number = {1},
pages = {24--35},
title = {{Fortran M: A Language for Modular Parallel Programming}},
url = {http://www.sciencedirect.com/science/article/pii/S0743731585710441},
volume = {26},
year = {1995}
}
@article{Foster1996,
abstract = {Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems.},
author = {Foster, Ian and Kesselman, Carl and Tuecke, Steven},
doi = {10.1006/jpdc.1996.0108},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = {aug},
number = {1},
pages = {70--82},
title = {{The Nexus Approach to Integrating Multithreading and Communication}},
url = {http://www.sciencedirect.com/science/article/pii/S0743731596901082 http://linkinghub.elsevier.com/retrieve/pii/S0743731596901082},
volume = {37},
year = {1996}
}
@article{Culler,
author = {Culler, David E. and Dusseau, A. and Goldstein, Seth Copen and Krishnamurthy, Arvind and Lumetta, Steven and {Von Eicken}, Thorsten and Yelick, Katherine},
doi = {10.1109/SUPERC.1993.1263470},
file = {:home/etn/Documents/PhD/Biblio/Culler et al. - Unknown - Parallel programming in Split-C.pdf:pdf},
isbn = {0-8186-4340-4},
issn = {1063-9535},
keywords = {C language,Computer languages,Computer science,Cost function,Frequency,Message passing,Parallel processing,Parallel programming,Predictive models,Program processors,Signal processing,Split-C language,assignment operators,data parallel programming,distributed memory multiprocessors,global address space,high performance programming,language concepts,locality,message passing,parallel extension,parallel languages,parallel programming,performance results,program optimization,remote access,shared memory,software performance evaluation},
language = {English},
pages = {262--273},
publisher = {IEEE},
title = {{Parallel programming in Split-C}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1263470}
}
@article{Johnson1995,
author = {Johnson, Kirk L. and Kaashoek, M. Frans and Wallach, Deborah A.},
doi = {10.1145/224057.224073},
file = {:home/etn/Documents/PhD/Biblio/Johnson, Kaashoek, Wallach - 1995 - CRL High-Performance All-Software Distributed Shared Memory.pdf:pdf},
isbn = {0-89791-715-4},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
month = {dec},
number = {5},
pages = {213--226},
publisher = {ACM},
title = {{CRL: High-Performance All-Software Distributed Shared Memory}},
url = {http://dl.acm.org/citation.cfm?id=224057.224073},
volume = {29},
year = {1995}
}
@article{Walker1996,
author = {Walker, DW and Dongarra, JJ},
file = {:home/etn/Documents/PhD/Biblio/Walker, Dongarra - 1996 - MPI a standard message passing interface.ps:ps},
isbn = {0168-7875},
issn = {0168-7875},
journal = {Supercomputer},
pages = {56--68},
title = {{MPI: a standard message passing interface}},
url = {http://users.cs.cf.ac.uk/David.W.Walker/papers/supercomputer96.ps},
volume = {12},
year = {1996}
}
@article{Sunderam1994,
abstract = {The PVM system, a software framework for heterogeneous concurrent computing in networked environments, has evolved in the past several years into a viable technology for distributed and parallel processing in a variety of disciplines. PVM supports a straightforward but functionally complete message passing model, and is capable of harnessing the combined resources of typically heterogeneous networked computing platforms to deliver high levels of performance and functionality. In this paper, we describe the architecture of PVM system, and discuss its computing model, the programming interface it supports, auxiliary facilities for process groups and MPP support, and some of the internal implementation techniques employed. Performance issues, dealing primarily with communication overheads, are analyzed, and recent findings as well as experimental enhancements are presented. In order to demonstrate the viability of PVM for large scale scientific supercomputing, the paper includes representative case studies in materials science, environmental science, and climate modeling. We conclude with a discussion of related projects and future directions, and comment on near and long-term potential for network computing with the PVM system.},
author = {Sunderam, V.S and Geist, G.A and Dongarra, J and Manchek, R},
doi = {10.1016/0167-8191(94)90027-2},
file = {:home/etn/Documents/PhD/Biblio/Sunderam et al. - 1994 - The PVM concurrent computing system Evolution, experiences, and trends.pdf:pdf},
isbn = {0167-8191},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Case studies,Message passing,Networked computing platforms,PVM system,Performance},
month = {apr},
number = {4},
pages = {531--545},
title = {{The PVM concurrent computing system: Evolution, experiences, and trends}},
url = {http://www.sciencedirect.com/science/article/pii/0167819194900272},
volume = {20},
year = {1994}
}
@article{Bauer2012,
abstract = {Modern parallel architectures have both heterogeneous processors and deep, complex memory hierarchies. We present Legion, a programming model and runtime system for achieving high performance on these machines. Legion is organized around logical regions, which express both locality and independence of program data, and tasks, functions that perform computations on regions. We describe a runtime system that dynamically extracts parallelism from Legion programs, using a distributed, parallel scheduling algorithm that identifies both independent tasks and nested parallelism. Legion also enables explicit, programmer controlled movement of data through the memory hierarchy and placement of tasks based on locality information via a novel mapping interface. We evaluate our Legion implementation on three applications: fluid-flow on a regular grid, a three-level AMR code solving a heat diffusion equation, and a circuit simulation.},
author = {Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
doi = {10.1109/SC.2012.71},
file = {:home/etn/Documents/PhD/Biblio/Bauer et al. - 2012 - Legion Expressing Locality and Independence with Logical Regions.pdf:pdf},
isbn = {9781467308069},
issn = {21674329},
journal = {Proceedings of the International Conference on High Performance Computing Networking Storage and Analysis SC 12},
month = {nov},
pages = {1--11},
publisher = {IEEE Computer Society Press},
title = {{Legion: Expressing Locality and Independence with Logical Regions}},
url = {http://dl.acm.org/citation.cfm?id=2388996.2389086},
year = {2012}
}
@article{Slaughter2015,
author = {Slaughter, Elliott and Lee, Wonchan and Treichler, Sean and Bauer, Michael},
doi = {10.1145/2807591.2807629},
file = {:home/etn/Documents/PhD/Biblio/Slaughter et al. - 2015 - Regent A High-Productivity Programming Language for HPC with Logical Regions.pdf:pdf},
isbn = {9781450337236},
journal = {SC},
keywords = {legion,logical regions,regent,task-based runtimes},
title = {{Regent : A High-Productivity Programming Language for HPC with Logical Regions}},
url = {http://legion.stanford.edu/pdfs/regent2015.pdf},
year = {2015}
}
@misc{Kaiser2015,
author = {Kaiser, Hartmut and Heller, Thomas and Bourgeois, Daniel},
booktitle = {Proceedings of the First International Workshop on Extreme Scale Programming Models and Middleware - ESPM '15},
file = {:home/etn/Documents/PhD/Biblio/Kaiser, Heller, Bourgeois - 2015 - Higher-level Parallelization for Local and Distributed Asynchronous Task-Based Programming.pdf:pdf},
title = {{Higher-level Parallelization for Local and Distributed Asynchronous Task-Based Programming}},
url = {http://stellar.cct.lsu.edu/pubs/executors{\_}espm2{\_}2015.pdf},
urldate = {2015-11-24},
year = {2015}
}
@article{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine a},
doi = {10.1145/1562764.1562783},
file = {:home/etn/Documents/PhD/Biblio/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
isbn = {UCB/EECS-2006-183},
issn = {00010782},
journal = {EECS Department University of California Berkeley Tech Rep UCBEECS2006183},
pages = {19},
pmid = {8429457},
title = {{The Landscape of Parallel Computing Research : A View from Berkeley}},
url = {http://rrsg.ee.uct.ac.za/courses/EEE4084F/Archive/2014/Assignments/Reading Assignments/R01+Berkeley+2006+-+Landscale+of+Parallel+Computing+Research.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8705{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {18},
year = {2006}
}
