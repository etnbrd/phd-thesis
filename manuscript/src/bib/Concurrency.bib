Automatically generated by Mendeley Desktop 1.16-dev1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rotem-Gal-Oz2006,
author = {Rotem-Gal-Oz, A},
journal = {{\ldots} http://www. rgoarchitects. com/Files/fallacies. {\ldots}},
title = {{Fallacies of distributed computing explained}},
url = {http://www.rgoarchitects.com/Files/fallacies.pdf},
year = {2006}
}
@article{Zaharia2012,
abstract = {Many important "big data" applications need to process data arriving in real time. However, current programming models for distributed stream processing are relatively low-level, often leaving the user to worry about consistency of state across the system and fault recovery. Furthermore, the models that provide fault recovery do so in an expensive manner, requiring either hot replication or long recovery times. We propose a new programming model, discretized streams (D-Streams), that offers a high-level functional programming API, strong consistency, and efficient fault recovery. D-Streams support a new recovery mechanism that improves efficiency over the traditional replication and upstream backup solutions in streaming databases: parallel recovery of lost state across the cluster. We have prototyped D-Streams in an extension to the Spark cluster computing framework called Spark Streaming, which lets users seamlessly intermix streaming, batch and interactive queries.},
author = {Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Shenker, Scott and Stoica, Ion},
file = {:home/etn/Documents/PhD/Biblio/Zaharia et al. - 2012 - Discretized streams an efficient and fault-tolerant model for stream processing on large clusters.pdf:pdf},
journal = {Proceedings of the 4th USENIX conference on Hot Topics in Cloud Ccomputing},
pages = {10--10},
title = {{Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters}},
url = {https://www.usenix.org/system/files/conference/hotcloud12/hotcloud12-final28.pdf},
year = {2012}
}
@article{Qian2013,
author = {Qian, Z and He, Y and Su, C and Wu, Z and Zhu, H},
journal = {Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys '13)},
title = {{Timestream: Reliable stream computation in the cloud}},
url = {http://dl.acm.org/citation.cfm?id=2465353},
year = {2013}
}
@book{Morrison1994,
author = {Morrison, JP},
pages = {1--377},
title = {{Flow-Based Programming}},
url = {http://ersaconf.org/ersa-adn/papers/adn003.pdf},
year = {1994}
}
@book{Morrison1994b,
author = {Morrison, JP},
title = {{Flow-based programming - introduction}},
url = {http://ersaconf.org/ersa-adn/papers/adn003.pdf},
year = {1994}
}
@article{Isard2007,
author = {Isard, M and Budiu, M and Yu, Y and Birrell, A and Fetterly, D},
journal = {ACM SIGOPS Operating  {\ldots}},
title = {{Dryad: distributed data-parallel programs from sequential building blocks}},
url = {http://dl.acm.org/citation.cfm?id=1273005},
year = {2007}
}
@article{Stonebraker2005,
author = {Stonebraker, M and {\c{C}}etintemel, U and Zdonik, S},
journal = {ACM SIGMOD Record},
title = {{The 8 requirements of real-time stream processing}},
url = {http://dl.acm.org/citation.cfm?id=1107504},
year = {2005}
}
@inproceedings{Labonte2004,
abstract = {Stream programming is currently being pushed as a way to expose concurrency and separate communication from computation. Since there are many stream languages and potential stream execution engines, this paper proposes an abstract machine model that captures the essential characteristics of stream architectures, the Stream Virtual Machine (SVM). The goal of the SVM is to improve interoperability, allow developpment of common compilation tools and reason about stream program performance. The SVM contains control processors, slave kernel processors, and slave DMA units. Is is presented along with the compilation process that takes a stream program down to the SVM and finally down to machine binary. To extract the parameters for our SVM model, we use micro-kernels to characterize two graphics processors and a stream engine, Imagine. The results are encouraging; the model estimates the performance of the target machines with high accuracy.},
author = {Labonte, Francois and Mattson, Peter and Thies, William and Buck, Ian and Kozyrakis, Christos and Horowitz, Mark},
booktitle = {Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques},
keywords = {alire,dataflow},
organization = {IEEE Computer Society},
pages = {267--277},
publisher = {Ieee},
title = {{The stream virtual machine}},
url = {http://dl.acm.org/citation.cfm?id=1026015 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1342560},
year = {2004}
}
@article{Wei2012,
author = {Wei, Haitao and Yu, Junqing and Yu, Huafei and Qin, Mingkang and Gao, Guangrong},
keywords = {alire,dataflow},
publisher = {IEEE},
title = {{Software pipelining for stream programs on resource constrained multi-core architectures}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6143921},
year = {2012}
}
@article{Bhattacharya2001,
author = {Bhattacharya, Bishnupriya and Bhattacharyya, Shuvra S},
journal = {Signal Processing, IEEE Transactions on},
keywords = {alire,dataflow},
number = {10},
pages = {2408--2421},
publisher = {IEEE},
title = {{Parameterized dataflow modeling for DSP systems}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=950795},
volume = {49},
year = {2001}
}
@article{Zhang2008,
author = {Zhang, David and Li, Qiuyuan J and Rabbah, Rodric M and Amarasinghe, Saman P},
journal = {SIGARCH Computer Architecture News},
keywords = {alire,dataflow},
number = {2},
pages = {18--27},
title = {{A lightweight streaming layer for multicore execution}},
volume = {36},
year = {2008}
}
@article{Pop2013,
abstract = {We present OpenStream, a data-flow extension of OpenMP to express
dynamic dependent tasks. The lan- guage supports nested task creation,
modular composition, variable and unbounded sets of producers/con-
sumers, and first-class streams. These features, enabled by our original
compilation flow, allow translating high-level parallel programming
patterns, like dependences arising from StarSs? array regions, or
univer- sal low-level primitives like futures. In particular, these
dynamic features can be embedded efficiently and naturally into an
unmanaged imperative language, avoiding the complexity and overhead
of a concurrent garbage collector. We demonstrate the performance
advantages of a data-flow execution model compared to more restricted
task and barrier models. We also demonstrate the efficiency of our
compilation and runtime algorithms for the support of complex dependence
patterns arising from StarSs benchmarks.},
author = {Pop, Antoniu and Cohen, Albert},
journal = {TACO},
keywords = {dataflow,important,lu},
number = {4},
pages = {53},
title = {{OpenStream: Expressiveness and data-flow compilation of OpenMP streaming programs}},
volume = {9},
year = {2013}
}
@inproceedings{Giorgi2012,
abstract = {The TERAFLUX project is a Future and Emerging Technologies (FET) Large-Scale
Project funded by the European Union. TERAFLUX is at the forefront
of major research challenges such as programmability, manageable
architecture design, reliability of many-core or 1000+ core chips.
In the near future, new computing systems will consist of a huge
number of transistors - probably 1 Tera or 1000 billions by 2020:
we name such systems as "Teradevices". In this project, the aim is
to solve the three challenges at once by using the dataflow principles
wherever they are applicable or make sense in the general economy
of the system. An Instruction Set Extension (ISE) for the x86-64
is illustrated. This ISE supports the dataflow execution of threads.},
author = {Giorgi, Roberto},
booktitle = {Proceedings of the 9th conference on Computing Frontiers},
keywords = {dataflow},
organization = {ACM},
pages = {303--304},
title = {{TERAFLUX: exploiting dataflow parallelism in teradevices}},
url = {http://dl.acm.org/citation.cfm?id=2212959},
year = {2012}
}
@inproceedings{Damavandpeyma2013,
author = {Damavandpeyma, Morteza and Stuijk, Sander and Basten, Twan and Geilen, Marc and Corporaal, Henk},
booktitle = {Proceedings of the 19th IEEE Real-Time and Embedded Technology and Applications Symposium},
keywords = {alire,dataflow},
title = {{Throughput-Constrained DVFS for Scenario-Aware Dataflow Graphs}},
year = {2013}
}
@phdthesis{Thies2009,
author = {Thies, William},
keywords = {alire,dataflow},
school = {Massachusetts Institute of Technology},
title = {{Language and compiler support for stream programs}},
year = {2009}
}
@inproceedings{Sermulins2005,
author = {Sermulins, Janis and Thies, William and Rabbah, Rodric and Amarasinghe, Saman},
booktitle = {ACM SIGPLAN Notices},
keywords = {alire,dataflow},
number = {7},
organization = {ACM},
pages = {115--126},
title = {{Cache aware optimization of stream programs}},
url = {http://dl.acm.org/citation.cfm?id=1065927},
volume = {40},
year = {2005}
}
@inproceedings{Lin2008,
author = {Lin, Yuan and Choi, Yoonseo and Mahlke, Scott A and Mudge, Trevor N and Chakrabarti, Chaitali},
booktitle = {ICSAMOS},
keywords = {alire,dataflow},
pages = {10--17},
title = {{A parameterized dataflow language extension for embedded streaming systems}},
year = {2008}
}
@inproceedings{Poplavko2010,
author = {Poplavko, Peter and Geilen, Marc and Basten, Twan},
booktitle = {Computer Design (ICCD), 2010 IEEE International Conference on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {282--288},
title = {{Predicting the throughput of multiprocessor applications under dynamic workload}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5647740},
year = {2010}
}
@inproceedings{Bonfietti2010,
author = {Bonfietti, Alessio and Benini, Luca and Lombardi, Michele and Milano, Michela},
booktitle = {Design, Automation {\&} Test in Europe Conference {\&} Exhibition (DATE), 2010},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {897--902},
title = {{An efficient and complete approach for throughput-maximal SDF allocation and scheduling on multi-core platforms}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5456924},
year = {2010}
}
@phdthesis{Soule2012b,
author = {Soul{\'{e}}, Robert},
keywords = {alire,dataflow},
school = {New York University},
title = {{Reusable Software Infrastructure for Stream Processing}},
year = {2012}
}
@inproceedings{Buck1992,
author = {Buck, Joseph and Lee, Edward A},
booktitle = {Data Flow Workshop},
keywords = {alire,dataflow},
organization = {Citeseer},
title = {{The token flow model}},
year = {1992}
}
@inproceedings{Stuijk2010,
author = {Stuijk, Sander and Geilen, Marc and Basten, Twan},
booktitle = {Digital System Design: Architectures, Methods and Tools (DSD), 2010 13th Euromicro Conference on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {548--555},
title = {{A predictable multiprocessor design flow for streaming applications with dynamic behaviour}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5615547},
year = {2010}
}
@inproceedings{Dashti2013,
author = {Dashti, Mohammad and Fedorova, Alexandra and Funston, Justin and Gaud, Fabien and Lachaize, Renaud and Lepers, Baptiste and Qu{\'{e}}ma, Vivien and Roth, Mark},
booktitle = {Proceedings of the eighteenth international conference on Architectural support for programming languages and operating systems},
keywords = {alire,dataflow,heterogeneous},
organization = {ACM},
pages = {381--394},
title = {{Traffic management: A holistic approach to memory placement on numa systems}},
year = {2013}
}
@phdthesis{Hormati2011,
author = {Hormati, Amir H},
keywords = {alire,dataflow,heterogeneous},
school = {IBM},
title = {{Compiling Stream Applications for Heterogeneous Architectures}},
year = {2011}
}
@mastersthesis{Tan2010,
author = {Tan, Ceryen},
keywords = {dataflow,important,lu},
school = {Massachusetts Institute of Technology},
title = {{A hybrid static/dynamic approach to scheduling stream programs}},
url = {http://18.7.29.232/handle/1721.1/61291},
year = {2009}
}
@article{Fradet2011,
author = {Fradet, Pascal and Girault, Alain and Poplavko, Peter and Others},
keywords = {dataflow,important,lu},
title = {{SPDF: A Schedulable Parametric Data-Flow MoC (Extended Version)}},
url = {http://hal.archives-ouvertes.fr/hal-00666284/},
year = {2011}
}
@inproceedings{Cohen2012,
author = {Cohen, Albert and G{\'{e}}rard, L{\'{e}}onard and Pouzet, Marc},
booktitle = {EMSOFT},
keywords = {alire,dataflow,synchrone},
pages = {197--206},
title = {{Programming parallelism with futures in lustre}},
year = {2012}
}
@inproceedings{Gummaraju2008,
author = {Gummaraju, Jayanth and Coburn, Joel and Turner, Yoshio and Rosenblum, Mendel},
booktitle = {ASPLOS},
keywords = {alire,dataflow},
pages = {297--307},
title = {{Streamware: programming general-purpose multicore processors using streams}},
year = {2008}
}
@inproceedings{Geilen2010,
author = {Geilen, Marc and Sander},
booktitle = {Hardware/Software Codesign and System Synthesis (CODES+ISSS), 2010 IEEE/ACM/IFIP International Conference on},
keywords = {alire,dataflow},
pages = {125--134},
title = {{Worst-case performance analysis of Synchronous Dataflow scenarios}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751491},
year = {2010}
}
@mastersthesis{Wong2012,
author = {Wong, Eric},
keywords = {alire,dataflow},
school = {Massachusetts Institute of Technology},
title = {{Optimizations in Stream Programming for Multimedia Applications}},
year = {2012}
}
@inproceedings{Pastrnak2004,
author = {Pastrnak, Milan and Poplavko, Peter and Farin, Dirk S and Others},
booktitle = {System-on-Chip for Real-Time Applications, 2004. Proceedings. 4th IEEE International Workshop on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {206--209},
title = {{Data-flow timing models of dynamic multimedia applications for multiprocessor systems}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1319879},
year = {2004}
}
@inproceedings{Weis2011,
author = {Weis, Sebastian and Garbade, Arne and Wolf, Julian and Fechner, Bernhard and Mendelson, Avi and Giorgi, Roberto and Ungerer, Theo},
booktitle = {Data-Flow Execution Models for Extreme Scale Computing (DFM), 2011 First Workshop on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {38--44},
title = {{A Fault Detection and Recovery Architecture for a Teradevice Dataflow System}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6176403},
year = {2011}
}
@techreport{Soule2012a,
abstract = {Developers increasingly use stream processing languages to write applications that process large volumes of data with high throughput. Unfortunately, when choosing which stream processing language to use, they face a difficult choice. On the one hand, dynamically scheduled languages allow developers to write a wider range of applications, but cannot take advantage of many crucial optimizations. On the other hand, statically scheduled languages are extremely performant, but cannot express many important streaming applications. This paper presents the design of a hybrid scheduler for stream processing languages. The compiler partitions the streaming application into coarse-grained subgraphs sepa- rated by dynamic rate boundaries. It then applies static op- timizations to those subgraphs. We have implemented this scheduler as an extension to the StreamIt compiler, and eval- uated its performance against three scheduling techniques used by dynamic systems: OS thread, demand, and no-op. Our scheduler not only allows the previously static version of StreamIt to run dynamic rate applications, but it outper- forms the three dynamic alternatives. This demonstrates that our scheduler strikes the right balance between expressivity and performance for stream processing languages.},
author = {Soul{\'{e}}, Robert and Gordon, Michael I and Amarasinghe, Saman and Grimm, Robert and Hirzel, Martin},
institution = {New York University},
keywords = {dataflow},
number = {Technical Report TR2012-948},
title = {{Hitting the Sweet Spot for Streaming Languages: Dynamic Expressivity with Static Optimization}},
year = {2012}
}
@inproceedings{Spring2007,
author = {Spring, Jesper Honig and Privat, Jean and Guerraoui, Rachid and Vitek, Jan},
booktitle = {OOPSLA},
keywords = {dataflow,lu},
pages = {211--228},
title = {{Streamflex: high-throughput stream programming in java}},
year = {2007}
}
@article{Castrillon2013,
author = {Castrillon, Jeronimo and Leupers, Rainer and Ascheid, Gerd},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {alire,dataflow,heterogeneous},
pages = {527--545},
publisher = {IEEE},
title = {{Maps: Mapping concurrent dataflow applications to heterogeneous mpsocs}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6062671},
volume = {9},
year = {2013}
}
@article{Johnston2004,
abstract = {Many developments have taken place within dataflow programming languages in the past decade. In particular, there has been a great deal of activity and advancement in the field of dataflow visual programming languages. The motivation for this article is to review the content of these recent developments and how they came about. It is supported by an initial review of dataflow programming in the 1970s and 1980s that led to current topics of research. It then discusses how dataflow programming evolved toward a hybrid von Neumann dataflow formulation, and adopted a more coarse-grained approach. Recent trends toward dataflow visual programming languages are then discussed with reference to key graphical dataflow languages and their development environments. Finally, the article details four key open topics in dataflow programming languages.},
author = {Johnston, Wesley M and Hanna, J R and Millar, Richard J},
journal = {ACM Computing Surveys (CSUR)},
keywords = {alire,dataflow,survey},
pages = {1--34},
publisher = {ACM},
title = {{Advances in dataflow programming languages}},
url = {http://dl.acm.org/citation.cfm?id=1013209},
volume = {36},
year = {2004}
}
@inproceedings{Yu2011,
author = {Yu, Zhibin and Righi, Andrea and Giorgi, Roberto},
booktitle = {FUTURE COMPUTING 2011, The Third International Conference on Future Computational Technologies and Applications},
keywords = {alire,dataflow},
pages = {100--106},
title = {{A Case Study on the Design Trade-off of a Thread Level Data Flow based Many-core Architecture}},
url = {http://www.thinkmind.org/index.php?view=article{\&}amp;articleid=future{\_}computing{\_}2011{\_}4{\_}40{\_}30125},
year = {2011}
}
@incollection{Carpenter2009,
author = {Carpenter, Paul M and Ramirez, Alex and Ayguade, Eduard},
booktitle = {Embedded Computer Systems: Architectures, Modeling, and Simulation},
keywords = {alire,dataflow},
pages = {12--23},
publisher = {Springer},
title = {{The abstract streaming machine: compile-time performance modelling of stream programs on heterogeneous multiprocessors}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-03138-0{\_}3},
year = {2009}
}
@inproceedings{Silc1997,
author = {Silc, Jurij and Robi{\v{c}}, Borut and Ungerer, Theo},
booktitle = {Progress in computer research},
keywords = {alire,dataflow},
organization = {Nova Science Publishers, Inc.},
pages = {1--33},
title = {{Asynchrony in parallel computing: from dataflow to multithreading}},
url = {http://dl.acm.org/citation.cfm?id=568127},
year = {1997}
}
@phdthesis{Moreira2012,
author = {Moreira, Orlando},
keywords = {SDR,alire,dataflow},
school = {Ph. D. dissertation, TU Eindhoven},
title = {{Temporal analysis and scheduling of hard real-time radios running on a multi-processor}},
year = {2012}
}
@inproceedings{Wipliez2010,
author = {Wipliez, Matthieu and Raulet, Micka{\"{e}}l},
booktitle = {Design and Architectures for Signal and Image Processing (DASIP), 2010 Conference on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {303--310},
title = {{Classification and transformation of dynamic dataflow programs}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5706280},
year = {2010}
}
@inproceedings{Gautier2007,
author = {Gautier, Thierry and Besseron, Xavier and Pigeon, Laurent},
booktitle = {Proceedings of the 2007 international workshop on Parallel symbolic computation},
keywords = {alire,dataflow,important},
organization = {ACM},
pages = {15--23},
title = {{KAAPI: A thread scheduling runtime system for data flow computations on cluster of multi-processors}},
url = {http://dl.acm.org/citation.cfm?id=1278182},
year = {2007}
}
@inproceedings{Wu2009,
author = {Wu, Nan and Wen, Mei and Wu, Wei and Ren, Ju and Su, Huayou and Xun, Changqing and Zhang, Chunyuan},
booktitle = {Proceedings of the 17th ACM international conference on Multimedia},
keywords = {alire,dataflow},
organization = {ACM},
pages = {371--380},
title = {{Streaming HD H. 264 encoder on programmable processors}},
url = {http://dl.acm.org/citation.cfm?id=1631324},
year = {2009}
}
@article{Lee1995,
author = {Lee, Edward A and Parks, Thomas M},
journal = {Proceedings of the IEEE},
keywords = {alire,dataflow},
number = {5},
pages = {773--801},
publisher = {IEEE},
title = {{Dataflow process networks}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=381846},
volume = {83},
year = {1995}
}
@phdthesis{Carpenter2011,
author = {Carpenter, Paul},
keywords = {alire,dataflow,heterogeneous},
publisher = {Universitat Polit{\{}{\`{e}}{\}}cnica de Catalunya},
title = {{Running stream-like programs on heterogeneous multi-core systems}},
url = {http://www.tdx.cat/handle/10803/84186},
year = {2011}
}
@inproceedings{Gummaraju2007,
author = {Gummaraju, Jayanth and Erez, Mattan and Coburn, Joel and Rosenblum, Mendel and Dally, William J},
booktitle = {Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques},
keywords = {alire,dataflow},
organization = {IEEE Computer Society},
pages = {3--12},
title = {{Architectural support for the stream execution model on general-purpose processors}},
url = {http://dl.acm.org/citation.cfm?id=1299106},
year = {2007}
}
@inproceedings{Damavandpeyma2012,
author = {Morteza and Stuijk, Sander and Geilen, Marc and Basten, Twan and Corporaal, Henk},
booktitle = {Computer Design (ICCD), 2012 IEEE 30th International Conference on},
doi = {10.1109/ICCD.2012.6378644},
keywords = {alire,dataflow},
pages = {219--226},
title = {{Parametric throughput analysis of scenario-aware dataflow graphs}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378644},
year = {2012}
}
@inproceedings{Soule2012c,
author = {Soul{\'{e}}, Robert and Hirzel, Martin and Gedik, Bu$\backslash$ugra and Grimm, Robert},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
keywords = {alire,dataflow},
organization = {ACM},
pages = {20--31},
title = {{From a calculus to an execution environment for stream processing}},
url = {http://dl.acm.org/citation.cfm?id=2335487},
year = {2012}
}
@inproceedings{Stuijk2007,
author = {Stuijk, Sander and Basten, T and Geilen, M C W and Corporaal, Henk},
booktitle = {Design Automation Conference, 2007. DAC'07. 44th ACM/IEEE},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {777--782},
title = {{Multiprocessor resource allocation for throughput-constrained synchronous dataflow graphs}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4261288},
year = {2007}
}
@inproceedings{Ghamarian2008,
author = {Ghamarian, A H and Geilen, M C W and Basten, T and Stuijk, S},
booktitle = {Design, Automation and Test in Europe, 2008. DATE '08},
doi = {10.1109/DATE.2008.4484672},
keywords = {alire,dataflow},
pages = {116--121},
title = {{Parametric Throughput Analysis of Synchronous Data Flow Graphs}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484672},
year = {2008}
}
@inproceedings{Sanchez2011,
author = {Sanchez, Daniel and Lo, David and Yoo, Richard M and Sugerman, Jeremy and Kozyrakis, Christos},
booktitle = {PACT},
keywords = {alire,dataflow},
pages = {22--32},
title = {{Dynamic Fine-Grain Scheduling of Pipeline Parallelism}},
year = {2011}
}
@inproceedings{Hormati2009,
author = {Hormati, Amir H and Choi, Yoonseo and Kudlur, Manjunath and Rabbah, Rodric and Mudge, Trevor and Mahlke, Scott},
booktitle = {Parallel Architectures and Compilation Techniques, 2009. PACT'09. 18th International Conference on},
keywords = {alire,dataflow,heterogeneous},
organization = {IEEE},
pages = {214--223},
title = {{Flextream: Adaptive compilation of streaming applications for heterogeneous architectures}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5260540},
year = {2009}
}
@phdthesis{Vrba2009,
author = {Vrba, {\v{Z}}eljko},
keywords = {alire,dataflow},
school = {PhD thesis, Department of Informatics, University of Oslo, Norway},
title = {{Implementation and performance aspects of Kahn process networks}},
year = {2009}
}
@article{Lee1987,
annote = {Ptolemy},
author = {Lee, Edward A and Messerschmitt, David G},
journal = {Proceedings of the IEEE},
keywords = {alire,dataflow},
number = {9},
pages = {1235--1245},
publisher = {IEEE},
title = {{Synchronous data flow}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1458143},
volume = {75},
year = {1987}
}
@inproceedings{Stuijk2011,
author = {Stuijk, Sander and Geilen, Marc and Theelen, Bart and Basten, Twan},
booktitle = {Embedded Computer Systems (SAMOS), 2011 International Conference on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {404--411},
title = {{Scenario-aware dataflow: Modeling, analysis and implementation of dynamic applications}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6045491},
year = {2011}
}
@inproceedings{Siyoum2011,
author = {Siyoum, Firew and Geilen, Marc and Moreira, Orlando and Nas, Rick and Corporaal, Henk},
booktitle = {System on Chip (SoC), 2011 International Symposium on},
doi = {10.1109/ISSOC.2011.6089222},
keywords = {SDR,alire,dataflow},
pages = {14--21},
title = {{Analyzing synchronous dataflow scenarios for dynamic software-defined radio applications}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089222},
year = {2011}
}
@article{Murthy2002,
author = {Murthy, Praveen K and Lee, Edward A},
journal = {Signal Processing, IEEE Transactions on},
keywords = {alire,dataflow},
number = {8},
pages = {2064--2079},
publisher = {IEEE},
title = {{Multidimensional synchronous dataflow}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1018801},
volume = {50},
year = {2002}
}
@inproceedings{Lin2011,
author = {Lin, Jing and Srivatsa, Akshaya and Gerstlauer, Andreas and Evans, Brian L},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {1605--1608},
title = {?},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5946804},
year = {2011}
}
@inproceedings{Pop2011,
abstract = {This paper introduces an extension to OpenMP3.0 enabling stream programming
with minimal, incremental additions that seamlessly integrate into
the current specification. The stream programming model decomposes
programs into tasks and explicits the flow of data among them, thus
exposing data, task and pipeline parallelism. It helps the programmers
to express concurrency and data local- ity properties, avoiding non-portable
low-level code and early opti- mizations. We survey the diverse motivations
and constraints con- verging towards the design of our simple yet
powerful language extension, and we present experimental results
of a prototype im- plementation in a public branch of GCC 4.5.},
author = {Pop, Antoniu and Cohen, Albert},
booktitle = {HiPEAC},
keywords = {dataflow,important,lu},
pages = {5--14},
title = {{A stream-computing extension to OpenMP}},
year = {2011}
}
@inproceedings{Buck1993,
author = {Buck, Joseph Tobin and Lee, Edward A},
booktitle = {Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference on},
keywords = {alire,dataflow},
organization = {IEEE},
pages = {429--432},
title = {{Scheduling dynamic dataflow graphs with bounded memory using the token flow model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=319147},
volume = {1},
year = {1993}
}
@inproceedings{Gordon2002,
author = {Gordon, Michael I and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S and Lamb, Andrew A and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman P},
booktitle = {ASPLOS},
keywords = {dataflow,important,lu},
pages = {291--303},
title = {{A stream compiler for communication-exposed architectures}},
year = {2002}
}
@incollection{Goubier2011,
author = {Goubier, Thierry and Sirdey, Renaud and Louise, St{\'{e}}phane and David, Vincent},
booktitle = {Algorithms and Architectures for Parallel Processing},
keywords = {alire,dataflow},
pages = {385--394},
publisher = {Springer},
title = {{$\Sigma$C: A programming model and language for embedded manycores}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-24650-0{\_}33},
year = {2011}
}
@phdthesis{Gordon2010,
abstract = {Given the ubiquity of multicore processors, there is an acute need
to enable the development of scalable parallel applications without
unduly burdening programmers. Currently, programmers are asked not
only to explicitly expose parallelism but also concern themselves
with issues of granu- larity, load-balancing, synchronization, and
communication. This thesis demonstrates that when algorithmic parallelism
is expressed in the form of a stream program, a compiler can effectively
and automatically manage the parallelism. Our compiler assumes responsibility
for low-level architec- tural details, transforming implicit algorithmic
parallelism into a mapping that achieves scalable parallel performance
for a given multicore target.

Stream programming is characterized by regular processing of sequences
of data, and it is a natural expression of algorithms in the areas
of audio, video, digital signal processing, network- ing, and encryption.
Streaming computation is represented as a graph of independent computa-
tion nodes that communicate explicitly over data channels. Our techniques
operate on contiguous regions of the stream graph where the input
and output rates of the nodes are statically deter- minable. Within
a static region, the compiler first automatically adjusts the granularity
and then exploits data, task, and pipeline parallelism in a holistic
fashion. We introduce techniques that data-parallelize nodes that
operate on overlapping sliding windows of their input, translating
se- rializing state into minimal and parametrized inter-core communication.
Finally, for nodes that cannot be data-parallelized due to state,
we are the first to automatically apply software-pipelining techniques
at a coarse granularity to exploit pipeline parallelism between stateful
nodes.

Our framework is evaluated in the context of the StreamIt programming
language. StreamIt is a high-level stream programming language that
has been shown to improve programmer pro- ductivity in implementing
streaming algorithms. We employ the StreamIt Core benchmark suite
of 12 real-world applications to demonstrate the effectiveness of
our techniques for varying multi- core architectures. For a 16-core
distributed memory multicore, we achieve a 14.9x mean speedup. For
benchmarks that include sliding-window computation, our sliding-window
data-parallelization techniques are required to enable scalable performance
for a 16-core SMP multicore (14x mean speedup) and a 64-core distributed
shared memory multicore (52x mean speedup).},
author = {Gordon, Michael I},
keywords = {dataflow,important,lu},
publisher = {Massachusetts Institute of Technology},
school = {Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science},
title = {{Compiler techniques for scalable performance of stream programs on multicore architectures}},
year = {2010}
}
@article{Hewitt1973,
author = {Hewitt, C and Bishop, P and Greif, I and Smith, B},
journal = {Proceedings of the 1st annual ACM SIGACT-SIGPLAN symposium on Principles of programming languages},
title = {{Actor induction and meta-evaluation}},
url = {http://dl.acm.org/citation.cfm?id=512942},
year = {1973}
}
@inproceedings{Dean2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
booktitle = {Proc. of the OSDI - Symp. on Operating Systems Design and Implementation},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
file = {:home/etn/Documents/PhD/Biblio/Dean, Ghemawat - 2004 - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
isbn = {9781595936868},
issn = {00010782},
number = {1},
pages = {137--149},
pmid = {11687618},
title = {{MapReduce: Simplified Data Processing on Large Clusters}},
url = {http://dl.acm.org/citation.cfm?id=1327492 http://doi.acm.org/10.1145/1327452.1327492 http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=53CA72B524B9A6153BFE89FE26FBB832?doi=10.1.1.163.5292},
volume = {51},
year = {2004}
}
@article{Amini2012,
author = {Amini, M},
title = {{Transformations de programme automatiques et source-{\`{a}}-source pour acc{\'{e}}l{\'{e}}rateurs mat{\'{e}}riels de type GPU}},
url = {http://hal.archives-ouvertes.fr/pastel-00958033/},
year = {2012}
}
@article{Petit2009,
author = {Petit, E},
title = {{Vers un partitionnement automatique d'applications en codelets sp{\'{e}}culatifs pour les syst{\`{e}}mes h{\'{e}}t{\'{e}}rog{\`{e}}nes {\`{a}} m{\'{e}}moires distribu{\'{e}}es}},
url = {http://hal.inria.fr/tel-00445512/},
year = {2009}
}
@article{Milner1992,
author = {Milner, R and Parrow, J and Walker, D},
file = {:home/etn/Documents/PhD/Biblio/Milner, Parrow, Walker - 1992 - A calculus of mobile processes, I.pdf:pdf},
journal = {Information and computation},
title = {{A calculus of mobile processes, I}},
url = {http://www.sciencedirect.com/science/article/pii/0890540192900084},
year = {1992}
}
@book{Milner1993,
author = {Milner, R},
title = {{The polyadic $\pi$-calculus: a tutorial}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-58041-3{\_}6},
year = {1993}
}
@article{Milner1991,
author = {Milner, R and Parrow, J and Walker, D},
file = {:home/etn/Documents/PhD/Biblio/Milner, Parrow, Walker - 1991 - Modal logics for mobile processes.pdf:pdf},
journal = {CONCUR'91},
title = {{Modal logics for mobile processes}},
url = {http://link.springer.com/chapter/10.1007/3-540-54430-5{\_}80},
year = {1991}
}
@article{Pierce2000,
author = {Pierce, BC and Turner, DN},
journal = {Proof, language, and interaction},
title = {{Pict: a programming language based on the Pi-Calculus.}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=g8DEO9DwmZoC{\&}oi=fnd{\&}pg=PA455{\&}dq=pict+pi+calculus{\&}ots={\_}k60hkkCaQ{\&}sig=A{\_}{\_}Ajof3EAYvQbwg8f62MfLqc1g},
year = {2000}
}
@article{Bocq2012,
author = {Bocq, S{\'{e}}bastien and Daenen, Koen},
doi = {10.1145/2398857.2384640},
isbn = {978-1-4503-1561-6},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {DSEl,DSl,concurrent,functional programming,multicore,parallel,process networks,scala,stream},
month = {nov},
number = {10},
pages = {315},
publisher = {ACM},
title = {{Molecule}},
url = {http://dl.acm.org/citation.cfm?id=2398857.2384640},
volume = {47},
year = {2012}
}
@article{Steen2007,
author = {Steen, Marteen Van and Tanenbaum, Andrew S},
journal = {Network},
title = {{Distributed Systems: principles and paradigms}},
url = {http://u.cs.biu.ac.il/{~}ariel/download/ds590/pdfs/notes.01.pdf},
year = {2007}
}
@article{Yu2009,
author = {Yu, Yuan and Isard, Michael and Fetterly, Dennis and Budiu, Mihai and Erlingsson, Ulfar and Gunda, Pradeep Kumar and Currey, Jon and McSherry, Frank and Achan, Kannan and Poulain, Christophe},
journal = {Microsoft Research},
title = {{Some sample programs written in DryadLINQ}},
url = {http://research.microsoft.com/jump/66811},
year = {2009}
}
@article{Balazinska2008,
author = {Balazinska, M and Balakrishnan, H},
journal = {{\ldots}  on Database Systems ( {\ldots}},
title = {{Fault-tolerance in the Borealis distributed stream processing system}},
url = {http://dl.acm.org/citation.cfm?id=1331907},
year = {2008}
}
@article{Chen2000,
author = {Chen, J and DeWitt, DJ and Tian, F and Wang, Y},
journal = {ACM SIGMOD Record},
title = {{NiagaraCQ: A scalable continuous query system for internet databases}},
url = {http://dl.acm.org/citation.cfm?id=335432},
year = {2000}
}
@article{Aref2004,
author = {Aref, WG and Elmagarmid, AK},
journal = {{\ldots}  on Data  {\ldots}},
title = {{Nile: a query processing engine for data streams}},
url = {http://www-users.cs.umn.edu/{~}mokbel/papers/NileDemo.pdf},
year = {2004}
}
@article{Naughton2001,
author = {Naughton, JF and DeWitt, DJ and Maier, D},
journal = {IEEE Data Eng. {\ldots}},
title = {{The Niagara internet query system}},
url = {http://www.cs.cornell.edu/People/Jai/papers/niagaraoverview.pdf},
year = {2001}
}
@book{Luckham2002,
author = {Luckham, DC},
title = {{The power of events}},
url = {https://sisis.rz.htw-berlin.de/inh2010/12375999.pdf},
year = {2002}
}
@article{Yang,
author = {Yang, F and Qian, Z and Chen, X and Beschastnikh, I},
journal = {research.microsoft.com},
title = {{Sonora: A Platform for Continuous Mobile-Cloud Computing}},
url = {http://research.microsoft.com/pubs/161446/paper.pdf},
year = {2012}
}
@article{Murray2011,
author = {Murray, DG and Schwarzkopf, M},
file = {:home/etn/Documents/PhD/Biblio/Murray, Schwarzkopf - 2011 - CIEL a universal execution engine for distributed data-flow computing.pdf:pdf},
journal = {Proceedings of the {\ldots}},
title = {{CIEL: a universal execution engine for distributed data-flow computing}},
url = {http://static.usenix.org/event/nsdi11/tech/full{\_}papers/Murray.pdf},
year = {2011}
}
@article{Gedik2008,
author = {Gedik, B and Andrade, H and Wu, KL and Yu, PS and Doo, M},
journal = {Proceedings of the 2008  {\ldots}},
title = {{SPADE: the system s declarative stream processing engine}},
url = {http://dl.acm.org/citation.cfm?id=1376729},
year = {2008}
}
@article{Barga2006,
author = {Barga, RS and Goldstein, J and Ali, M and Hong, M},
journal = {arXiv preprint cs/0612115},
title = {{Consistent streaming through time: A vision for event stream processing}},
url = {http://arxiv.org/abs/cs/0612115},
year = {2006}
}
@inproceedings{Meijer2006,
address = {New York, New York, USA},
author = {Meijer, Erik and Beckman, Brian and Bierman, Gavin},
booktitle = {Proceedings of the 2006 ACM SIGMOD international conference on Management of data - SIGMOD '06},
doi = {10.1145/1142473.1142552},
isbn = {1595934340},
month = {jun},
pages = {706},
publisher = {ACM Press},
title = {{LINQ}},
url = {http://dl.acm.org/citation.cfm?id=1142473.1142552},
year = {2006}
}
@article{Krishnamurthy2003,
author = {Krishnamurthy, S and Chandrasekaran, S},
file = {:home/etn/Documents/PhD/Biblio/Krishnamurthy, Chandrasekaran - 2003 - TelegraphCQ An architectural status report.pdf:pdf},
journal = {IEEE Data Eng. {\ldots}},
title = {{TelegraphCQ: An architectural status report}},
url = {http://db.lcs.mit.edu/madden/html/deissue-b.pdf{\#}page=13},
year = {2003}
}
@article{Zaharia,
author = {Zaharia, M and Chowdhury, M and Das, T and Dave, A},
file = {:home/etn/Documents/PhD/Biblio/Zaharia et al. - 2010 - Fast and interactive analytics over Hadoop data with Spark.pdf:pdf},
journal = {usenix.org},
pages = {45--51},
title = {{Fast and interactive analytics over Hadoop data with Spark}},
url = {https://www.usenix.org/system/files/login/articles/zaharia.pdf},
year = {2010}
}
@article{Akidau2013,
author = {Akidau, T and Balikov, A},
journal = {Proceedings of the VLDB Endowment 6.11},
title = {{MillWheel: Fault-Tolerant Stream Processing at Internet Scale}},
url = {http://research.google.com/pubs/archive/41378.pdf},
year = {2013}
}
@article{Gautier1987,
author = {Gautier, T and Guernic, P Le and Besnard, L},
journal = {Functional programming languages  {\ldots}},
title = {{Signal: A declarative language for synchronous programming of real-time systems}},
url = {http://link.springer.com/chapter/10.1007/3-540-18317-5{\_}15},
year = {1987}
}
@article{He2010,
author = {He, B and Yang, M and Guo, Z and Chen, R and Su, B},
file = {:home/etn/Documents/PhD/Biblio/He et al. - 2010 - Comet batched stream processing for data intensive distributed computing.pdf:pdf},
journal = {{\ldots} on Cloud computing},
title = {{Comet: batched stream processing for data intensive distributed computing}},
url = {http://dl.acm.org/citation.cfm?id=1807139},
year = {2010}
}
@article{Logothetis2010,
abstract = {This work addresses the need for stateful dataflow programs that can rapidly sift through huge, evolving data sets. These data-intensive applications perform complex multi-step computations over successive generations of data inflows, such as weekly web crawls, daily image/video uploads, log files, and growing social networks. While programmers may simply re-run the entire dataflow when new data arrives, this is grossly inefficient, increasing result latency and squandering hardware resources and energy. Alternatively, programmers may use prior results to incrementally incorporate the changes. However, current large-scale data processing tools, such as Map-Reduce or Dryad, limit how programmers incorporate and use state in data-parallel programs. Straightforward approaches to incorporating state can result in custom, fragile code and disappointing performance. This work presents a generalized architecture for continuous bulk processing (CBP) that raises the level of abstraction for building incremental applications. At its core is a flexible, groupwise processing operator that takes state as an explicit input. Unifying stateful programming with a data-parallel operator affords several fundamental opportunities for minimizing the movement of data in the underlying processing system. As case studies, we show how one can use a small set of flexible dataflow primitives to perform web analytics and mine large-scale, evolving graphs in an incremental fashion. Experiments with our prototype using real-world data indicate significant data movement and running time reductions relative to current practice. For example, incrementally computing PageRank using CBP can reduce data movement by 46{\%} and cut running time in half.},
author = {Logothetis, Dionysios and Olston, Christopher and Reed, Benjamin and Webb, Kevin C. and Yocum, Ken},
doi = {10.1145/1807128.1807138},
file = {:home/etn/Documents/PhD/Biblio/Logothetis et al. - 2010 - Stateful bulk processing for incremental analytics.pdf:pdf},
isbn = {9781450300360},
journal = {International Conference on Management of Data},
keywords = {cloud computing,incremental,mapreduce,parallel data processing},
pages = {51--62},
title = {{Stateful bulk processing for incremental analytics}},
url = {http://dl.acm.org/citation.cfm?id=1807138 http://portal.acm.org/citation.cfm?id=1807128.1807138},
year = {2010}
}
@article{Peng2010,
author = {Peng, D and Dabek, F},
journal = {OSDI},
title = {{Large-scale Incremental Processing Using Distributed Transactions and Notifications.}},
url = {http://www.usenix.org/event/osdi10/tech/full{\_}papers/Peng.pdf},
year = {2010}
}
@article{McSherry,
abstract = {We report on the design and implementation of Naiad, a set of declarative$\backslash$ndata-parallel language extensions and an associated runtime supporting efficient$\backslash$nand composable incremental and iterative computation. This combination is$\backslash$nenabled by a new computational model we call differential dataflow, in which$\backslash$nincremental computation can be performed using a partial, rather than total,$\backslash$norder on time. $\backslash$n$\backslash$n Naiad extends standard batch data-parallel processing models like MapReduce,$\backslash$nHadoop, and Dryad/DryadLINQ, to support efficient incremental updates to the$\backslash$ninputs in the manner of a stream processing system, while at the same time$\backslash$nenabling arbitrarily nested fixed-point iteration. In this paper, we evaluate a$\backslash$nprototype of Naiad that uses shared memory on a single multi-core computer. We$\backslash$napply Naiad to various computations, including several graph algorithms, and$\backslash$nobserve good scaling properties and efficient incremental recomputation.},
author = {McSherry, F and Isaacs, R and Isard, M and Murray, DG},
file = {:home/etn/Documents/PhD/Biblio/McSherry et al. - 2012 - Composable Incremental and Iterative Data-Parallel Computation with Naiad.pdf:pdf},
journal = {Microsoft Research},
title = {{Composable Incremental and Iterative Data-Parallel Computation with Naiad}},
url = {http://202.114.89.42/resource/pdf/6992.pdf},
year = {2012}
}
@article{Condie2010,
author = {Condie, T and Conway, N and Alvaro, P},
journal = {NSDI},
title = {{MapReduce Online.}},
url = {http://static.usenix.org/events/nsdi10/tech/full{\_}papers/condie.pdf},
year = {2010}
}
@misc{Apache2011,
author = {Apache},
title = {{Apache Flume}},
url = {http://flume.apache.org/},
year = {2011}
}
@article{Amini2006,
author = {Amini, L and Andrade, H and Bhagwan, R},
file = {:home/etn/Documents/PhD/Biblio/Amini, Andrade, Bhagwan - 2006 - SPC A distributed, scalable platform for data mining.pdf:pdf},
journal = {{\ldots} on Data mining {\ldots}},
title = {{SPC: A distributed, scalable platform for data mining}},
url = {http://dl.acm.org/citation.cfm?id=1289615},
year = {2006}
}
@misc{Marz2011,
abstract = {Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!},
author = {Marz, Nathan and Xu, James and Jackson, Jason and Feng, Andy},
title = {{Storm}},
url = {http://storm-project.net/},
year = {2011}
}
@inproceedings{Neumeyer2010,
abstract = {S4 is a general-purpose, distributed, scalable, partially fault-tolerant, pluggable platform that allows programmers to easily develop applications for processing continuous unbounded streams of data. Keyed data events are routed with affinity to Processing Elements (PEs), which consume the events and do one or both of the following: (1) emit one or more events which may be consumed by other PEs, (2) publish results. The architecture resembles the Actors model, providing semantics of encapsulation and location transparency, thus allowing applications to be massively concurrent while exposing a simple programming interface to application developers. In this paper, we outline the S4 architecture in detail, describe various applications, including real-life deployments. Our design is primarily driven by large scale applications for data mining and machine learning in a production environment. We show that the S4 design is surprisingly flexible and lends itself to run in large clusters built with commodity hardware.},
author = {Neumeyer, Leonardo and Robbins, Bruce and Nair, Anish and Kesari, Anand},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDMW.2010.172},
file = {:home/etn/Documents/PhD/Biblio/Neumeyer et al. - 2010 - S4 Distributed stream computing platform.pdf:pdf},
isbn = {9780769542577},
issn = {15504786},
keywords = {Actors programming model,Complex event processing,Concurrent programming,Data processing,Distributed programming,Map-reduce,Middleware,Parallel programming,Real-time search,Software design,Stream computing},
pages = {170--177},
title = {{S4: Distributed stream computing platform}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5693297},
year = {2010}
}
@article{Abadi2005,
author = {Abadi, DJ and Ahmad, Y and Balazinska, M},
journal = {CIDR},
title = {{The Design of the Borealis Stream Processing Engine.}},
url = {http://www.cs.harvard.edu/{~}mdw/course/cs260r/papers/borealis-cidr05.pdf},
year = {2005}
}
@article{Arvind2003,
author = {Arvind, DP and Arasu, A and Babcock, B},
journal = {IEEE Data Engineering {\ldots}},
title = {{Stream: The stanford stream data manager}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.6180},
year = {2003}
}
@article{Chandrasekaran2003,
author = {Chandrasekaran, S and Cooper, O},
journal = {Proceedings of the {\ldots}},
title = {{TelegraphCQ: continuous dataflow processing}},
url = {http://dl.acm.org/citation.cfm?id=872857},
year = {2003}
}
@article{Balakrishnan2004,
author = {Balakrishnan, H and Balazinska, M},
journal = {The VLDB Journal},
title = {{Retrospective on aurora}},
url = {http://link.springer.com/article/10.1007/s00778-004-0133-5},
year = {2004}
}
@misc{TechnicalComittee2003,
author = {{Technical Comittee}},
booktitle = {Data Engineering},
number = {1},
title = {{Special Issue on Data Stream Processing}},
url = {http://people.cs.aau.dk/{~}tbp/BIT/moede10/A03MAR-CD.pdf{\#}page=5},
volume = {26},
year = {2003}
}
@article{Abadi2003,
author = {Abadi, D and Carney, D and Cetintemel, U},
journal = {Proceedings of the {\ldots}},
title = {{Aurora: a data stream management system}},
url = {http://dl.acm.org/citation.cfm?id=872855},
year = {2003}
}
@article{Abadi2003a,
author = {Abadi, DJ and Carney, D},
journal = {The VLDB Journal— {\ldots}},
title = {{Aurora: a new model and architecture for data stream management}},
url = {http://dl.acm.org/citation.cfm?id=950485},
year = {2003}
}
@article{Bhatotia2011,
abstract = {Many online data sets evolve over time as new entries are slowly added and existing entries are deleted or modified. Taking advantage of this, systems for incremental bulk data processing, such as Google's Percolator, can achieve efficient updates. To achieve this efficiency, however, these systems lose compatibility with the simple programming models offered by non-incremental systems, e.g., MapReduce, and more importantly, requires the programmer to implement application-specific dynamic algorithms, ultimately increasing algorithm and code complexity. In this paper, we describe the architecture, implementation, and evaluation of Incoop, a generic MapReduce framework for incremental computations. Incoop detects changes to the input and automatically updates the output by employing an efficient, fine-grained result reuse mechanism. To achieve efficiency without sacrificing transparency, we adopt recent advances in the area of programming languages to identify the shortcomings of task-level memoization approaches, and to address these shortcomings by using several novel techniques: a storage system, a contraction phase for Reduce tasks, and an affinity-based scheduling algorithm. We have implemented Incoop by extending the Hadoop framework, and evaluated it by considering several applications and case studies. Our results show significant performance improvements without changing a single line of application code.},
author = {Bhatotia, Pramod and Wieder, Alexander and Rodrigues, Rodrigo and Acar, Umut a and Pasquin, Rafael},
doi = {10.1145/2038916.2038923},
file = {:home/etn/Documents/PhD/Biblio/Bhatotia et al. - 2011 - Incoop MapReduce for incremental computations(2).pdf:pdf},
isbn = {9781450309769},
issn = {1450309763},
journal = {Proceedings of the 2nd ACM Symposium on Cloud Computing - SOCC '11},
keywords = {memoization,self-adjusting computation,stability},
pages = {1--14},
pmid = {11412367},
title = {{Incoop: MapReduce for incremental computations}},
url = {http://dl.acm.org/citation.cfm?id=2038923 http://dl.acm.org/citation.cfm?doid=2038916.2038923},
year = {2011}
}
@article{Gilbert2002,
author = {Gilbert, S and Lynch, N},
file = {:home/etn/Documents/PhD/Biblio/Gilbert, Lynch - 2002 - Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services(2).pdf:pdf},
journal = {ACM SIGACT News},
title = {{Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services}},
url = {http://dl.acm.org/citation.cfm?id=564601},
year = {2002}
}
@article{DeCandia2007,
author = {DeCandia, G and Hastorun, D},
file = {:home/etn/Documents/PhD/Biblio/DeCandia, Hastorun - 2007 - Dynamo amazon's highly available key-value store.pdf:pdf},
journal = {ACM SIGOPS  {\ldots}},
title = {{Dynamo: amazon's highly available key-value store}},
url = {http://dl.acm.org/citation.cfm?id=1294281},
year = {2007}
}
@book{Fox1997,
author = {Fox, A and Gribble, SD and Chawathe, Y and Brewer, EA and Gauthier, P},
file = {:home/etn/Documents/PhD/Biblio/Fox et al. - 1997 - Cluster-based scalable network services.pdf:pdf},
title = {{Cluster-based scalable network services}},
url = {http://dl.acm.org/citation.cfm?id=266662},
year = {1997}
}
@article{Lamport1978,
author = {Lamport, L},
file = {:home/etn/Documents/PhD/Biblio/Lamport - 1978 - Time, clocks, and the ordering of events in a distributed system.pdf:pdf},
journal = {Communications of the ACM},
title = {{Time, clocks, and the ordering of events in a distributed system}},
url = {http://dl.acm.org/citation.cfm?id=359563},
year = {1978}
}
@article{Karger1997,
author = {Karger, D and Lehman, E and Leighton, T},
journal = {Proceedings of the  {\ldots}},
title = {{Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web}},
url = {http://dl.acm.org/citation.cfm?id=258660},
year = {1997}
}
@article{Welsh2000,
abstract = {Building highly concurrent systems, such as large-scale Internet services, requires managing many information flows at once and maintaining peak throughput when demand exceeds resource availability. In addition, any platform supporting Internet services must provide high availability and be able to cope with burstiness of load. Many approaches to building concurrent systems have been proposed, which generally fall into the two categories of threaded and event- driven programming. We propose that threads and events are actually on the ends of a design spectrum, and that the best implementation strategy for these applications is somewhere in between. We present a general-purpose design framework for building highly concurrent systems, based on three design components — tasks, queues, and thread pools — which encapsulate the concurrency, performance, fault isolation, and software engineering benefits of both threads and events. We present a set of design patterns that can be applied to map an application onto an implementation using these components. In addition, we provide an analysis of several systems (including an Internet services platform and a highly available, distributed, persistent data store) constructed using our framework, demonstrating its benefit for building and reasoning about concurrent applications.},
author = {Welsh, Matt and Gribble, Steven D and Brewer, Eric a and Culler, David},
file = {:home/etn/Documents/PhD/Biblio/Welsh et al. - 2000 - A design framework for highly concurrent systems.pdf:pdf},
journal = {University of California, Berkeley},
keywords = {Event,SEDA},
mendeley-tags = {Event,SEDA},
title = {{A design framework for highly concurrent systems}},
url = {http://www.cs.berkeley.edu/{~}culler/papers/events.pdf http://www.cs.berkeley.edu/{~}culler/papers/events.pdf$\backslash$nhttp://www.eecs.harvard.edu/{~}mdw/papers/events.pdf},
year = {2000}
}
@article{Milner1992a,
author = {Milner, R and Parrow, J and Walker, D},
file = {:home/etn/Documents/PhD/Biblio/Milner, Parrow, Walker - 1992 - A calculus of mobile processes, II.ps:ps},
journal = {Information and computation},
title = {{A calculus of mobile processes, II}},
url = {http://www.sciencedirect.com/science/article/pii/0890540192900095},
year = {1992}
}
@article{Amarasinghe1994,
author = {Amarasinghe, SP and Anderson, JM},
journal = {{\ldots}  and Compilers for Parallel {\ldots}},
title = {{An overview of a compiler for scalable parallel machines}},
url = {http://link.springer.com/chapter/10.1007/3-540-57659-2{\_}15},
year = {1994}
}
@article{Amarasinghe1995,
author = {Amarasinghe, SP and Anderson, JAM and Lam, MS and Tseng, CW},
file = {:home/etn/Documents/PhD/Biblio/Amarasinghe et al. - 1995 - An Overview of the SUIF Compiler for Scalable Parallel Machines.pdf:pdf},
journal = {PPSC},
title = {{An Overview of the SUIF Compiler for Scalable Parallel Machines.}},
url = {http://www.researchgate.net/publication/2646829{\_}Chapter{\_}1{\_}An{\_}Overview{\_}of{\_}the{\_}SUIF{\_}Compiler{\_}for{\_}Scalable{\_}Parallel{\_}Machines/file/60b7d5193bc4fef52b.pdf},
year = {1995}
}
@article{Lakshman2010,
author = {Lakshman, A and Malik, P},
journal = {ACM SIGOPS Operating Systems Review},
title = {{Cassandra: a decentralized structured storage system}},
url = {http://dl.acm.org/citation.cfm?id=1773922},
year = {2010}
}
@article{Buck2004,
author = {Buck, I and Foley, T and Horn, D},
file = {:home/etn/Documents/PhD/Biblio/Buck, Foley, Horn - 2004 - Brook for GPUs stream computing on graphics hardware.pdf:pdf},
journal = {{\ldots}  on Graphics (TOG)},
title = {{Brook for GPUs: stream computing on graphics hardware}},
url = {http://dl.acm.org/citation.cfm?id=1015800},
year = {2004}
}
@article{Chen2005,
author = {Chen, MK and Li, XF and Lian, R and Lin, JH and Liu, L},
file = {:home/etn/Documents/PhD/Biblio/Chen et al. - 2005 - Shangri-La achieving high performance from compiled network applications while enabling ease of programming.pdf:pdf},
journal = {ACM SIGPLAN  {\ldots}},
title = {{Shangri-La: achieving high performance from compiled network applications while enabling ease of programming}},
url = {http://dl.acm.org/citation.cfm?id=1065038},
year = {2005}
}
@article{Consel2003,
author = {Consel, C and Hamdi, H and R{\'{e}}veill{\`{e}}re, L},
file = {:home/etn/Documents/PhD/Biblio/Consel, Hamdi, R{\'{e}}veill{\`{e}}re - 2003 - Spidle a DSL approach to specifying streaming applications.pdf:pdf},
journal = {Generative  {\ldots}},
title = {{Spidle: a DSL approach to specifying streaming applications}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-39815-8{\_}1},
year = {2003}
}
@article{Mark2003,
author = {Mark, WR and Glanville, RS},
file = {:home/etn/Documents/PhD/Biblio/Mark, Glanville - 2003 - Cg A system for programming graphics hardware in a C-like language.pdf:pdf},
journal = {{\ldots}  Transactions on Graphics ( {\ldots}},
title = {{Cg: A system for programming graphics hardware in a C-like language}},
url = {http://dl.acm.org/citation.cfm?id=882362},
year = {2003}
}
@article{Zhang2005,
author = {Zhang, D and Li, ZZ and Song, H and Liu, L},
file = {:home/etn/Documents/PhD/Biblio/Zhang et al. - 2005 - A programming model for an embedded media processing architecture.pdf:pdf},
journal = {{\ldots} Systems: Architectures, Modeling, and {\ldots}},
title = {{A programming model for an embedded media processing architecture}},
url = {http://link.springer.com/chapter/10.1007/11512622{\_}27},
year = {2005}
}
@article{Jain2006,
author = {Jain, N and Amini, L and Andrade, H and King, R},
file = {:home/etn/Documents/PhD/Biblio/Jain et al. - 2006 - Design, implementation, and evaluation of the linear road benchmark on the stream processing core.pdf:pdf},
journal = {Proceedings of the {\ldots}},
title = {{Design, implementation, and evaluation of the linear road benchmark on the stream processing core}},
url = {http://dl.acm.org/citation.cfm?id=1142522},
year = {2006}
}
@article{Wu2007,
author = {Wu, KL and Hildrum, KW and Fan, W},
file = {:home/etn/Documents/PhD/Biblio/Wu, Hildrum, Fan - 2007 - Challenges and experience in prototyping a multi-modal stream analytic and monitoring application on System S.pdf:pdf},
journal = {Proceedings of the 33rd {\ldots}},
title = {{Challenges and experience in prototyping a multi-modal stream analytic and monitoring application on System S}},
url = {http://dl.acm.org/citation.cfm?id=1325986},
year = {2007}
}
@inproceedings{Clements2013a,
address = {New York, New York, USA},
author = {Clements, Austin T. and Kaashoek, M. Frans and Zeldovich, Nickolai and Morris, Robert T. and Kohler, Eddie},
booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles - SOSP '13},
doi = {10.1145/2517349.2522712},
file = {:home/etn/Documents/PhD/Biblio/Clements et al. - 2013 - The scalable commutativity rule.pdf:pdf},
isbn = {9781450323888},
month = {nov},
pages = {1--17},
publisher = {ACM Press},
title = {{The scalable commutativity rule}},
url = {http://dl.acm.org/citation.cfm?id=2517349.2522712},
year = {2013}
}
@article{Rinard1996,
author = {Rinard, MC and Diniz, PC},
file = {:home/etn/Documents/PhD/Biblio/Rinard, Diniz - 1996 - Commutativity analysis A new analysis framework for parallelizing compilers.pdf:pdf},
journal = {ACM SIGPLAN Notices},
title = {{Commutativity analysis: A new analysis framework for parallelizing compilers}},
url = {http://dl.acm.org/citation.cfm?id=231390},
year = {1996}
}
@article{VanCutsem2009,
abstract = {In modern programming languages, concurrency control can be traced back to one of two different schools: actor-based message passing concurrency and thread-based shared-state concurrency. This paper describes a linguistic symbiosis between two programming languages with such different concurrency models. More specifically, we describe a novel symbiosis between actors represented as event loops on the one hand and threads on the other. This symbiosis ensures that the invariants of the actor-based concurrency model are not violated by engaging in symbiosis with multithreaded programs. The proposed mapping is validated by means of a concrete symbiosis between AmbientTalk, a flexible, domain-specific language for writing distributed programs and Java, a conventional object-oriented language. This symbiosis allows the domain-specific language to reuse existing software components written in a multithreaded language without sacrificing the beneficial event-driven properties of the actor concurrency model.},
author = {{Van Cutsem}, Tom and Mostinckx, Stijn and {De Meuter}, Wolfgang},
doi = {10.1016/j.cl.2008.06.005},
file = {:home/etn/Documents/PhD/Biblio/Van Cutsem, Mostinckx, De Meuter - 2009 - Linguistic symbiosis between event loop actors and threads.pdf:pdf},
issn = {14778424},
journal = {Computer Languages, Systems {\&} Structures},
keywords = {Actors,AmbientTalk,Events,Futures,Linguistic symbiosis,Threads},
month = {apr},
number = {1},
pages = {80--98},
title = {{Linguistic symbiosis between event loop actors and threads}},
url = {http://www.sciencedirect.com/science/article/pii/S1477842408000249},
volume = {35},
year = {2009}
}
@article{Ongaro2014,
author = {Ongaro, D and Ousterhout, J},
file = {:home/etn/Documents/PhD/Biblio/Ongaro, Ousterhout - 2014 - In search of an understandable consensus algorithm.pdf:pdf},
journal = {Proc. USENIX Annual Technical Conference},
keywords = {raft},
mendeley-tags = {raft},
title = {{In search of an understandable consensus algorithm}},
url = {https://www.usenix.org/system/files/conference/atc14/atc14-paper-ongaro.pdf},
year = {2014}
}
@article{Lauer1979,
author = {Lauer, HC and Needham, RM},
file = {:home/etn/Documents/PhD/Biblio/Lauer, Needham - 1979 - On the duality of operating system structures.pdf:pdf},
journal = {ACM SIGOPS Operating Systems Review},
title = {{On the duality of operating system structures}},
url = {http://dl.acm.org/citation.cfm?id=850658},
year = {1979}
}
@article{Behren2003,
author = {von Behren, JR and Condit, J and Brewer, EA},
file = {:home/etn/Documents/PhD/Biblio/Behren, Condit, Brewer - 2003 - Why Events Are a Bad Idea (for High-Concurrency Servers).pdf:pdf},
journal = {HotOS},
title = {{Why Events Are a Bad Idea (for High-Concurrency Servers).}},
url = {http://static.usenix.org/publications/library/proceedings/hotos03/tech/full{\_}papers/vonbehren/vonbehren{\_}html/},
year = {2003}
}
@article{Ousterhout1996,
author = {Ousterhout, J},
file = {:home/etn/Documents/PhD/Biblio/Ousterhout - 1996 - Why threads are a bad idea (for most purposes).pdf:pdf},
journal = {Presentation given at the 1996 Usenix Annual Conference},
title = {{Why threads are a bad idea (for most purposes)}},
url = {http://chess.cs.umd.edu/class/spring2009/cmsc433/Lectures/threadVsEvents.pdf},
year = {1996}
}
@article{Li2007,
author = {Li, P and Zdancewic, S},
file = {:home/etn/Documents/PhD/Biblio/Li, Zdancewic - 2007 - Combining events and threads for scalable network services implementation and evaluation of monadic, application-.pdf:pdf},
journal = {ACM SIGPLAN Notices},
title = {{Combining events and threads for scalable network services implementation and evaluation of monadic, application-level concurrency primitives}},
url = {http://dl.acm.org/citation.cfm?id=1250756},
year = {2007}
}
@article{Adya2002,
author = {Adya, A and Howell, J and Theimer, M},
file = {:home/etn/Documents/PhD/Biblio/Adya, Howell, Theimer - 2002 - Cooperative Task Management Without Manual Stack Management.pdf:pdf},
journal = {USENIX Annual Technical Conference},
title = {{Cooperative Task Management Without Manual Stack Management.}},
url = {http://static.usenix.org/publications/library/proceedings/usenix02/full{\_}papers/adyahowell/adyahowell{\_}html/},
year = {2002}
}
@article{Behren2003a,
author = {Behren, R Von and Condit, J and Zhou, F},
file = {:home/etn/Documents/PhD/Biblio/Behren, Condit, Zhou - 2003 - Capriccio scalable threads for internet services.pdf:pdf},
journal = {ACM SIGOPS {\ldots}},
keywords = {Capriccio,Thread},
mendeley-tags = {Capriccio,Thread},
title = {{Capriccio: scalable threads for internet services}},
url = {http://dl.acm.org/citation.cfm?id=945471},
year = {2003}
}
@article{Gustafsson2005,
author = {Gustafsson, A},
file = {:home/etn/Documents/PhD/Biblio/Gustafsson - 2005 - Threads without the pain.pdf:pdf},
journal = {Queue},
title = {{Threads without the pain}},
url = {http://dl.acm.org/citation.cfm?id=1105678},
year = {2005}
}
@article{Gilbert2012,
author = {Gilbert, S and Lynch, NA},
file = {:home/etn/Documents/PhD/Biblio/Gilbert, Lynch - 2012 - Perspectives on the CAP Theorem.pdf:pdf},
title = {{Perspectives on the CAP Theorem}},
url = {http://dspace.mit.edu/handle/1721.1/79112},
year = {2012}
}
@article{Krohn2007,
author = {Krohn, MN and Kohler, E and Kaashoek, MF},
file = {:home/etn/Documents/PhD/Biblio/Krohn, Kohler, Kaashoek - 2007 - Events Can Make Sense.pdf:pdf},
journal = {USENIX Annual Technical Conference},
keywords = {TAME,event},
mendeley-tags = {TAME,event},
title = {{Events Can Make Sense.}},
url = {https://www.usenix.org/legacy/events/usenix07/tech/full{\_}papers/krohn/krohn{\_}html/},
year = {2007}
}
@article{Gribble2001,
abstract = {The Ninja project seeks to enable the broad innovation of robust, scalable, distributed Internet services, and to permit the emerging class of extremely heterogeneous devices to seamlessly access these services. Our architecture consists of four basic elements: bases, which are powerful workstation cluster environments with a software platform that simplifies scalable service construction; units, which are the devices by which users access the services; active proxies, which are transformational elements that are used for unit- or service-specific adaptation; and paths, which are an abstraction through which units, services, and active proxies are composed.},
author = {Gribble, Steven D. and Welsh, Matt and {Von Behren}, Rob and Brewer, Eric a. and Culler, David and Borisov, N. and Czerwinski, S. and Gummadi, R. and Hill, J. and Joseph, A. and Katz, R. H. and Mao, Z. M. and Ross, S. and Zhao, B.},
doi = {10.1016/S1389-1286(00)00179-1},
file = {:home/etn/Documents/PhD/Biblio/Gribble, Welsh, Behren - 2001 - The Ninja architecture for robust Internet-scale systems and services.pdf:pdf},
isbn = {1389-1286},
issn = {13891286},
journal = {Computer Networks},
keywords = {distributed systems,ninja architecture,pervasive computing,scalable services,thin clients},
number = {4},
pages = {473--497},
title = {{Ninja architecture for robust Internet-scale systems and services}},
url = {http://www.sciencedirect.com/science/article/pii/S1389128600001791},
volume = {35},
year = {2001}
}
@article{Hoare1978,
author = {Hoare, C. A. R.},
doi = {10.1145/359576.359585},
file = {:home/etn/Documents/PhD/Biblio/Hoare - 1978 - Communicating sequential processes.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {classes,concurrency,conditional critical regions,coroutines,data representations,guarded commands,input,iterative arrays,monitors,multiple entries,multiple exits,nondeterminacy,output,parallel programming,procedures,program structures,programming,programming languages,programming primitives,recursion},
month = {aug},
number = {8},
pages = {666--677},
publisher = {ACM},
title = {{Communicating sequential processes}},
url = {http://dl.acm.org/citation.cfm?id=359576.359585},
volume = {21},
year = {1978}
}
@misc{Pai1999,
abstract = {This paper presents the design of a new Web server architecture called the asymmetric multi-process event- driven (AMPED) architecture, and evaluates the perfor- mance of an implementation of this architecture, the Flash Web server. The Flash Web server combines the high performance of single-process event-driven servers on cached workloads with the performance of multi- process and multi-threaded servers on disk-bound work- loads. Furthermore, the Flash Web server is easily portable since it achieves these results using facilities available in all modern operating systems. The performance of different Web server architec- tures is evaluated in the context of a single implemen- tation in order to quantify the impact of a server’s con- currency architecture on its performance. Furthermore, the performance of Flash is compared with two widely- used Web servers, Apache and Zeus. Results indicate that Flash can match or exceed the performance of exist- ing Web servers by up to 50{\%} across a wide range of real workloads. We also present results that show the contri- bution of various optimizations embedded in Flash.},
author = {Pai, Vivek S and Druschel, Peter and Zwaenepoel, Willy},
booktitle = {Proceedings of the 1999 USENIX Annual Technical Conference},
doi = {10.1.1.119.6738},
file = {:home/etn/Documents/PhD/Biblio/Pai, Druschel, Zwaenepoel - 1999 - Flash An Efficient and Portable Web Server.pdf:pdf},
keywords = {Flash,event},
mendeley-tags = {Flash,event},
pages = {14},
title = {{Flash : An Efficient and Portable Web Server}},
url = {https://www.usenix.org/event/usenix99/full{\_}papers/pai/pai{\_}html/node7.html https://www.usenix.org/legacy/event/usenix99/full{\_}papers/pai/pai.pdf},
urldate = {2015-03-04},
year = {1999}
}
@article{Gilbert2002a,
author = {Gilbert, Seth and Lynch, Nancy},
doi = {10.1145/564585.564601},
file = {:home/etn/Documents/PhD/Biblio/Gilbert, Lynch - 2002 - Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services(2).pdf:pdf},
issn = {01635700},
journal = {ACM SIGACT News},
keywords = {CAP},
mendeley-tags = {CAP},
month = {jun},
number = {2},
pages = {51},
publisher = {ACM},
title = {{Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services}},
url = {http://dl.acm.org/citation.cfm?id=564585.564601},
volume = {33},
year = {2002}
}
@article{Salmito2013,
author = {Salmito, Tiago and de Moura, Ana Lucia and Rodriguez, Noemi},
doi = {10.1109/ICPP.2013.80},
file = {:home/etn/Documents/PhD/Biblio/Salmito, Moura, Rodriguez - 2013 - A Flexible Approach to Staged Events.pdf:pdf},
isbn = {978-0-7695-5117-3},
issn = {0190-3918},
journal = {2013 42nd International Conference on Parallel Processing},
keywords = {-concurrency,event-driven,stages,threads},
language = {English},
month = {oct},
pages = {661--670},
publisher = {IEEE},
title = {{A Flexible Approach to Staged Events}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6687404 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6687404},
year = {2013}
}
@article{Welsh2001,
author = {Welsh, M and Culler, D and Brewer, E},
file = {:home/etn/Documents/PhD/Biblio/Welsh, Culler, Brewer - 2001 - SEDA an architecture for well-conditioned, scalable internet services.pdf:pdf},
journal = {ACM SIGOPS Operating Systems Review},
title = {{SEDA: an architecture for well-conditioned, scalable internet services}},
url = {http://dl.acm.org/citation.cfm?id=502057},
year = {2001}
}
@article{Fernandez2014,
author = {Fernandez, RC and Weidlich, M},
file = {:home/etn/Documents/PhD/Biblio/Fernandez, Weidlich - 2014 - Scalable stateful stream processing for smart grids.pdf:pdf},
journal = {Proceedings of the 8th {\ldots}},
title = {{Scalable stateful stream processing for smart grids}},
url = {http://dl.acm.org/citation.cfm?id=2611326},
year = {2014}
}
@article{Fernandez2013,
author = {Fernandez, R Castro},
file = {:home/etn/Documents/PhD/Biblio/Fernandez - 2013 - Integrating scale out and fault tolerance in stream processing using operator state management.pdf:pdf},
journal = {Proceedings of the  {\ldots}},
title = {{Integrating scale out and fault tolerance in stream processing using operator state management}},
url = {http://dl.acm.org/citation.cfm?id=2465282},
year = {2013}
}
@article{Migliavacca2010,
author = {Migliavacca, M and Eyers, D},
journal = {Middleware'10 Posters  {\ldots}},
title = {{SEEP: scalable and elastic event processing}},
url = {http://dl.acm.org/citation.cfm?id=1930032},
year = {2010}
}
@article{Fernandez2013a,
author = {Fernandez, RC and Migliavacca, M},
file = {:home/etn/Documents/PhD/Biblio/Fernandez, Migliavacca - 2013 - Scalable and Fault-tolerant Stateful Stream Processing.pdf:pdf},
journal = {ICCSW},
title = {{Scalable and Fault-tolerant Stateful Stream Processing.}},
url = {http://drops.dagstuhl.de/opus/volltexte/2013/4266/},
year = {2013}
}
@article{Golab2003,
author = {Golab, L and {\"{O}}zsu, MT},
file = {:home/etn/Documents/PhD/Biblio/Golab, {\"{O}}zsu - 2003 - Issues in data stream management.pdf:pdf},
journal = {ACM Sigmod Record},
title = {{Issues in data stream management}},
url = {http://dl.acm.org/citation.cfm?id=776986},
year = {2003}
}
@article{Fernandez2014a,
author = {Fernandez, Raul Castro and Migliavacca, Matteo and Kalyvianaki, Evangelia and Pietzuch, Peter},
file = {:home/etn/Documents/PhD/Biblio/Fernandez et al. - 2014 - Making state explicit for imperative big data processing.pdf:pdf},
journal = {USENIX ATC},
title = {{Making state explicit for imperative big data processing}},
url = {https://www.usenix.org/system/files/conference/atc14/atc14-paper-castro{\_}fernandez.pdf},
year = {2014}
}
@article{Holtman2008,
abstract = {The universal scalability law (USL) is an analytic model used to quantify application scaling. It is universal because it subsumes Amdahl's law and Gustafson linearized scaling as special cases. Using simulation, we show: (i) that the USL is equivalent to synchronous queueing in a load-dependent machine repairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be regarded as boundaries defining three scalability zones. Typical throughput measurements lie across all three zones. Simulation scenarios provide deeper insight into queueing effects and thus provide a clearer indication of which application features should be tuned to get into the optimal performance zone.},
archivePrefix = {arXiv},
arxivId = {0809.2541},
author = {Holtman, Jim and Gunther, Neil J.},
eprint = {0809.2541},
file = {:home/etn/Documents/PhD/Biblio/Holtman, Gunther - 2008 - Getting in the Zone for Successful Scalability.pdf:pdf},
month = {sep},
pages = {14},
title = {{Getting in the Zone for Successful Scalability}},
url = {http://arxiv.org/abs/0809.2541},
year = {2008}
}
@article{Gunther,
author = {Gunther, NJ},
journal = {Springer, Berlin Heidelberg},
title = {{Guerrilla Capacity Planning}},
url = {https://scholar.google.fr/scholar?q=Guerrilla+capacity+planning{\&}btnG={\&}hl=en{\&}as{\_}sdt=0,5{\#}2},
year = {2007}
}
@book{Gunther2007,
author = {Gunther, NJ},
title = {{What is guerrilla capacity planning?}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-540-31010-5{\_}1.pdf},
year = {2007}
}
@article{Gunther2002,
author = {Gunther, NJ},
file = {:home/etn/Documents/PhD/Biblio/Gunther - 2002 - A New Interpretation of Amdahl's Law and Geometric Scalability.pdf:pdf},
journal = {arXiv preprint cs/0210017},
title = {{A New Interpretation of Amdahl's Law and Geometric Scalability}},
url = {http://arxiv.org/abs/cs/0210017},
year = {2002}
}
@article{Gunther1996,
author = {Gunther, NJ},
journal = {In other words},
title = {{Understanding the MP effect: Multiprocessing in pictures}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.511{\&}rep=rep1{\&}type=pdf},
year = {1996}
}
@article{Gunther1993,
author = {Gunther, NJ},
file = {:home/etn/Documents/PhD/Biblio/Gunther - 1993 - A simple capacity model of massively parallel transaction systems.pdf:pdf},
journal = {CMG-CONFERENCE-},
title = {{A simple capacity model of massively parallel transaction systems}},
url = {http://www.perfdynamics.com/Papers/njgCMG93.pdf},
year = {1993}
}
@article{Gunther2008,
abstract = {The universal scalability law of computational capacity is a rational function C{\_}p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree polynomial in the number of physical processors p, that has been long used for statistical modeling and prediction of computer system performance. We prove that C{\_}p is equivalent to the synchronous throughput bound for a machine-repairman with state-dependent service rate. Simpler rational functions, such as Amdahl's law and Gustafson speedup, are corollaries of this queue-theoretic bound. C{\_}p is further shown to be both necessary and sufficient for modeling all practical characteristics of computational scalability.},
archivePrefix = {arXiv},
arxivId = {0808.1431},
author = {Gunther, Neil J.},
eprint = {0808.1431},
file = {:home/etn/Documents/PhD/Biblio/Gunther - 2008 - A General Theory of Computational Scalability Based on Rational Functions.pdf:pdf},
journal = {ArXiv e-prints},
pages = {14},
title = {{A General Theory of Computational Scalability Based on Rational Functions}},
url = {http://arxiv.org/abs/0808.1431},
year = {2008}
}
@article{Yavits2014,
author = {Yavits, L and Morad, A and Ginosar, R},
file = {:home/etn/Documents/PhD/Biblio/Yavits, Morad, Ginosar - 2014 - The effect of communication and synchronization on Amdahl's law in multicore systems.pdf:pdf},
journal = {Parallel Computing},
title = {{The effect of communication and synchronization on Amdahl's law in multicore systems}},
url = {http://www.sciencedirect.com/science/article/pii/S0167819113001324},
year = {2014}
}
@article{Nelson1996,
author = {Nelson, R},
journal = {Communications of the ACM},
title = {{Including queueing effects in Amdahl's law}},
url = {http://dl.acm.org/citation.cfm?id=272715},
year = {1996}
}
@article{Kleinrock1992,
author = {Kleinrock, L and Huang, JH},
journal = {{\ldots}  Engineering, IEEE Transactions on},
title = {{On parallel processing systems: Amdahl's law generalized and some results on optimal design}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=135776},
year = {1992}
}
@inproceedings{Choudhury2011,
abstract = {The Universal Scalability Law (USL) of computational capacity has been proposed by Neil J. Gunther. USL abstracts the coefficients of inter process interactions and other contentions in the area of parallel and distributed computing in a set of constant parameters {\{}$\sigma$, $\lambda${\}}. One cannot apply USL for the purpose of predicting performance, unless the values of those constant parameters are known. A computationally light weight and theoretically correct algorithm to estimate those parameters from measured performance data is not available yet. Simple linear-regression or standard least-square-error-approximation is a widely used efficient statistical technique to estimate parameters. A simple linear-regression cannot be applied directly to estimate the coefficients $\sigma$, $\lambda$ of USL, because USL is a rational function. In this work, we propose a novel and elegant algorithm based on standard least-square-error-approximation or linear-regression to estimate the parameters. The explanation of failure of simple linear-regression is discussed by visiting the basic theory of linear-regression. A novel approach, consisting of algebraic manipulations to transform the problem into two linear-regression problems, is presented. The linear-regression is applied successively in a certain order to estimate the constant parameters, $\sigma$, $\lambda$, of USL. The proposed technique is applied to a set of measured performance data to validate and verify the proposed technique.},
author = {Choudhury, Jayanta},
booktitle = {2011 IEEE INTERNATIONAL CONFERENCE ON ELECTRO/INFORMATION TECHNOLOGY},
doi = {10.1109/EIT.2011.5978568},
file = {:home/etn/Documents/PhD/Biblio/Choudhury - 2011 - Novel regression approach to estimate the parameters of “Universal Scalability Law”.pdf:pdf},
isbn = {978-1-61284-465-7},
issn = {2154-0357},
keywords = {Computational modeling,Conferences,Equations,Gunther's Law,Mathematical model,Performance modeling,Prediction algorithms,Scalability,Throughput,Universal Scalability Law,computational capacity,computational complexity,computer performance,distributed computing,interprocess interactions,least square error approximation,least squares approximations,linear regression technique,multicore processor performance,parallel computing,parallel processing,parameter estimation,regression analysis,relative performance,universal scalability law},
month = {may},
pages = {1--5},
publisher = {IEEE},
shorttitle = {Electro/Information Technology (EIT), 2011 IEEE In},
title = {{Novel regression approach to estimate the parameters of “Universal Scalability Law”}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5978568},
year = {2011}
}
@article{Hill2008,
author = {Hill, MD and Marty, MR},
file = {:home/etn/Documents/PhD/Biblio/Hill, Marty - 2008 - Amdahl's law in the multicore era.pdf:pdf},
journal = {Computer},
title = {{Amdahl's law in the multicore era}},
url = {http://mobile.computer.org/csdl/mags/co/2008/07/mco2008070033.html},
year = {2008}
}
@article{Yao2009,
author = {Yao, E and Bao, Y and Tan, G and Chen, M},
file = {:home/etn/Documents/PhD/Biblio/Yao et al. - 2009 - Extending Amdahl's law in the multicore era.pdf:pdf},
journal = {ACM SIGMETRICS Performance {\ldots}},
title = {{Extending Amdahl's law in the multicore era}},
url = {http://dl.acm.org/citation.cfm?id=1639571},
year = {2009}
}
@article{Cassidy2012,
author = {Cassidy, AS and Andreou, AG},
file = {:home/etn/Documents/PhD/Biblio/Cassidy, Andreou - 2012 - Beyond Amdahl's law an objective function that links multiprocessor performance gains to delay and energy.pdf:pdf},
journal = {Computers, IEEE Transactions on},
title = {{Beyond Amdahl's law: an objective function that links multiprocessor performance gains to delay and energy}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6007130},
year = {2012}
}
@article{Gunther2010,
author = {Gunther, N},
journal = {{\ldots}  Web Performance and  {\ldots}},
title = {{Hidden scalability gotchas in memcached and friends}},
url = {http://assets.en.oreilly.com/1/event/44/Hidden Scalability Gotchas in Memcached and Friends Presentation.pdf},
year = {2010}
}
@inproceedings{Amdahl1967,
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
author = {Amdahl, Gene M.},
booktitle = {AFIPS Spring Joint Computer Conference, 1967. AFIPS '67 (Spring). Proceedings of the},
doi = {doi: 10.1145/1465482.1465560},
file = {:home/etn/Documents/PhD/Biblio/Amdahl - 1967 - Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities.pdf:pdf},
isbn = {1558605398},
issn = {18816096},
keywords = {parallel-computing},
pages = {483--485},
pmid = {21914993},
title = {{Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities}},
url = {http://dl.acm.org/citation.cfm?id=1465560 http://delivery.acm.org/10.1145/1470000/1465560/p483-amdahl.pdf?ip=202.189.127.238{\&}id=1465560{\&}acc=ACTIVE SERVICE{\&}key=CDD1E79C27AC4E65.DE0A32330AE3471B.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=646862329{\&}CFTOKEN=33418},
volume = {30},
year = {1967}
}
@article{Gustafson1988,
author = {Gustafson, JL},
file = {:home/etn/Documents/PhD/Biblio/Gustafson - 1988 - Reevaluating Amdahl's law.pdf:pdf},
journal = {Communications of the ACM},
title = {{Reevaluating Amdahl's law}},
url = {http://dl.acm.org/citation.cfm?id=42415},
year = {1988}
}
@misc{Khanyile2012,
abstract = {IAENG International Journal of Computer Science, vol. 39(3), pp 312-320},
author = {Khanyile, NP and Tapamo, J-R and Dube, E},
file = {:home/etn/Documents/PhD/Biblio/Khanyile, Tapamo, Dube - 2012 - An analytic model for predicting the performance of distributed applications on multicore clusters.pdf:pdf},
issn = {1819-656X},
keywords = {Algorithms,Amdahl's law,Bandwith,Distributed programming,Latency,Multicore clusters,Propagation delay},
language = {en},
month = {aug},
publisher = {International Association of Engineers (IAENG)},
title = {{An analytic model for predicting the performance of distributed applications on multicore clusters}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/6099},
year = {2012}
}
@article{Suleman2009,
author = {Suleman, MA and Mutlu, O},
file = {:home/etn/Documents/PhD/Biblio/Suleman, Mutlu - 2009 - Accelerating critical section execution with asymmetric multi-core architectures.pdf:pdf},
journal = {ACM SIGARCH Computer  {\ldots}},
title = {{Accelerating critical section execution with asymmetric multi-core architectures}},
url = {http://dl.acm.org/citation.cfm?id=1508274},
year = {2009}
}
@article{Chandy2006,
author = {Chandy, KM},
file = {:home/etn/Documents/PhD/Biblio/Chandy - 2006 - Event-driven applications Costs, benefits and design approaches.pdf:pdf},
journal = {Gartner Application Integration and Web Services {\ldots}},
title = {{Event-driven applications: Costs, benefits and design approaches}},
url = {https://scholar.google.com/scholar?q=Event-Driven Applications: Costs, Benefits and Design Approaches{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}0},
year = {2006}
}
@article{Hewitt1973a,
author = {Hewitt, C and Bishop, P and Steiger, R},
file = {:home/etn/Documents/PhD/Biblio/Hewitt, Bishop, Steiger - 1973 - A universal modular actor formalism for artificial intelligence.pdf:pdf},
journal = {Proceedings of the 3rd international joint conference on Artificial intelligence},
title = {{A universal modular actor formalism for artificial intelligence}},
url = {http://dl.acm.org/citation.cfm?id=1624804},
year = {1973}
}
@article{Power2010,
author = {Power, R and Li, J},
file = {:home/etn/Documents/PhD/Biblio/Power, Li - 2010 - Piccolo Building Fast, Distributed Programs with Partitioned Tables.pdf:pdf},
journal = {OSDI},
title = {{Piccolo: Building Fast, Distributed Programs with Partitioned Tables.}},
url = {http://static.usenix.org/event/osdi10/tech/full{\_}papers/Power.pdf},
year = {2010}
}
@article{Fu2001,
author = {Fu, X and Shi, W and Akkerman, A and Karamcheti, V},
file = {:home/etn/Documents/PhD/Biblio/Fu et al. - 2001 - CANS Composable, Adaptive Network Services Infrastructure.pdf:pdf},
journal = {USITS},
title = {{CANS: Composable, Adaptive Network Services Infrastructure.}},
url = {http://static.usenix.org/event/usits01/full{\_}papers/fu/fu.pdf},
year = {2001}
}
@article{Radoi2014,
author = {Radoi, C and Fink, SJ and Rabbah, R and Sridharan, M},
file = {:home/etn/Documents/PhD/Biblio/Radoi et al. - 2014 - Translating imperative code to MapReduce.pdf:pdf},
journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
title = {{Translating imperative code to MapReduce}},
url = {http://dl.acm.org/citation.cfm?id=2660228},
year = {2014}
}
@book{Banerjee2013,
author = {Banerjee, U},
title = {{Loop parallelization}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=9xXrBwAAQBAJ{\&}oi=fnd{\&}pg=PR13{\&}dq=loop+parallelization{\&}ots=flLbnvNcCM{\&}sig=a1VFNFLS3{\_}TRjBVdpPr8mrH-0DE},
year = {2013}
}
@article{Bartenstein2014,
author = {Bartenstein, Thomas W and Liu, Yu David},
doi = {10.1145/2660193.2660225},
file = {:home/etn/Documents/PhD/Biblio/Bartenstein, Liu - 2014 - Rate Types for Stream Programs.pdf:pdf},
isbn = {9781450325851},
journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
keywords = {called data streams,data processing rates,data throughput,figure 1,items,large sequence of data,performance reasoning,rate,stream,stream programming,throughput ratio and natural,type systems},
pages = {213--232},
title = {{Rate Types for Stream Programs}},
url = {http://dl.acm.org/citation.cfm?id=2660225},
year = {2014}
}
@article{Hewitt1977,
author = {Hewitt, Carl},
file = {:home/etn/Documents/PhD/Biblio/Hewitt - 1977 - Viewing control structures as patterns of passing messages.pdf:pdf},
journal = {Artificial intelligence},
title = {{Viewing control structures as patterns of passing messages}},
url = {http://www.sciencedirect.com/science/article/pii/0004370277900339},
year = {1977}
}
@inproceedings{Caspi1987,
address = {New York, New York, USA},
author = {Caspi, P. and Pilaud, D. and Halbwachs, N. and Plaice, J. A.},
booktitle = {Proceedings of the 14th ACM SIGACT-SIGPLAN symposium on Principles of programming languages - POPL '87},
doi = {10.1145/41625.41641},
isbn = {0897912152},
month = {oct},
pages = {178--188},
publisher = {ACM Press},
title = {{LUSTRE: a declarative language for real-time programming}},
url = {http://dl.acm.org/citation.cfm?id=41625.41641},
year = {1987}
}
@article{XU2015,
author = {XU, LE},
file = {:home/etn/Documents/PhD/Biblio/XU - 2015 - Stela ondemand elasticity in distributed data stram processing systems.pdf:pdf},
title = {{Stela: ondemand elasticity in distributed data stram processing systems}},
url = {http://dprg.cs.uiuc.edu/docs/le{\_}msthesis/StelaThesis.pdf},
year = {2015}
}
@article{Reed2012,
author = {Reed, DP},
file = {:home/etn/Documents/PhD/Biblio/Reed - 2012 - Simultaneous Considered Harmful Modular Parallelism.pdf:pdf},
journal = {HotPar},
title = {{" Simultaneous" Considered Harmful: Modular Parallelism.}},
url = {https://scholar.google.com/scholar?q={\%}E2{\%}80{\%}9CSimultaneous{\%}E2{\%}80{\%}9D+Considered+Harmful{\%}3A+Modular+Parallelism{\&}btnG={\&}hl=en{\&}as{\_}sdt=0{\%}2C39{\#}0},
year = {2012}
}
@article{Matsakis2012a,
abstract = {This paper presents a lightweight task framework and accompanying type system that statically guarantee deterministic execution. The framework is based on the familiar model of fork-join parallelism, but with two important twists. First, child tasks do not begin execution immediately upon creation, but rather they are both scheduled and joined as one atomic action; this change prevents the parent task from racing with its children. Second, the body of a child task is specified as a parallel closure. Parallel closures are a novel variation on traditional closures in which the data inherited from the environment is read-only. Parallel closures have the important property that they can be executed in parallel with one another without creating data races, even if they share the same environment. We also have a controlled means to grant mutable access to data in the environment where necessary. We have implemented a prototype of our framework in Java. The prototype includes a typechecker that enforces the constraint that parallel closures cannot modify their environment. The paper describes how the prototype has been used to implement a number of realistic examples and also explains how parallel closures can support the creation of structured parallel programming abstractions.},
author = {Matsakis, Nicholas D},
file = {:home/etn/Documents/PhD/Biblio/Matsakis - 2012 - Parallel Closures A new twist on an old idea.pdf:pdf},
journal = {HotPar'12 Proceedings of the 4th USENIX conference on Hot Topics in Parallelism},
pages = {5--5},
title = {{Parallel Closures A new twist on an old idea}},
url = {http://dl.acm.org/citation.cfm?id=2342793 https://www.usenix.org/system/files/conference/hotpar12/hotpar12-final5.pdf},
year = {2012}
}
@article{McCool2010,
author = {McCool, MD},
file = {:home/etn/Documents/PhD/Biblio/McCool - 2010 - Structured parallel programming with deterministic patterns.pdf:pdf},
journal = {Proceedings of the 2nd USENIX conference on Hot  {\ldots}},
title = {{Structured parallel programming with deterministic patterns}},
url = {https://www.usenix.org/event/hotpar10/tech/full{\_}papers/McCool.pdf},
year = {2010}
}
@article{Moore1965,
author = {Moore, G},
file = {:home/etn/Documents/PhD/Biblio/Moore - 1965 - Cramming More Components Onto Integrated Circuits.pdf:pdf},
journal = {Electronics},
number = {38},
pages = {8},
title = {{Cramming More Components Onto Integrated Circuits}},
url = {https://scholar.google.com/scholar?q=cramming+more+components+1965{\&}btnG={\&}hl=en{\&}as{\_}sdt=0{\%}2C39{\#}5},
year = {1965}
}
@misc{Engberg1986,
abstract = {Milner's Calculus of  Communicating Systems (CCS) is extended with a mechanism for label passing - as an attempt to remedy some of the shortcomings of CCS w.r.t. dynamic change of agent interconnections. In the extended calculus, restriction is viewed formally as a binder, and the calculus allows dynamic change of scope (of label) in connection with communication. It is proved that algebraic properties of strong (and observational) equivalence for CCS are preserved by the extension.  Examples illustrating the expressive power of the calculus and its  methods for reasoning are given.},
author = {Engberg, Uffe and Nielsen, Mogens},
booktitle = {DAIMI Report Series},
file = {:home/etn/Documents/PhD/Biblio/Engberg, Nielsen - 1986 - A Calculus of Communicating Systems with Label Passing.pdf:pdf},
issn = {2245-9316},
language = {en},
month = {may},
number = {208},
title = {{A Calculus of Communicating Systems with Label Passing}},
url = {http://ojs.statsbiblioteket.dk/index.php/daimipb/article/view/7559},
volume = {15},
year = {1986}
}
@article{Milner1975,
author = {Milner, R},
journal = {Studies in Logic and the Foundations of Mathematics},
title = {{Processes: A mathematical model of computing agents}},
url = {https://scholar.google.com/scholar?q=Processes{\%}3A A Mathematical Model of Computing Agents{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}0},
year = {1975}
}
@article{Milner1980,
abstract = {An abstract is not available.},
author = {Milner, Robin},
doi = {10.1007/3-540-15670-4{\_}10},
file = {:home/etn/Documents/PhD/Biblio/Milner - 1986 - A calculus of communicating systems.pdf:pdf},
isbn = {978-3-540-15670-3},
issn = {0302-9743},
journal = {LFCS Report Series},
number = {7},
pages = {1--171},
publisher = {Springer},
title = {{A calculus of communicating systems}},
volume = {86},
year = {1986}
}
@article{Karp1969,
abstract = {This paper introduces a model called the parallel program schema for the representation and study of programs containing parallel sequencing. The model is related to Ianov's program schema, but extends it, both by modelling memory structure in more detail and by admitting parallel computation. The emphasis is on decision procedures, both for traditional properties, such as equivalence, and for new properties particular to parallel computation, such as determinacy and boundedness.},
author = {Karp, Richard M. and Miller, Raymond E.},
doi = {10.1016/S0022-0000(69)80011-5},
file = {:home/etn/Documents/PhD/Biblio/Karp, Miller - 1969 - Parallel program schemata.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
month = {may},
number = {2},
pages = {147--195},
title = {{Parallel program schemata}},
url = {http://www.sciencedirect.com/science/article/pii/S0022000069800115},
volume = {3},
year = {1969}
}
@book{Brauer1980,
address = {Berlin, Heidelberg},
doi = {10.1007/3-540-100016},
editor = {Brauer, Wilfried},
isbn = {978-3-540-10001-0},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Net Theory and Applications}},
url = {http://www.springerlink.com/index/10.1007/3-540-100016},
volume = {84},
year = {1980}
}
@article{Wirth1977,
author = {Wirth, N.},
doi = {10.1002/spe.4380070102},
file = {:home/etn/Documents/PhD/Biblio/Wirth - 1977 - Modula A language for modular multiprogramming.pdf:pdf},
issn = {00380644},
journal = {Software: Practice and Experience},
month = {jan},
number = {1},
pages = {1--35},
title = {{Modula: A language for modular multiprogramming}},
url = {http://doi.wiley.com/10.1002/spe.4380070102},
volume = {7},
year = {1977}
}
@article{Hansen1978,
author = {Hansen, Per Brinch},
doi = {10.1145/359642.359651},
file = {:home/etn/Documents/PhD/Biblio/Hansen - 1978 - Distributed processes a concurrent programming concept.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {buffers,classes,concurrent programming,coroutines,distributed processes,guarded regions,input/output,microprocessor networks,monitors,nondeterminism,path expressions,process communication and scheduling,processes,programming languages,semaphores,sorting arrays},
month = {nov},
number = {11},
pages = {934--941},
publisher = {ACM},
title = {{Distributed processes: a concurrent programming concept}},
url = {http://dl.acm.org/citation.cfm?id=359642.359651},
volume = {21},
year = {1978}
}
@techreport{Kahn1976,
abstract = {Les concepts de coroutine et de processus interviennent dans une grande vari{\'{e}}t{\'{e}} d'applications, o{\`{u}} il est en g{\'{e}}n{\'{e}}ral n{\'{e}}cessaire de produire ou de transformer des donn{\'{e}}es de fa{\c{c}}on progressive. Nous pr{\'{e}}sentons un langage, fond{\'{e}} sur une vue s{\'{e}}mantique pr{\'{e}}cises de l'interaction entre processus, qui facilite la programmation de r{\'{e}}seaux de processus qui {\'{e}}voluent dynamiquement. Ces r{\'{e}}seaux ont un comportement externe unique, qu'ils soient ex{\'{e}}cut{\'{e}}s de mani{\`{e}}re s{\'{e}}quentielle ou parall{\`{e}}le. Les avantages d'une s{\'{e}}mantique d{\'{e}}notationelle simple sont illustr{\'{e}}s par des preuves de programmes. Ce langage de programmation permet aussi de clarifier les relations entre plusieurs concepts : coroutines, appel par n{\'{e}}cessit{\'{e}}, structures de donn{\'{e}}es dynamiques et calcul parall{\`{e}}le.},
author = {Kahn, Gilles and Macqueen, David},
file = {:home/etn/Documents/PhD/Biblio/Kahn, Macqueen - 1976 - Coroutines and Networks of Parallel Processes.pdf:pdf},
language = {en},
pages = {20},
title = {{Coroutines and Networks of Parallel Processes}},
url = {https://hal.inria.fr/inria-00306565/},
year = {1976}
}
@article{Hoare1974,
author = {Hoare, C. A. R.},
doi = {10.1145/355620.361161},
file = {:home/etn/Documents/PhD/Biblio/Hoare - 1974 - Monitors an operating system structuring concept.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {monitors,mutual exclusion,operating systems,scheduling,structured multiprogramming,synchronization,system implementation languages},
month = {oct},
number = {10},
pages = {549--557},
publisher = {ACM},
title = {{Monitors: an operating system structuring concept}},
url = {http://dl.acm.org/citation.cfm?id=355620.361161},
volume = {17},
year = {1974}
}
@article{Milner2009,
author = {Milner, Robin},
doi = {10.1017/S0960129500001407},
file = {:home/etn/Documents/PhD/Biblio/Milner - 2009 - Functions as processes.pdf:pdf},
issn = {0960-1295},
journal = {Mathematical Structures in Computer Science},
language = {English},
month = {mar},
number = {02},
pages = {119},
publisher = {Cambridge University Press},
title = {{Functions as processes}},
url = {http://journals.cambridge.org/abstract{\_}S0960129500001407},
volume = {2},
year = {2009}
}
@incollection{Hoare2002,
abstract = {The objectives in the construction of a theory of parallel programming as a basis for a high-level programming language feature are: Security from error. In many of the applications of parallel programming the cost of programming error is very high, often inhibiting the use of computers in environments for which they would otherwise be highly suitable. Parallel programs are particularly prone to time-dependent errors, which either cannot be detected by program testing nor by run-time checks. It is therefore very important that a high-level language designed for this purpose should provide complete security against time-dependent errors by means of a compile-time check. Efficiency. The spread of real-time computer applications is severely limited by computing costs; and in particular by the cost of main store. If a feature to assist in parallel programming is to be added to a language used for this purpose, it must not entail any noticeable extra run-time overhead in space or speed, neither on programs which use the feature heavily, nor on programs which do not; efficient implementation should be possible on a variety of hardware designs, both simple and complex; and there should be no need for bulky or slow compilers. Conceptual simplicity.},
author = {Hoare, C. a. R.},
booktitle = {The origin of concurrent programming},
isbn = {978-1-4419-2986-0, 978-1-4757-3472-0},
issn = {<null>},
pages = {231--244},
title = {{Towards a Theory of Parallel Programming}},
url = {http://link.springer.com/chapter/10.1007/978-1-4757-3472-0{\_}6 http://link.springer.com/chapter/10.1007/978-1-4757-3472-0{\_}6$\backslash$nhttp://link.springer.com/chapter/10.1007/978-1-4757-3472-0{\_}6{\#}page-1},
year = {2002}
}
@article{Dijkstra1975,
author = {Dijkstra, Edsger W.},
doi = {10.1145/360933.360975},
file = {:home/etn/Documents/PhD/Biblio/Dijkstra - 1975 - Guarded commands, nondeterminacy and formal derivation of programs.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {case-construction,correctness proof,derivation of programs,nondeterminancy,program semantics,programming language semantics,programming languages,programming methodology,repetition,sequencing primitives,termination},
month = {aug},
number = {8},
pages = {453--457},
publisher = {ACM},
title = {{Guarded commands, nondeterminacy and formal derivation of programs}},
url = {http://dl.acm.org/citation.cfm?id=360933.360975},
volume = {18},
year = {1975}
}
@article{Clinger1981,
author = {Clinger, William Douglas},
file = {:home/etn/Documents/PhD/Biblio/Clinger - 1981 - Foundations of Actor Semantics.pdf:pdf},
language = {eng},
month = {may},
title = {{Foundations of Actor Semantics}},
url = {http://dspace.mit.edu/handle/1721.1/6935},
year = {1981}
}
@inproceedings{Hewitt2007a,
author = {Hewitt, Carl},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-74459-7{\_}19},
file = {:home/etn/Documents/PhD/Biblio/Hewitt - 2007 - What is commitment Physical, organizational, and social (revised).pdf:pdf},
isbn = {3540744576},
issn = {03029743},
pages = {293--307},
title = {{What is commitment? Physical, organizational, and social (revised)}},
url = {http://www.pcs.usp.br/{~}coin-aamas06/10{\_}commitment-43{\_}16pages.pdf},
volume = {4386 LNAI},
year = {2007}
}
@article{Hewitt2007,
author = {Hewitt, Carl},
file = {:home/etn/Documents/PhD/Biblio/Hewitt - 2007 - Large-scale Organizational Computing requires Unstratified Paraconsistency and Reflection(2).pdf:pdf;:home/etn/Documents/PhD/Biblio/Hewitt - 2007 - Large-scale Organizational Computing requires Unstratified Paraconsistency and Reflection.pdf:pdf},
journal = {COIN},
title = {{Large-scale Organizational Computing requires Unstratified Paraconsistency and Reflection}},
url = {http://www.ia.urjc.es/COIN2007/COIN2007.pdf{\#}page=105},
year = {2007}
}
@article{Brookes1984,
author = {Brookes, S. D. and Hoare, C. A. R. and Roscoe, A. W.},
doi = {10.1145/828.833},
file = {:home/etn/Documents/PhD/Biblio/Brookes, Hoare, Roscoe - 1984 - A Theory of Communicating Sequential Processes.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = {jun},
number = {3},
pages = {560--599},
publisher = {ACM},
title = {{A Theory of Communicating Sequential Processes}},
url = {http://dl.acm.org/citation.cfm?id=828.833},
volume = {31},
year = {1984}
}
@article{Hall2002,
author = {Hall, A. and Chapman, R.},
doi = {10.1109/52.976937},
file = {:home/etn/Documents/PhD/Biblio/Hall, Chapman - 2002 - Correctness by construction developing a commercial secure system.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {Certification,Data security,Information security,Information technology,Packaging,Praxis Critical Systems,Smart cards,Software packages,Throughput,Usability,User interfaces,commercial productivity,formal specification,performance,requirements elicitation,secure certification,security constraints,security of data,smart cards,software engineering,usability,user interface prototyping},
language = {English},
number = {1},
pages = {18--25},
publisher = {IEEE},
title = {{Correctness by construction: developing a commercial secure system}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=976937},
volume = {19},
year = {2002}
}
@misc{PALAMIDESSI2003,
author = {PALAMIDESSI, CATUSCIA},
booktitle = {Mathematical Structures in Computer Science},
doi = {10.1017/S0960129503004043},
file = {:home/etn/Documents/PhD/Biblio/PALAMIDESSI - 2003 - Comparing the expressive power of the synchronous and asynchronous pi-calculi.pdf:pdf},
issn = {09601295},
language = {English},
month = {oct},
number = {5},
pages = {685--719},
publisher = {Cambridge University Press},
title = {{Comparing the expressive power of the synchronous and asynchronous pi-calculi}},
url = {http://journals.cambridge.org/abstract{\_}S0960129503004043},
volume = {13},
year = {2003}
}
@article{Hansen1978a,
author = {Hansen, P.B. and Staunstrup, J.},
doi = {10.1109/TSE.1978.233856},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Computer science,Concurrent programs,Delay,Programming profession,Specification languages,Sufficient conditions,guarded regions,mutual exclusion,program implementation,program specification,program verification,transition commands},
language = {English},
month = {sep},
number = {5},
pages = {365--370},
publisher = {IEEE},
title = {{Specification and Implementation of Mutual Exclusion}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1702551},
volume = {SE-4},
year = {1978}
}
@article{Lamport1982,
author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
doi = {10.1145/357172.357176},
file = {:home/etn/Documents/PhD/Biblio/Lamport, Shostak, Pease - 1982 - The Byzantine Generals Problem.pdf:pdf},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
month = {jul},
number = {3},
pages = {382--401},
publisher = {ACM},
title = {{The Byzantine Generals Problem}},
url = {http://dl.acm.org/citation.cfm?id=357172.357176},
volume = {4},
year = {1982}
}
@article{Hewitt1977a,
author = {Hewitt, Carl and Baker, Henry, Jr},
file = {:home/etn/Documents/PhD/Biblio/Hewitt, Baker - 1977 - Actors and Continuous Functionals,(2).pdf:pdf},
keywords = {*COMPUTER COMMUNICATIONS,*PARALLEL PROCESSING,*SEQUENCES,GRAPHS,INTEGRATED CIRCUITS,LOGIC CIRCUITS,PROGRAMMING LANGUAGES},
month = {dec},
title = {{Actors and Continuous Functionals,}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADA052266},
year = {1977}
}
@incollection{He1990,
author = {He, Jifeng and Josephs, Mark B and Hoare, C A R},
booktitle = {Programming Concepts and Methods},
file = {:home/etn/Documents/PhD/Biblio/He, Josephs, Hoare - 1990 - A Theory of Synchrony and Asynchrony.pdf:pdf},
pages = {459--478},
title = {{A Theory of Synchrony and Asynchrony}},
url = {http://www.cs.ox.ac.uk/files/6120/He 90 - Theory.pdf},
year = {1990}
}
@article{Dijkstra,
annote = {This is the reference paper for Semaphores.},
author = {Dijkstra, Edsger},
file = {:home/etn/Documents/PhD/Biblio/Dijkstra - Unknown - Over de sequentialiteit van procesbeschrijvingen.PDF:PDF},
title = {{Over de sequentialiteit van procesbeschrijvingen}}
}
@article{Hansen1975,
author = {Hansen, Per Brinch},
doi = {10.1109/TSE.1975.6312840},
file = {:home/etn/Documents/PhD/Biblio/Hansen - 1975 - The programming language Concurrent Pascal.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Abstract data types,Computer languages,Data structures,Monitoring,Operating systems,Permission,Program processors,Programming,access rights,classes,computer operating systems,concurrent pascal,concurrent processes,concurrent programming languages,concurrent programming tools,hierarchical operating systems,monitors,operating systems (computers),procedure oriented languages,programming,programming language,scheduling,shared data structures,structured multiprogramming,structured programming},
language = {English},
month = {jun},
number = {2},
pages = {199--207},
publisher = {IEEE},
title = {{The programming language Concurrent Pascal}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6312840},
volume = {SE-1},
year = {1975}
}
@book{Armstrong1993,
author = {Armstrong, Joe and Virding, Robert and Wikstrom, Claes and Williams, Mike},
file = {:home/etn/Documents/PhD/Biblio/Armstrong et al. - 1993 - Concurrent Programming in ERLANG.pdf:pdf},
title = {{Concurrent Programming in ERLANG}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.6333},
year = {1993}
}
@article{Kahn1974,
abstract = {In this paper, we describe a simple language for parallel programming. Its semantics is studied thoroughly. The desirable properties of this language and its deficiencies are exhibited by this theoretical study. Basic results on parallel program schemata are given. We hope in this way to make a case for a more formal (i.e. mathematical) approach to the design of languages for systems programming and the design of operating systems.},
author = {Kahn, Gilles},
file = {:home/etn/Documents/PhD/Biblio/Kahn - 1974 - The semantics of a simple language for parallel programming.pdf:pdf},
journal = {In Information Processing’74: Proceedings of the IFIP Congress},
keywords = {dataflow,important,lu},
pages = {471--475},
publisher = {North-Holland},
title = {{The semantics of a simple language for parallel programming}},
url = {http://www.tik.ee.ethz.ch/tik/education/lectures/hswcd/papers/2{\_}KahnProcessNetworks.pdf http://www.citeulike.org/group/872/article/349829},
volume = {74},
year = {1974}
}
@inproceedings{Charles2005,
address = {New York, New York, USA},
author = {Charles, Philippe and Grothoff, Christian and Saraswat, Vijay and Donawa, Christopher and Kielstra, Allan and Ebcioglu, Kemal and von Praun, Christoph and Sarkar, Vivek},
booktitle = {Proceedings of the 20th annual ACM SIGPLAN conference on Object oriented programming systems languages and applications - OOPSLA '05},
doi = {10.1145/1094811.1094852},
file = {:home/etn/Documents/PhD/Biblio/Charles et al. - 2005 - X10.pdf:pdf},
isbn = {1595930310},
issn = {0362-1340},
keywords = {Java,X10,atomic blocks,clocks,data distribution,multithreading,non-uniform cluster computing (NUCC),partitioned global address space (PGAS),places,productivity,scalability},
month = {oct},
number = {10},
pages = {519},
publisher = {ACM Press},
title = {{X10}},
url = {http://dl.acm.org/citation.cfm?id=1094811.1094852},
volume = {40},
year = {2005}
}
@article{Chamberlain2007,
abstract = {In this paper we consider productivity challenges for parallel programmers and explore ways that parallel language design might help improve end-user productivity. We offer a candidate list of desirable qualities for a parallel programming language, and describe how these qualities are addressed in the design of the Chapel language. In doing so, we provide an overview of Chapel's features and how they help address parallel productivity. We also survey current techniques for parallel programming and describe ways in which we consider them to fall short of our idealized productive programming model.},
author = {Chamberlain, B.L. and Callahan, D. and Zima, H.P.},
doi = {10.1177/1094342007078442},
file = {:home/etn/Documents/PhD/Biblio/Chamberlain, Callahan, Zima - 2007 - Parallel Programmability and the Chapel Language.pdf:pdf},
issn = {1094-3420},
journal = {International Journal of High Performance Computing Applications},
month = {aug},
number = {3},
pages = {291--312},
title = {{Parallel Programmability and the Chapel Language}},
url = {http://hpc.sagepub.com/content/21/3/291.short},
volume = {21},
year = {2007}
}
@inproceedings{Chapman2010,
address = {New York, New York, USA},
author = {Chapman, Barbara and Curtis, Tony and Pophale, Swaroop and Poole, Stephen and Kuehn, Jeff and Koelbel, Chuck and Smith, Lauren},
booktitle = {Proceedings of the Fourth Conference on Partitioned Global Address Space Programming Model - PGAS '10},
doi = {10.1145/2020373.2020375},
file = {:home/etn/Documents/PhD/Biblio/Chapman et al. - 2010 - Introducing OpenSHMEM.pdf:pdf},
isbn = {9781450304610},
keywords = {ACM proceedings,OpenSHMEM,PGAS,SHMEM},
month = {oct},
pages = {1--3},
publisher = {ACM Press},
title = {{Introducing OpenSHMEM}},
url = {http://dl.acm.org/citation.cfm?id=2020373.2020375},
year = {2010}
}
@inproceedings{El-Ghazawi2006,
address = {New York, New York, USA},
author = {El-Ghazawi, Tarek and Smith, Lauren},
booktitle = {Proceedings of the 2006 ACM/IEEE conference on Supercomputing - SC '06},
doi = {10.1145/1188455.1188483},
isbn = {0769527000},
month = {nov},
pages = {27},
publisher = {ACM Press},
title = {{UPC: unified parallel C}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=1188483{\&}type=html},
year = {2006}
}
@article{Numrich1998,
author = {Numrich, Robert W. and Reid, John},
doi = {10.1145/289918.289920},
file = {:home/etn/Documents/PhD/Biblio/Numrich, Reid - 1998 - Co-array Fortran for parallel programming.pdf:pdf},
issn = {10617264},
journal = {ACM SIGPLAN Fortran Forum},
month = {aug},
number = {2},
pages = {1--31},
publisher = {ACM},
title = {{Co-array Fortran for parallel programming}},
url = {http://dl.acm.org/citation.cfm?id=289918.289920},
volume = {17},
year = {1998}
}
@article{Arasu2005,
author = {Arasu, Arvind and Babu, Shivnath and Widom, Jennifer},
doi = {10.1007/s00778-004-0147-z},
file = {:home/etn/Documents/PhD/Biblio/Arasu, Babu, Widom - 2005 - The CQL continuous query language semantic foundations and query execution.pdf:pdf},
issn = {1066-8888},
journal = {The VLDB Journal},
keywords = {Continuous queries,Data streams,Query language,Query processing},
month = {jul},
number = {2},
pages = {121--142},
publisher = {Springer-Verlag New York, Inc.},
title = {{The CQL continuous query language: semantic foundations and query execution}},
url = {http://dl.acm.org/citation.cfm?id=1146461.1146463},
volume = {15},
year = {2005}
}
@article{Arasu2003,
abstract = {The STREAM project at Stanford is developing a general-purpose system for processing continuous queries over multiple continuous data streams and stored relations. It is designed to handle high-volume and bursty data streams with large numbers of complex continuous queries. We describe the status of the system as of early 2003 and outline our ongoing research directions.},
author = {Arasu, Arvind and Babcock, Brian and Babu, Shivnath and Datar, Mayur and Ito, Keith and Motwani, Rajeev and Nishizawa, Itaru and Srivastava, Utkarsh and Thomas, Dilys and Varma, Rohit and Widom, Jennifer},
doi = {10.1145/872757.872854},
file = {:home/etn/Documents/PhD/Biblio/Arasu et al. - 2003 - STREAM The Stanford Stream Data Manager.pdf:pdf},
isbn = {158113634X},
issn = {07308078},
journal = {IEEE Data Engineering Bulletin},
number = {March 2003},
pages = {19--26},
pmid = {622423},
title = {{STREAM: The Stanford Stream Data Manager}},
url = {http://ilpubs.stanford.edu:8090/583/1/2003-21.pdf},
volume = {26},
year = {2003}
}
@inproceedings{Lerner2003,
abstract = {An order-dependent query is one whose result (interpreted as a multiset) changes if the order of the input records is changed. In a stock-quotes database, for instance, retriev- ing all quotes concerning a given stock for a given day does not depend on order, because the collection of quotes does not depend on order. By contrast, finding a stock’s five- price moving-average in a trades table gives a result that depends on the order of the table. Query languages based on the relational data model can handle order-dependent queries only through add-ons. SQL:1999, for instance, has a new “window” mechanism which can sort data in limited parts of a query. Add-ons make order-dependent queries difficult to write and to optimize. In this paper we show that order can be a natural property of the underlying data model and algebra. We introduce a new query language and algebra, called AQuery, that supports order from-the- ground-up. New order-related query transformations arise in this setting. We show by experiment that this framework – language plus optimization techniques – brings orders-of- magnitude improvement over SQL:1999 systems on many natural order-dependent queries.},
author = {Lerner, Alberto and Shasha, Dennis},
booktitle = {Proceedings of the 29th international conference on Very large data bases-Volume 29},
file = {:home/etn/Documents/PhD/Biblio/Lerner, Shasha - 2003 - Aquery Query language for ordered data, optimization techniques, and experiments.pdf:pdf},
isbn = {0127224424},
month = {sep},
pages = {345--356},
publisher = {VLDB Endowment},
title = {{Aquery: Query language for ordered data, optimization techniques, and experiments}},
url = {http://dl.acm.org/citation.cfm?id=1315451.1315482 http://portal.acm.org/citation.cfm?id=1315482{\&}amp;dl=GUIDE,},
year = {2003}
}
@article{Zaharia2010,
author = {Zaharia, M and Chowdhury, M},
file = {:home/etn/Documents/PhD/Biblio/Zaharia, Chowdhury - 2010 - Spark cluster computing with working sets.pdf:pdf},
journal = {HotCloud'10 Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
title = {{Spark: cluster computing with working sets}},
url = {http://static.usenix.org/legacy/events/hotcloud10/tech/full{\_}papers/Zaharia.pdf http://www.usenix.org/event/hotcloud10/tech/full{\_}papers/Zaharia.pdf},
year = {2010}
}
@article{Gunda2010,
abstract = {Managing data and computation is at the heart of datacenter computing. Manual management of data can lead to data loss, wasteful consumption of storage, and laborious bookkeeping. Lack of proper management of computation can result in lost opportunities to share common computations across multiple jobs or to compute results incrementally. Nectar is a system designed to address the aforementioned problems. It automates and unifies the management of data and computation within a datacenter. In Nectar, data and computation are treated interchangeably by associating data with its computation. Derived datasets, which are the results of computations, are uniquely identified by the programs that produce them, and together with their programs, are automatically managed by a datacenter wide caching service. Any derived dataset can be transparently regenerated by re-executing its program, and any computation can be transparently avoided by using previously cached results. This enables us to greatly improve datacenter management and resource utilization: obsolete or infrequently used derived datasets are automatically garbage collected, and shared common computations are computed only once and reused by others. This paper describes the design and implementation of Nectar, and reports on our evaluation of the system using analytic studies of logs from several production clusters and an actual deployment on a 240-node cluster.},
author = {Gunda, Pradeep Kumar and Ravindranath, Lenin and Thekkath, Chandramohan a and Yu, Yuan and Zhuang, Li},
file = {:home/etn/Documents/PhD/Biblio/Gunda et al. - 2010 - Nectar Automatic Management of Data and Computation in Datacenters.pdf:pdf},
isbn = {978-1-931971-79-9},
journal = {Technology},
pages = {1--8},
title = {{Nectar : Automatic Management of Data and Computation in Datacenters}},
url = {https://www.usenix.org/legacy/event/osdi10/tech/full{\_}papers/Gunda.pdf http://www.usenix.org/event/osdi10/tech/full{\_}papers/Gunda.pdf},
year = {2010}
}
@article{Murray2013,
abstract = {Naiad is a distributed system for executing data parallel, cyclic dataflow programs. It offers the high throughput of batch processors, the low latency of stream processors, and the ability to perform iterative and incremental computations. Although existing systems offer some of these features, applications that require all three have relied on multiple platforms, at the expense of efficiency, maintainability, and simplicity. Naiad resolves the complexities of combining these features in one framework. A new computational model, timely dataflow, underlies Naiad and captures opportunities for parallelism across a wide class of algorithms. This model enriches dataflow computation with timestamps that represent logical points in the computation and provide the basis for an efficient, lightweight coordination mechanism. We show that many powerful high-level programming models can be built on Naiad's low-level primitives, enabling such diverse tasks as streaming data analysis, iterative machine learning, and interactive graph mining. Naiad outperforms specialized systems in their target application domains, and its unique features enable the development of new high-performance applications.},
address = {New York, New York, USA},
author = {Murray, Derek G. and McSherry, Frank and Isaacs, Rebecca and Isard, Michael and Barham, Paul and Abadi, Martin},
doi = {10.1145/2517349.2522738},
file = {:home/etn/Documents/PhD/Biblio/Murray et al. - 2013 - Naiad.pdf:pdf},
isbn = {9781450323888},
journal = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles - SOSP '13},
month = {nov},
pages = {439--455},
publisher = {ACM Press},
title = {{Naiad}},
url = {http://dl.acm.org/citation.cfm?id=2517349.2522738 http://dl.acm.org/citation.cfm?id=2522738$\backslash$nhttp://dl.acm.org/citation.cfm?doid=2517349.2522738},
year = {2013}
}
@inproceedings{Xin2013,
address = {New York, New York, USA},
author = {Xin, Reynold S. and Rosen, Josh and Zaharia, Matei and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
booktitle = {Proceedings of the 2013 international conference on Management of data - SIGMOD '13},
doi = {10.1145/2463676.2465288},
file = {:home/etn/Documents/PhD/Biblio/Xin et al. - 2013 - Shark.pdf:pdf},
isbn = {9781450320375},
keywords = {data warehouse,databases,hadoop,machine learning,shark,spark},
month = {jun},
pages = {13},
publisher = {ACM Press},
title = {{Shark}},
url = {http://dl.acm.org/citation.cfm?id=2463676.2465288},
year = {2013}
}
@article{Tolooee2015,
author = {Tolooee, Cameron and Malensek, Matthew and Pallickara, Sangmi Lee},
doi = {10.1002/cpe.3651},
file = {:home/etn/Documents/PhD/Biblio/Tolooee, Malensek, Pallickara - 2015 - A scalable framework for continuous query evaluations over multidimensional, scientific datasets.pdf:pdf},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
month = {sep},
pages = {n/a--n/a},
title = {{A scalable framework for continuous query evaluations over multidimensional, scientific datasets}},
url = {http://doi.wiley.com/10.1002/cpe.3651},
year = {2015}
}
@article{Ashcroft1977,
author = {Ashcroft, Edward A. and Wadge, William W.},
doi = {10.1145/359636.359715},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {alire,dataflow,formal systems,iteration,program proving,semantics,structured programming},
month = {jul},
number = {7},
pages = {519--526},
publisher = {ACM},
title = {{Lucid, a nonprocedural language with iteration}},
url = {http://dl.acm.org/citation.cfm?id=359636.359715 http://dl.acm.org/citation.cfm?id=359715},
volume = {20},
year = {1977}
}
@article{Hirzel2011,
abstract = {Various research communities have independently arrived at stream processing as a programming model for efficient and parallel computing. These communities include digital signal processing, databases, operating systems, and complex event processing. Since each community faces applications with challenging performance requirements, each of them has developed some of the same optimizations, but often with conflicting terminology and unstated assumptions. This article presents a survey of optimizations for stream processing. It is aimed both at users who need to understand and guide the system’s optimizer and at implementers who need to make engineering tradeoffs. To consolidate terminology, this article is organized as a catalog, in a style similar to catalogs of design patterns or refactorings. To make assumptions explicit and help understand tradeoffs, each optimization is presented with its safety constraints (when does it preserve correctness?) and a profitability experiment (when does it improve performance?). We hope that this survey will help future streaming system builders to stand on the shoulders of giants from not just their own community.},
author = {Hirzel, Martin and Soul{\'{e}}, Robert and Schneider, Scott and Gedik, Buğra and Grimm, Robert},
doi = {10.1145/2528412},
file = {:home/etn/Documents/PhD/Biblio//Hirzel et al. - 2014 - A catalog of stream processing optimizations.pdf:pdf},
institution = {Research Report RC25215, IBM},
issn = {03600300},
journal = {ACM Computing Surveys (CSUR)},
keywords = {alire,dataflow},
number = {4},
pages = {1--34},
title = {{A catalog of stream processing optimizations}},
url = {http://dl.acm.org/citation.cfm?id=2528412 http://dl.acm.org/citation.cfm?doid=2597757.2528412},
volume = {46},
year = {2014}
}
@inproceedings{Toshniwal2014,
address = {New York, New York, USA},
author = {Toshniwal, Ankit and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong},
booktitle = {Proceedings of the 2014 ACM SIGMOD international conference on Management of data - SIGMOD '14},
doi = {10.1145/2588555.2595641},
file = {:home/etn/Documents/PhD/Biblio/Toshniwal et al. - 2014 - Storm@ twitter.pdf:pdf},
isbn = {9781450323765},
keywords = {real-time query processing,stream data management},
month = {jun},
pages = {147--156},
publisher = {ACM Press},
title = {{Storm@ twitter}},
url = {http://dl.acm.org/citation.cfm?id=2595641 http://dl.acm.org/citation.cfm?id=2588555.2595641},
year = {2014}
}
@article{Jones2011,
author = {Hoffman, Karl Heinz and Meyer, Arnd},
title = {{Parallel Algorithms and Cluster Computing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.2094{\&}rep=rep1{\&}type=pdf{\#}page=9},
year = {2006}
}
@inproceedings{Kamruzzaman2013,
address = {New York, New York, USA},
author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on - SC '13},
doi = {10.1145/2503210.2503295},
file = {:home/etn/Documents/PhD/Biblio/Kamruzzaman, Swanson, Tullsen - 2013 - Load-balanced pipeline parallelism.pdf:pdf},
isbn = {9781450323789},
keywords = {Instruction sets,Load management,Pipeline processing,Pipelines,Synchronization,chip multiprocessors,compiler-based technique,compilers,data parallel fashion,inter-thread communication,load-balanced pipeline parallelism,load-balancing,locality,multiprocessing systems,parallel processing,parallel systems,pipeline parallelism,pipeline processing,pipeline stage automatic extraction,program compilers,resource allocation,sequential stages,sequential threads,single thread acceleration,synchronisation,token-based chunked synchronization},
language = {English},
pages = {1--12},
publisher = {ACM Press},
title = {{Load-balanced pipeline parallelism}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6877447},
year = {2013}
}
@article{Flynn1972,
author = {Flynn, Michael J.},
doi = {10.1109/TC.1972.5009071},
file = {:home/etn/Documents/PhD/Biblio/Flynn - 1972 - Some Computer Organizations and Their Effectiveness.pdf:pdf},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Automata,Computer aided instruction,Computer organization,Concurrent computing,Parallel processing,Performance evaluation,Time sharing computer systems,Transmission electron microscopy,instruction stream,overlapped,parallel processors,resource hierarchy},
language = {English},
month = {sep},
number = {9},
pages = {948--960},
publisher = {IEEE},
title = {{Some Computer Organizations and Their Effectiveness}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5009071},
volume = {C-21},
year = {1972}
}
@article{Stonebraker1986,
abstract = {There are three dominent themes in building high transaction rate multiprocessor systems, namely shared memory (e.g. Synapse, IBM/AP configurations), shared disk (e.g. VAX/cluster, any multi-ported disk system), and shared nothing (e.g. Tandem, Tolerant). This paper argues that shared nothing is the pre- ferred approach.},
author = {Stonebraker, Michael},
file = {:home/etn/Documents/PhD/Biblio/Stonebraker - 1986 - The Case for Shared Nothing.pdf:pdf},
issn = {09547762},
journal = {Contract},
number = {1},
pages = {1--5},
pmid = {8717696},
title = {{The Case for Shared Nothing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.5370{\&}rep=rep1{\&}type=pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.5370{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {9},
year = {1986}
}
@misc{Cole1988,
author = {Cole, M. I.},
keywords = {Programming parellel computers Computer software A},
language = {eng},
publisher = {University of Edinburgh},
title = {{Algorithmic skeletons : A structured approach to the management of parallel computation}},
url = {http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.384168},
year = {1988}
}
@article{Gonzalez-Velez2010,
author = {Gonz{\'{a}}lez-V{\'{e}}lez, Horacio and Leyton, Mario},
doi = {10.1002/spe.1026},
file = {:home/etn/Documents/PhD/Biblio/Gonz{\'{a}}lez-V{\'{e}}lez, Leyton - 2010 - A survey of algorithmic skeleton frameworks high-level structured parallel programming enablers.pdf:pdf},
issn = {00380644},
journal = {Software: Practice and Experience},
month = {nov},
number = {12},
pages = {1135--1160},
title = {{A survey of algorithmic skeleton frameworks: high-level structured parallel programming enablers}},
url = {http://doi.wiley.com/10.1002/spe.1026},
volume = {40},
year = {2010}
}
@incollection{Cong2005,
abstract = {Lock-free shared data structures in the setting of distributed computing have received a fair amount of attention. Major motivations of lock-free data structures include increasing fault tolerance of a (possibly heterogeneous) system and alleviating the problems associated with critical sections such as priority inversion and deadlock. For parallel computers with tightly-coupled processors and shared memory, these issues are no longer major concerns. While many of the results are applicable especially when the model used is shared memory multiprocessors, no prior studies have considered improving the performance of a parallel implementation by way of lock-free programming. As a matter of fact, often times in practice lock-free data structures in a distributed setting do not perform as well as those that use locks. As the data structures and algorithms for parallel computing are often drastically different from those in distributed computing, it is possible that lock-free programs perform better. In this paper we compare the similarity and difference of lock-free programming in both distributed and parallel computing environments and explore the possibility of adapting lock-free programming to parallel computing to improve performance. Lock-free programming also provides a new way of simulating PRAM and asynchronous PRAM algorithms on current parallel machines.},
author = {Cong, Guojing and Bader, David},
booktitle = {Lecture Notes in Computer Science, Volume 3296/2005},
doi = {10.1007/978-3-540-30474-6{\_}54},
file = {:home/etn/Documents/PhD/Biblio/Cong, Bader - 2005 - Lock-Free Parallel Algorithms An Experimental Study.pdf:pdf},
pages = {1873--1875},
title = {{Lock-Free Parallel Algorithms: An Experimental Study}},
url = {http://www.cc.gatech.edu/{~}bader/papers/lockfree-HiPC2004.pdf http://www.springerlink.com/content/xw2v4a7h74j8ykhp/},
volume = {3296},
year = {2005}
}
@article{Sundell2003,
abstract = {We present an efficient and practical lock-free implementation of a concurrent priority queue that is suitable for both fully concurrent (large multi-processor) systems as well as pre-emptive (multi-process) systems. Many algorithms for concurrent priority queues are based on mutual exclusion. However, mutual exclusion causes blocking which has several drawbacks and degrades the system's overall performance. Non-blocking algorithms avoid blocking, and are either lock-free or wait-free. Previously known non-blocking algorithms of priority queues did not perform well in practice because of their complexity, and they are often based on non-available atomic synchronization primitives. Our algorithm is based on the randomized sequential list structure called Skiplist, and a real-time extension of our algorithm is also described. In our performance evaluation we compare our algorithm with some of the most efficient implementations of priority queues known. The experimental results clearly show that our lock-free implementation outperforms the other lock-based implementations in all cases for 3 threads and more, both on fully concurrent as well as on pre-emptive systems.},
author = {Sundell, H. and Tsigas, P.},
doi = {10.1109/IPDPS.2003.1213189},
file = {:home/etn/Documents/PhD/Biblio/Sundell, Tsigas - 2003 - Fast and lock-free concurrent priority queues for multi-thread systems.pdf:pdf},
isbn = {0-7695-1926-1},
issn = {07437315},
journal = {Proceedings International Parallel and Distributed Processing Symposium},
number = {C},
pages = {11},
title = {{Fast and lock-free concurrent priority queues for multi-thread systems}},
url = {http://www.non-blocking.com/download/SunT03{\_}PQueue{\_}TR.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1213189},
volume = {00},
year = {2003}
}
@article{Niu2011,
abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1106.5730},
author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
eprint = {1106.5730},
file = {:home/etn/Documents/PhD/Biblio/Niu et al. - 2011 - HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {21},
title = {{HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}},
url = {http://i.stanford.edu/hazy/papers/hogwild-nips.pdf http://arxiv.org/abs/1106.5730},
year = {2011}
}
@misc{Anderson1990,
author = {Anderson, James H. and Gouda, Mohamed G.},
file = {:home/etn/Documents/PhD/Biblio/Anderson, Gouda - 1990 - The virtue of Patience Concurrent Programming With And Without Waiting.pdf:pdf},
title = {{The virtue of Patience: Concurrent Programming With And Without Waiting}},
url = {http://www.cs.utexas.edu/ftp/techreports/tr90-23.pdf},
year = {1990}
}
@article{Herlihy1990,
author = {Herlihy, M.},
doi = {10.1145/99164.99185},
file = {:home/etn/Documents/PhD/Biblio/Herlihy - 1990 - A methodology for implementing highly concurrent data structures.pdf:pdf},
isbn = {0-89791-350-7},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
month = {mar},
number = {3},
pages = {197--206},
publisher = {ACM},
title = {{A methodology for implementing highly concurrent data structures}},
url = {http://dl.acm.org/citation.cfm?id=99164.99185},
volume = {25},
year = {1990}
}
@article{Lamport1977,
author = {Lamport, Leslie},
doi = {10.1145/359863.359878},
file = {:home/etn/Documents/PhD/Biblio/Lamport - 1977 - Concurrent reading and writing.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {asynchronous multiprocessing,multiprocess synchronization,readers/writers problem,shared data},
month = {nov},
number = {11},
pages = {806--811},
publisher = {ACM},
title = {{Concurrent reading and writing}},
url = {http://dl.acm.org/citation.cfm?id=359863.359878},
volume = {20},
year = {1977}
}
@inproceedings{Herlihy1988,
address = {New York, New York, USA},
author = {Herlihy, Maurice P.},
booktitle = {Proceedings of the seventh annual ACM Symposium on Principles of distributed computing - PODC '88},
doi = {10.1145/62546.62593},
file = {:home/etn/Documents/PhD/Biblio/Herlihy - 1988 - Impossibility and universality results for wait-free synchronization.pdf:pdf},
isbn = {0897912772},
month = {jan},
pages = {276--290},
publisher = {ACM Press},
title = {{Impossibility and universality results for wait-free synchronization}},
url = {http://dl.acm.org/citation.cfm?id=62546.62593},
year = {1988}
}
@inproceedings{Nelson2004,
address = {New York, New York, USA},
author = {Nelson, Jay},
booktitle = {Proceedings of the 2004 ACM SIGPLAN workshop on Erlang - ERLANG '04},
doi = {10.1145/1022471.1022480},
file = {:home/etn/Documents/PhD/Biblio/Nelson - 2004 - Structured programming using processes.pdf:pdf},
isbn = {1581139187},
keywords = {COPL,Erlang,concurrency oriented programming language,erlang,inductive decomposition},
mendeley-tags = {Erlang},
month = {sep},
pages = {54--64},
publisher = {ACM Press},
title = {{Structured programming using processes}},
url = {http://dl.acm.org/citation.cfm?id=1022471.1022480},
year = {2004}
}
@inproceedings{Nugteren2012,
abstract = {Recent advances in multi-core and many-core processors re- quires programmers to exploit an increasing amount of par- allelism from their applications. Data parallel languages such as CUDA and OpenCL make it possible to take ad- vantage of such processors, but still require a large amount of effort from programmers. A number of parallelizing source-to-source compilers have recently been developed to ease programming of multi-core and many-core processors. This work presents and evalu- ates a number of such tools, focused in particular on C-to- CUDA transformations targeting GPUs. We compare these tools both qualitatively and quantitatively to each other and identify their strengths and weaknesses. In this paper, we address the weaknesses by presenting a new classification of algorithms. This classification is used in a new source-to-source compiler, which is based on the algo- rithmic skeletons technique. The compiler generates target code based on skeletons of parallel structures, which can be seen as parameterisable library implementations for a set of algorithm classes. We furthermore demonstrate that the presented compiler requires little modifications to the original sequential source code, generates readable code for further fine-tuning, and delivers superior performance compared to other tools for a set of 8 image processing kernels.},
address = {New York, New York, USA},
author = {Nugteren, Cedric and Corporaal, Henk},
booktitle = {Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units},
doi = {10.1145/2159430.2159431},
file = {:home/etn/Documents/PhD/Biblio/Nugteren, Corporaal - 2012 - Introducing 'Bones' a parallelizing source-to-source compiler based on algorithmic skeletons.pdf:pdf},
isbn = {9781450312332},
keywords = {Graphics Processing Units,Parallel Programming,Source-to-Source Compilation,algorithmic skeletons},
month = {mar},
pages = {1--10},
publisher = {ACM Press},
title = {{Introducing 'Bones': a parallelizing source-to-source compiler based on algorithmic skeletons}},
url = {http://dl.acm.org/citation.cfm?id=2159430.2159431 http://dl.acm.org/citation.cfm?id=2159431},
year = {2012}
}
@article{Stone2010,
abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
author = {Stone, John E. and Gohara, David and Shi, Guochun},
doi = {10.1109/MCSE.2010.69},
file = {:home/etn/Documents/PhD/Biblio/Stone, Gohara, Shi - 2010 - OpenCL A Parallel Programming Standard for Heterogeneous Computing Systems.pdf:pdf},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
month = {may},
number = {3},
pages = {66--73},
publisher = {AIP Publishing},
title = {{OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems}},
url = {http://scitation.aip.org/content/aip/journal/cise/12/3/10.1109/MCSE.2010.69},
volume = {12},
year = {2010}
}
@inproceedings{Hall1995,
address = {New York, New York, USA},
author = {Hall, Mary H. and Amarasinghe, Saman P. and Murphy, Brian R. and Liao, Shih-Wei and Lam, Monica S.},
booktitle = {Proceedings of the 1995 ACM/IEEE conference on Supercomputing (CDROM) - Supercomputing '95},
doi = {10.1145/224170.224337},
file = {:home/etn/Documents/PhD/Biblio/Hall et al. - 1995 - Detecting coarse-grain parallelism using an interprocedural parallelizing compiler.pdf:pdf},
isbn = {0897918169},
keywords = {compiler optimizations,interprocedural data-flow analysis,parallelizing compilers,shared memory multiprocessors},
month = {dec},
pages = {49--es},
publisher = {ACM Press},
title = {{Detecting coarse-grain parallelism using an interprocedural parallelizing compiler}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=224337{\&}type=html},
year = {1995}
}
@article{Beck1991,
author = {Beck, Micah and Johnson, Richard and Pingali, Keshav},
doi = {10.1016/0743-7315(91)90016-3},
file = {:home/etn/Documents/PhD/Biblio/Beck, Johnson, Pingali - 1991 - From control flow to dataflow.ps$\backslash$;jsessionid=D4194E1CBD3276E264A2D1FD1507F780:ps;jsessionid=D4194E1CBD3276E264A2D1FD1507F780},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
number = {2},
pages = {118--129},
title = {{From control flow to dataflow}},
volume = {12},
year = {1991}
}
@article{Johnston2004a,
author = {Johnston, Wesley M. and Hanna, J. R. Paul and Millar, Richard J.},
doi = {10.1145/1013208.1013209},
file = {:home/etn/Documents/PhD/Biblio/Johnston, Hanna, Millar - 2004 - Advances in dataflow programming languages.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Dataflow,co-ordination languages,component software,data flow visual programming,graphical programming,multithreading,software engineering},
month = {mar},
number = {1},
pages = {1--34},
publisher = {ACM},
title = {{Advances in dataflow programming languages}},
url = {http://dl.acm.org/citation.cfm?id=1013208.1013209},
volume = {36},
year = {2004}
}
@article{Li2012,
abstract = {This article presents a general algorithm for transforming sequential imperative programs into parallel data-flow programs. The algorithm operates on a program dependence graph in static-single-assignment form, extracting task, pipeline, and data parallelism from arbitrary control flow, and coarsening its granularity using a generalized form of typed fusion. A prototype based on GNU Compiler Collection (GCC) is applied to the automatic parallelization of recursive C programs.},
author = {Li, Feng and Pop, Antoniu and Cohen, Albert},
doi = {10.1109/MM.2012.49},
file = {:home/etn/Documents/PhD/Biblio/Li, Pop, Cohen - 2012 - Automatic Extraction of Coarse-Grained Data-Flow Threads from Imperative Programs.pdf:pdf},
issn = {0272-1732},
journal = {IEEE Micro},
keywords = {Instruction sets,Parallel processing,Pipeline processing,Radiation detectors,SSA form,Sequential analysis,Synchronization,automatic parallelization,data-flow model,loop fusion,program dependence graph,sequential imperative programs},
language = {English},
month = {jul},
number = {4},
pages = {19--31},
publisher = {IEEE Computer Society},
title = {{Automatic Extraction of Coarse-Grained Data-Flow Threads from Imperative Programs}},
url = {http://www.computer.org/csdl/mags/mi/2012/04/mmi2012040019.html},
volume = {32},
year = {2012}
}
@inproceedings{Vandierendonck2010a,
abstract = {Speeding up sequential programs on multicores is a challenging problem that is in urgent need of a solution. Automatic paral- lelization of irregular pointer-intensive codes, exemplified by the SPECint codes, is a very hard problem. This paper shows that, with a helping hand, such auto-parallelization is possible and fruitful. This paper makes the following contributions: (i) A compiler- framework for extracting pipeline-like parallelism from outer pro- gram loops is presented. (ii) Using a light-weight programming model based on annotations, the programmer helps the compiler to find thread-level parallelism. Each of the annotations specifies only a small piece of semantic information that compiler analy- sis misses, e.g. stating that a variable is dead at a certain program point. The annotations are designed such that correctness is eas- ily verified. Furthermore, we present a tool for suggesting annota- tions to the programmer. (iii) The methodology is applied to auto- parallelize several SPECint benchmarks. For the benchmark with most parallelism (hmmer), we obtain a scalable 7-fold speedup on an AMD quad-core dual processor. The annotations constitute a parallel programming model that relies extensively on a sequential program representation. Hereby, the complexity of debugging is not increased and it does not ob- scure the source code. These properties could prove valuable to increase the efficiency of parallel programming.},
address = {New York, New York, USA},
author = {Vandierendonck, Hans and Rul, Sean and {De Bosschere}, Koen},
booktitle = {Proceedings of the 19th international conference on Parallel architectures and compilation techniques},
doi = {10.1145/1854273.1854322},
file = {:home/etn/Documents/PhD/Biblio/Vandierendonck, Rul, De Bosschere - 2010 - The Paralax infrastructure.pdf:pdf},
isbn = {9781450301787},
keywords = {Semi-automatic parallelization,semantic annotations},
month = {sep},
pages = {389--399},
publisher = {ACM Press},
title = {{The Paralax infrastructure: automatic parallelization with a helping hand}},
url = {http://dl.acm.org/citation.cfm?id=1854273.1854322 http://portal.acm.org/citation.cfm?id=1854322},
year = {2010}
}
@inproceedings{Alvaro2014,
author = {Alvaro, Peter and Conway, Neil and Hellerstein, Joseph M. and Maier, David},
booktitle = {2014 IEEE 30th International Conference on Data Engineering},
doi = {10.1109/ICDE.2014.6816639},
file = {:home/etn/Documents/PhD/Biblio/Alvaro et al. - 2014 - Blazes Coordination analysis for distributed programs.pdf:pdf},
isbn = {978-1-4799-2555-1},
keywords = {BLAZES program,Bloom declarative language,Fault tolerance,Fault tolerant systems,Semantics,Servers,Storms,Topology,Twitter,Twitter Storm system,annotated programs,application-specific coordination code synthesis,coordination protocols,cross-platform program analysis framework,distributed consistency,distributed processing,distributed programs,distributed systems,program diagnostics,program location identification,scalable distributed architectures},
language = {English},
month = {mar},
pages = {52--63},
publisher = {IEEE},
title = {{Blazes: Coordination analysis for distributed programs}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6816639},
year = {2014}
}
@article{Rajopadhye2006,
abstract = {We present optimization techniques for high level equational programs that are generalizations of affine control loops (ACLs). Significant parts of the SpecFP and PerfectClub benchmarks are ACLs. They often contain reductions: associative and commutative operators applied to a collection of values. They also often exhibit reuse: intermediate values computed or used at different index points being identical. We develop various techniques to automatically exploit reuse to simplify the computational complexity of evaluating reductions. Finally, we present an algorithm for the optimal application of such simplifications resulting in an equivalent specification with minimum complexity.},
author = {Gautam and Rajopadhye, S.},
doi = {10.1145/1111320.1111041},
isbn = {1595930272},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {1,equational programming,i,incremen-,k,loop optimization,motivating example and introduction,polyhedral model,program transformation,s,tal computation},
month = {jan},
number = {1},
pages = {30--41},
publisher = {ACM},
title = {{Simplifying reductions}},
url = {http://dl.acm.org/citation.cfm?id=1111320.1111041},
volume = {41},
year = {2006}
}
@article{Dennard2007,
abstract = {This paper considers the design, fabrication, and characterization of very small MOSFET switching devices suitable for digital integrated circuits using dimensions of the order of 1 mu. Scaling relationships are presented which show how a conventional MOSFET can be reduced in size. The performance improvement expected from using these very small devices in highly miniaturized integrated circuits is projected.},
author = {Dennard, Robert H. and Gaensslen, Fritz H. and Yu, Hwa-Nien and Rideovt, V. Leo and Bassous, Ernest and Leblanc, Andre R.},
doi = {10.1109/N-SSC.2007.4785543},
file = {:home/etn/Documents/PhD/Biblio/Dennard et al. - 2007 - Design of Ion-Implanted MOSFET's with Very Small Physical Dimensions.pdf:pdf},
isbn = {1098-4232 VO - 12},
issn = {1098-4232},
journal = {IEEE Solid-State Circuits Newsletter},
keywords = {Dennard scaling},
number = {1},
title = {{Design of Ion-Implanted MOSFET's with Very Small Physical Dimensions}},
url = {http://www.ece.ucsb.edu/courses/ECE225/225{\_}W07Banerjee/reference/Dennard.pdf},
volume = {12},
year = {2007}
}
@phdthesis{Mendis2015,
abstract = {Highly optimized programs are prone to bit rot, where performance quickly becomes sub- optimal in the face of new hardware and compiler techniques. In this paper we show how to automatically lift performance-critical stencil kernels from a stripped x86 binary and generate the corresponding code in the high-level domain-specific language Halide. Using Halide’s state-of-the-art optimizations targeting current hardware, we show that new opti- mized versions of these kernels can replace the originals to rejuvenate the application for newer hardware. The original optimized code for kernels in stripped binaries is nearly impossible to analyze statically. Instead, we rely on dynamic traces to regenerate the kernels. We perform buffer structure reconstruction to identify input, intermediate and output buffer shapes. We ab- stract from a forest of concrete dependency trees which contain absolute memory addresses to symbolic trees suitable for high-level code generation. This is done by canonicalizing trees, clustering them based on structure, inferring higher-dimensional buffer accesses and finally by solving a set of linear equations based on buffer accesses to lift them up to simple, high-level expressions. Helium can handle highly optimized, complex stencil kernels with input-dependent con- ditionals. We lift seven kernels from Adobe Photoshop giving a 75{\%} performance improve- ment, four kernels from IrfanView, leading to 4.97× performance, and one stencil from the miniGMG multigrid benchmark netting a 4.25× improvement in performance. We manually rejuvenated Photoshop by replacing eleven of Photoshop’s filters with our lifted implemen- tations, giving 1.12× speedup without affecting the user experience.},
author = {Mendis, Charith and Bosboom, Jeffrey and Wu, Kevin and Kamil, Shoaib and Ragan-kelley, Jonathan and Paris, Sylvain and Zhao, Qin and Amarasinghe, Saman and Mendis, Thirimadura Charith Yasendra},
file = {:home/etn/Documents/PhD/Biblio/Mendis et al. - 2015 - Helium Lifting High-Performance Stencil Kernels from Stripped x86 Binaries to Halide DSL Code.pdf:pdf},
isbn = {9781450334686},
keywords = {autotuning,binary instrumentation,dynamic analysis,helium,image processing,reverse engineering,stencil com-,x86},
pages = {100},
title = {{Helium: Lifting High-Performance Stencil Kernels from Stripped x86 Binaries to Halide DSL Code}},
url = {https://groups.csail.mit.edu/commit/papers/2015/mendis-pldi15-helium.pdf},
year = {2015}
}
@article{Bohr2007,
author = {Bohr, Mark},
doi = {10.1109/N-SSC.2007.4785534},
file = {:home/etn/Documents/PhD/Biblio/Bohr - 2007 - A 30 Year Retrospective on Dennard's MOSFET Scaling Paper.pdf:pdf},
issn = {1098-4232},
journal = {IEEE Solid-State Circuits Newsletter},
keywords = {Industries,Integrated circuit interconnections,Logic gates,MOSFET circuits,Silicon,Transistors,Voltage control},
language = {English},
month = {jan},
number = {1},
pages = {11--13},
publisher = {IEEE},
title = {{A 30 Year Retrospective on Dennard's MOSFET Scaling Paper}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4785534},
volume = {12},
year = {2007}
}
@article{Yuki2013,
author = {Yuki, Tomofumi and Gupta, Gautam and Kim, Daegon and Pathan, Tanveer and Rajopadhye, Sanjay},
doi = {10.1007/978-3-642-37658-0{\_}2},
file = {:home/etn/Documents/PhD/Biblio/Yuki, Gupta, Kim - 2013 - Alphaz A system for design space exploration in the polyhedral model.pdf:pdf},
isbn = {9783642376573},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {17--31},
title = {{AlphaZ: A system for design space exploration in the polyhedral model}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37658-0{\_}2},
volume = {7760 LNCS},
year = {2013}
}
@phdthesis{Mauras1989,
author = {Mauras, Christophe},
keywords = {en fran{\c{c}}ais},
month = {jan},
publisher = {Rennes 1},
title = {{Alpha : un langage equationnel pour la conception et la programmation d'architectures paralleles synchrones}},
url = {http://www.theses.fr/1989REN10116},
year = {1989}
}
@article{Herrmann2000,
author = {Herrmann, CA},
file = {:home/etn/Documents/PhD/Biblio/Herrmann - 2000 - The skeleton based parallelization of divide and conquer recursions.ps:ps},
title = {{The skeleton based parallelization of divide and conquer recursions}},
url = {https://scholar.google.com/scholar?q=The Skeleton-Based Parallelization of Divide-and-Conquer Recursions{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}0},
year = {2000}
}
@inproceedings{Madsen2015,
address = {New York, New York, USA},
author = {Madsen, Kasper Grud Skat and Zhou, Yongluan},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management - CIKM '15},
doi = {10.1145/2806416.2806449},
file = {:home/etn/Documents/PhD/Biblio/Madsen, Zhou - 2015 - Dynamic Resource Management In a Massively Parallel Stream Processing Engine.pdf:pdf},
isbn = {9781450337946},
keywords = {elasticity,fault-tolerance,resource management},
month = {oct},
pages = {13--22},
publisher = {ACM Press},
title = {{Dynamic Resource Management In a Massively Parallel Stream Processing Engine}},
url = {http://dl.acm.org/citation.cfm?id=2806416.2806449},
year = {2015}
}
@article{Yao2015,
author = {Yao, Hong and Xu, Jinlai and Luo, Zhongwen and Zeng, Deze},
doi = {10.1002/cpe.3702},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
month = {oct},
pages = {n/a--n/a},
title = {{MEMoMR: Accelerate MapReduce via reuse of intermediate results}},
url = {http://doi.wiley.com/10.1002/cpe.3702},
year = {2015}
}
@article{Sun2015,
abstract = {In the big data era, big data stream computing, as a computing paradigm, is gaining traction for real-time and online data computing applications, and is specially designed to solve the dilemma of real-time data stream computing by processing data online within real-time constraints. It is used to compute large amounts of data in the form of continuous data streams. Each computation is represented by a data stream graph, usually a directed graph. In this paper, the computing paradigm of big data stream computation in data stream graph is given, and some application scenarios and big data stream characteristics are presented. A series of challenges in designing a scalable big data stream computing system in big data environments are summarised by referring to some research results of big data stream computing system, which include stateless system architecture, elastically adaptive scheduling strategy and fine-grained fault tolerance strategy. All these challenges will greatly help us to understand big dat...},
author = {Sun, Dawei and Liu, Chunxiao and Ren, Dongfeng},
journal = {International Journal of Wireless and Mobile Computing},
keywords = {adaptive scheduling,big data stream computing,fine-grained fault tolerance,stateless system architecture,system design,task scheduling},
language = {en},
month = {oct},
publisher = {Inderscience Publishers (IEL)},
title = {{Prospects, challenges and latest developments in designing a scalable big data stream computing system}},
url = {http://www.inderscienceonline.com/doi/abs/10.1504/IJWMC.2015.072567},
year = {2015}
}
@inproceedings{Nystrom2003,
address = {New York, New York, USA},
author = {Nystr{\"{o}}m, J. H. and Trinder, P. W. and King, D. J.},
booktitle = {Proceedings of the 2003 ACM SIGPLAN workshop on Erlang - ERLANG '03},
doi = {10.1145/940880.940881},
isbn = {1581137729},
month = {aug},
pages = {1--7},
publisher = {ACM Press},
title = {{Evaluating distributed functional languages for telecommunications software}},
url = {http://dl.acm.org/citation.cfm?id=940880.940881},
year = {2003}
}
@article{Reese2008,
author = {Reese, Will},
issn = {1075-3583},
journal = {Linux Journal},
month = {sep},
number = {173},
pages = {2},
publisher = {Belltown Media},
title = {{Nginx: the high-performance web server and reverse proxy}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=1412204{\&}type=html},
volume = {2008},
year = {2008}
}
@book{Foster1995,
abstract = {From the Publisher: At last, a practitioner's guide to parallel programming! Students and professionals who use parallel or distributed computer systems will be able to solve real problems with Designing and Building Parallel Programs. This book provides a comprehensive introduction to parallel algorithm design, performance analysis, and program construction. It describes the tools needed to write parallel programs and provides numerous examples. A unique feature is the companion on-line version, accessible via the World Wide Web using browsers such as Mosaic. This provides a convenient hypertext version of the text with pointers to programming tools, example programs, and other resources on parallel and distributed computing.},
author = {Foster, Ian},
booktitle = {Interface},
file = {:home/etn/Documents/PhD/Biblio/Foster - 1995 - Designing and Building Parallel Programs.pdf:pdf},
isbn = {0201575949},
title = {{Designing and Building Parallel Programs}},
url = {http://www-rohan.sdsu.edu/faculty/mthomas/courses/docs/foster/Foster{\_}Designing{\_}and{\_}Building{\_}Parallel{\_}Programs.pdf},
year = {1995}
}
@article{Harrison1989,
author = {Harrison, Williams Ludwell},
doi = {10.1007/BF01808954},
issn = {0892-4635},
journal = {Lisp and Symbolic Computation},
month = {oct},
number = {3-4},
pages = {179--396},
title = {{The interprocedural analysis and automatic parallelization of Scheme programs}},
url = {http://link.springer.com/10.1007/BF01808954},
volume = {2},
year = {1989}
}
@phdthesis{Nicolay2010,
author = {Nicolay, Jens},
booktitle = {Citeseer},
file = {:home/etn/Documents/PhD/Biblio/Nicolay - 2010 - Automatic Parallelization of Scheme Programs using Static Analysis.pdf:pdf},
title = {{Automatic Parallelization of Scheme Programs using Static Analysis}},
url = {http://prog.vub.ac.be/{~}cderoove/publications/master{\_}thesis{\_}jens{\_}nicolay.pdf},
year = {2010}
}
@inproceedings{Jr1990,
abstract = {We need a programming model that com- bines the advantages of the synchronous and asyn- chronous parallel styles. Synchronous programs are de- terminate (thus easier to reason about) and avoid syn- chronization overheads. Asynchronous programs are more flexible and handle conditionals more efficiently. Here we propose a programming model with the ben- efits of both styles. We allow asynchronous threads of control but restrict shared-memory accesses and other side effects so as to prevent the behavior of the program from depending on any accidents of execution order that can arise from the indeterminacy of the asynchronous process model. These restrictions may be enforced either dynam- ically (at run time) or statically (at compile time). In this paper we concentrate on dynamic enforcement, and exhibit an implementation of a parallel dialect of Scheme based on these ideas. A single successful exe- cution of a parallel program in this model constitutes a proof that the program is free of race conditions (for that particular set of input data). We also speculate on a design for a programming lan- guage using static enforcement. The notion of distinct- ness is important to proofs of noninterference. An ap- propriately designed programming language must sup port such concepts as “all the elements of this array are distinct,” perhaps through its type system. This parallel programming model does not support all styles of parallel programming, but we argue that it can support a large class of interesting algorithms with considerably greater efficiency (in some cases) than a strict SIMD approach and considerably greater safety (in all cases) than a full-blown MIMD approach.},
author = {Guy, L and Steele, Jr},
booktitle = {Proceedings of the 17th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
doi = {10.1145/96709.96731},
file = {:home/etn/Documents/PhD/Biblio/Guy, Steele - 1990 - Making asynchronous parallelism safe for the world.pdf:pdf},
isbn = {0-89791-343-4},
issn = {07308566},
pages = {218--231},
title = {{Making asynchronous parallelism safe for the world}},
url = {http://dl.acm.org/citation.cfm?id=96731},
year = {1990}
}
@article{Harris2010,
abstract = {Abstract The advent of multicore processors has renewed interest in the idea of incorporating transactions into the programming model used to write parallel programs. This approach, known as transactional memory, offers an alternative, and hopefully better, way to coordinate concurrent threads. The ACI (atomicity, consistency, isolation) properties of transactions provide a foundation to ensure that concurrent reads and writes of shared data do not produce inconsistent or incorrect results. At a higher level, a computation wrapped in a transaction executes atomically - either it completes successfully and commits its result in its entirety or it aborts. In addition, isolation ensures the transaction produces the same result as if no other transactions were executing concurrently. Although transactions are not a parallel programming panacea, they shift much of the burden of synchronizing and coordinating parallel computations from a programmer to a compiler, to a language runtime system, or to hardware. Th...},
author = {Harris, Tim and Larus, James and Rajwar, Ravi},
doi = {10.2200/S00272ED1V01Y201006CAC011},
issn = {1935-3235},
journal = {Synthesis Lectures on Computer Architecture},
keywords = {cache coherence,compilers,computer architecture,computer hardware,concurrent programming,lock-free data structures,nonblocking algorithms,parallel programming,programming languages,synchronization,transactional memory},
language = {en},
month = {dec},
number = {1},
pages = {1--263},
publisher = {Morgan {\&} Claypool Publishers},
title = {{Transactional Memory, 2nd edition}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/s00272ed1v01y201006cac011},
volume = {5},
year = {2010}
}
@inproceedings{Valois1995,
address = {New York, New York, USA},
author = {Valois, John D.},
booktitle = {Proceedings of the fourteenth annual ACM symposium on Principles of distributed computing - PODC '95},
doi = {10.1145/224964.224988},
isbn = {0897917103},
month = {aug},
pages = {214--222},
publisher = {ACM Press},
title = {{Lock-free linked lists using compare-and-swap}},
url = {http://dl.acm.org/citation.cfm?id=224964.224988},
year = {1995}
}
@article{Herlihy1991,
abstract = {A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, there is no wait-free implementation of X by Y. We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest{\&}set and fetch{\&}add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.},
author = {Herlihy, Maurice},
doi = {10.1145/114005.102808},
file = {:home/etn/Documents/PhD/Biblio/Herlihy - 1991 - Wait-free synchronization.pdf:pdf},
isbn = {9780123705914},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
keywords = {linearization,wait-free synchronization},
month = {jan},
number = {1},
pages = {124--149},
publisher = {ACM},
title = {{Wait-free synchronization}},
url = {http://dl.acm.org/citation.cfm?id=114005.102808 http://cs.brown.edu/{~}mph/Herlihy91/p124-herlihy.pdf},
volume = {13},
year = {1991}
}
@inproceedings{Hendler2004,
address = {New York, New York, USA},
author = {Hendler, Danny and Shavit, Nir and Yerushalmi, Lena},
booktitle = {Proceedings of the sixteenth annual ACM symposium on Parallelism in algorithms and architectures - SPAA '04},
doi = {10.1145/1007912.1007944},
isbn = {1581138407},
month = {jun},
pages = {206},
publisher = {ACM Press},
title = {{A scalable lock-free stack algorithm}},
url = {http://dl.acm.org/citation.cfm?id=1007912.1007944},
year = {2004}
}
@inproceedings{Wimmer2015,
address = {New York, New York, USA},
author = {Wimmer, Martin and Gruber, Jakob and Tr{\"{a}}ff, Jesper Larsson and Tsigas, Philippas},
booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming - PPoPP 2015},
doi = {10.1145/2688500.2688547},
isbn = {9781450332057},
keywords = {Task-parallel programming,concurrent data structure relaxation,priority-queue,shared memory},
month = {jan},
pages = {277--278},
publisher = {ACM Press},
title = {{The lock-free k-LSM relaxed priority queue}},
url = {http://dl.acm.org/citation.cfm?id=2688500.2688547},
year = {2015}
}
@inproceedings{Ramachandran2015,
address = {New York, New York, USA},
author = {Ramachandran, Arunmoezhi and Mittal, Neeraj},
booktitle = {Proceedings of the 2015 International Conference on Distributed Computing and Networking - ICDCN '15},
doi = {10.1145/2684464.2684472},
isbn = {9781450329286},
keywords = {Binary,Concurrent Data Structure,Lock-Free Algorithm,Search Tree},
month = {jan},
pages = {1--10},
publisher = {ACM Press},
title = {{A Fast Lock-Free Internal Binary Search Tree}},
url = {http://dl.acm.org/citation.cfm?id=2684464.2684472},
year = {2015}
}
@inproceedings{Timnat2012,
abstract = {The linked-list data structure is fundamental and ubiquitous. Lock-free versions of the linked-list are well known. However, the existence of a practical wait-free linked-list has been open. In this work we designed such a linked-list. To achieve better performance, we have also extended this design using the fast-path-slow-path methodology. The resulting implementation achieves performance which is competitive with that of Harris’s lock-free list, while still guaranteeing non-starvation via wait-freedom.We have also developed a proof for the correctness and the wait-freedom of our design.},
author = {Timnat, Shahar and Braginsky, Anastasia and Kogan, Alex and Petrank, Erez},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-35476-2{\_}23},
isbn = {9783642354755},
issn = {03029743},
pages = {330--344},
title = {{Wait-free linked-lists}},
url = {http://www.cs.technion.ac.il/{~}erez/Papers/wfll-short.pdf},
volume = {7702 LNCS},
year = {2012}
}
@article{Nvidia2007,
author = {Nvidia, C},
title = {{Compute unified device architecture programming guide}},
url = {https://scholar.google.com/scholar?q=CUDA{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}1},
year = {2007}
}
@article{Dagum1998,
abstract = {At its most elemental level, OpenMP is a set of compiler$\backslash$ndirectives and callable runtime library routines that extend Fortran$\backslash$n(and separately, C and C++ to express shared memory parallelism. It$\backslash$nleaves the base language unspecified, and vendors can implement OpenMP$\backslash$nin any Fortran compiler. Naturally, to support pointers and$\backslash$nallocatables, Fortran 90 and Fortran 95 require the OpenMP$\backslash$nimplementation to include additional semantics over Fortran 77. OpenMP$\backslash$nleverages many of the X3H5 concepts while extending them to support$\backslash$ncoarse grain parallelism. The standard also includes a callable runtime$\backslash$nlibrary with accompanying environment variables},
author = {Dagum, L. and Menon, R.},
doi = {10.1109/99.660313},
isbn = {1070-9924 VO - 5},
issn = {1070-9924},
journal = {IEEE Computational Science and Engineering},
keywords = {ANSI standards,Coherence,Computer architecture,Fortran,Fortran 90,Fortran 95,Fortran compiler,Hardware,Message passing,OpenMP,Parallel processing,Parallel programming,Power system modeling,Scalability,Software systems,X3H5 concepts,allocatables,application program interfaces,callable runtime library,callable runtime library routines,coarse grain parallelism,compiler directives,environment variables,industry standard API,parallel programming,pointers,shared memory parallelism,shared memory programming,shared memory systems,software portability,software reviews,software standards},
language = {English},
number = {1},
pages = {46--55},
publisher = {IEEE},
title = {{OpenMP: an industry standard API for shared-memory programming}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=660313},
volume = {5},
year = {1998}
}
@article{Kuper2015,
author = {Kuper, Lindsey},
title = {{Prospect: Finding and Exploiting Parallelism in a Productivity Language for Scientific Computing - SPLASH 2015}},
url = {http://2015.splashcon.org/event/splash2015-splash-i-lindsey-kuper-talk},
year = {2015}
}
@article{Grosser2011,
abstract = {Various powerful polyhedral techniques exist to optimize computation intensive programs effectively. Applying these techniques on any non-trivial program is still surprisingly difficult and often not as effective as expected. Most polyhedral tools are limited to a specific programming language. Even for this language, relevant code needs to match specific syntax that rarely appears in existing code. It is therefore hard or even impossible to process existing programs automatically. In addition, most tools target C or OpenCL code, which prevents effective communication with compiler internal optimizers. As a result target architecture specific optimizations are either little effective or not approached at all. In this paper we present Polly, a project to enable polyhedral optimizations in LLVM. Polly automatically detects and transforms relevant programparts in a language-independent and syntactically transparent way. Therefore, it supports programs written in most common programming languages and constructs like C++ iterators, goto based loops and pointer arithmetic. Internally it provides a state-of-the-art polyhedral library with full support for Z-polyhedra, ad- vanced data dependency analysis and support for external optimizers. Polly includes integrated SIMD and OpenMP code generation. Through LLVM, machine code for CPUs and GPU accelerators, C source code and even hardware descriptions can be targeted.},
author = {Grosser, Tobias and Zheng, Hongbin and Aloor, Raghesh and Simb{\"{u}}rger, Andreas and Gr{\"{o}}{\ss}linger, Armin and Pouchet, Louis-No{\"{e}}l},
journal = {Proceedings of the First International Workshop on Polyhedral Compilation Techniques (IMPACT '11)},
pages = {None},
title = {{Polly - Polyhedral optimization in LLVM}},
url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-07.pdf},
year = {2011}
}
@misc{Trifunovic2010,
abstract = {Modern compilers are responsible for adapting the semantics of source programs into a form that makes efficient use of a highly complex, heterogeneous machine. This adaptation amounts to solve an optimization problem in a huge and unstructured search space, while predicting the performance outcome of complex sequences of program transformations. The polyhedral model of compilation is aimed at these challenges. Its geometrical, non-inductive semantics enables the construction of better-structured optimization problems and precise analytical models. Recent work demonstrated the scalability of the main polyhedral algorithms to real-world programs. Its integration into production compilers is under way, pioneered by the Graphite branch of the GNU Compiler Collection (GCC). Two years after the effective beginning of the project, this paper reports on original questions and innovative solutions that arose during the design and implementation of Graphite.},
author = {Trifunovic, Konrad and Cohen, Albert and Edelsohn, David and Li, Feng and Grosser, Tobias and Jagasia, Harsha and Ladelsky, Razya and Pop, Sebastian and Sj{\"{o}}din, Jan and Upadrasta, Ramakrishna},
booktitle = {GCC Research Opportunities Workshop (GROW'10)},
file = {:home/etn/Documents/PhD/Biblio/Trifunovic et al. - 2010 - GRAPHITE Two Years After First Lessons Learned From Real-World Polyhedral Compilation.pdf:pdf},
language = {en},
month = {jan},
title = {{GRAPHITE Two Years After: First Lessons Learned From Real-World Polyhedral Compilation}},
url = {https://hal.inria.fr/inria-00551516/},
year = {2010}
}
@article{Bastoul2004,
abstract = {Abstract. We seek to extend the scope and efficiency of iterative com- pilation techniques by searching not only for program transformation parameters but for the most appropriate transformations themselves. For that purpose, we need a generic way to express program transforma- ...},
address = {Berlin, Heidelberg},
author = {Bastoul, C{\'{e}}dric and Cohen, Albert and Girbal, Sylvain and Sharma, Saurabh and Temam, Olivier},
doi = {10.1007/b95707},
editor = {Rauchwerger, Lawrence},
isbn = {978-3-540-21199-0},
issn = {978-3-540-21199-0},
journal = {LCPC '04 Languages and Compilers for Parallel Computing},
number = {Chapter 14},
pages = {209--225},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Putting Polyhedral Loop Transformations to Work}},
url = {http://link.springer.com/10.1007/b95707 http://www.springerlink.com/index/10.1007/978-3-540-24644-2{\_}14$\backslash$npapers2://publication/doi/10.1007/978-3-540-24644-2{\_}14},
volume = {2958},
year = {2004}
}
@article{Darema1988,
abstract = {We present a single-program-multiple-data computational model which we have implemented in the EPEX system to run in parallel mode FORTRAN scientific application programs. The computational model assumes a shared memory organization and is based on the scheme that all processes executing a program in parallel remain in existence for the entire execution; however, the tasks to be executed by each process are determined dynamically during execution by the use of appropriate synchronizing constructs that are imbedded in the program. We have demonstrated the applicability of the model in the parallelization of several applications. We discuss parallelization features of these applications and performance issues such as overhead, speedup, efficiency.},
author = {Darema, F. and George, D.A. and Norton, V.A. and Pfister, G.F.},
doi = {10.1016/0167-8191(88)90094-4},
issn = {01678191},
journal = {Parallel Computing},
keywords = {EPEX/FORTRAN,Shared memory multiprocessor,computational model,parallelization features},
month = {apr},
number = {1},
pages = {11--24},
title = {{A single-program-multiple-data computational model for EPEX/FORTRAN}},
url = {http://www.sciencedirect.com/science/article/pii/0167819188900944},
volume = {7},
year = {1988}
}
@inproceedings{Auguin1983,
author = {Auguin, Michel and Larbey, Francois},
booktitle = {Microcomputers: developments in industry, business, and education},
keywords = {SPMD},
mendeley-tags = {SPMD},
pages = {311--318},
title = {{OPSILA: an advanced SIMD for numerical analysis and signal processing}},
url = {https://scholar.google.com/scholar?q=OPSILA {\%}3A an advanced SIMD for numerical analysis and signal processing{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}0},
year = {1983}
}
@incollection{Darema2001,
abstract = {I proposed the SPMD (Single Program Multiple Data) model, in January 19841, as a means for enabling parallel execution of applications on multiprocessors, and in particular for highly parallel machines like the RP3 (the IBM Research Parallel Processor Prototype2). This talk will provide a review of the origins of the SPMD, it’s early use in enabling parallel execution of scientific applications and it’s implementation in one of the first3 parallel programming environments. In fact [3] was the first programming environment that implemented the SPMD model; other environments in the 1985 timeframe were based on the fork-and-join (F{\&}J) model.},
author = {Darema, Frederica},
booktitle = {Parallel Computing},
doi = {10.1007/3-540-45417-9{\_}1},
isbn = {978-3-540-42609-7},
pages = {1},
title = {{The SPMD Model: Past , Present and Future}},
year = {2001}
}
@article{Chan2004,
abstract = {Many parallel applications involve different independent tasks$\backslash$nwith their own data. Using the MPMD model, programmers can have$\backslash$na modular view and simplified structure of the parallel$\backslash$nprograms. Although MPI supports both SPMD and MPMD models for$\backslash$nprogramming, MPI libraries do not provide an efficient way for$\backslash$ntask communication for the MPMD model. We have developed a$\backslash$nprogramming environment, called ClusterGOP, for building and$\backslash$ndeveloping parallel applications. Based on the graph-oriented$\backslash$nprogramming (GOP) model, ClusterGOPprovides higher-level$\backslash$nabstractions for message-passing parallel programming with the$\backslash$nsupport of software tools for developing and running parallel$\backslash$napplications. In this paper, we describe how ClusterGOP supports$\backslash$nprogramming of MPMD parallel applications on top of MPI. We$\backslash$ndiscuss the issues of implementing the MPMD model in ClusterGOP$\backslash$nusing MPI and evaluate the performanceby using example$\backslash$napplications.},
author = {Chan, F and Cao, J N and Chan, A T S and Guo, M Y},
file = {:home/etn/Documents/PhD/Biblio/Chan et al. - 2004 - Programming support for MPMD parallel computing in ClusterGOP.pdf:pdf},
journal = {IEICE Transactions on Information and Systems},
number = {7},
pages = {1693--1702},
title = {{Programming support for MPMD parallel computing in ClusterGOP}},
url = {http://www.cs.sjtu.edu.cn/{~}guo-my/PDF/Journals/J43.pdf},
volume = {E87D},
year = {2004}
}
@book{Snir1996,
author = {Snir, O and Huss-Lederman, W and Dongarra, M P I},
isbn = {9780071598422},
title = {{MPI: The Complete Reference}},
url = {https://scholar.google.com/scholar?q=MPI{\%}3A the complete referen{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}3 https://scholar.google.com/scholar?q=MPI{\%}3A the complete referen{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}1},
year = {1996}
}
@article{Chang1997,
abstract = {The MPMD approach for parallel computing is attractive for programmers who seek fast development cycles, high code re-use, and modular programming, or whose applications exhibit irregular computation loads and communication patterns. RPC is widely adopted as the communication abstraction for crossing address space boundaries. However, the communication overheads of existing RPC-based systems are usually an order of magnitude higher than those found in highly tuned SPMD systems. This problem has thus far limited the appeal of high-level programming languages based on MPMD models in the parallel computing community. This paper investigates the fundamental limitations of MPMD communication using a case study of two parallel programming languages, Compositional C++ (CC++) and Split-C, that provide support for a global name space. To establish a common comparison basis, our implementation of CC++ was developed to use MRPC, a RPC system optimized for MPMD parallel computing and based on Active Messages. Basic RPC performance in CC++ is within a factor of two from those of Split-C and other messaging layers. CC++ applications perform within a factor of two to six from comparable Split-C versions, which represent an order of magnitude improvement over previous CC++ implementations. The results suggest that RPC-based communication can be used effectively in many high-performance MPMD parallel applications.},
author = {Chang, Chi-Chao and Czajkowski, G. and Eicken, T. Von and Kesselman, C.},
doi = {10.1109/SC.1997.10040},
file = {:home/etn/Documents/PhD/Biblio/Chang et al. - 1997 - Evaluating the Performance Limitations of MPMD Communication.pdf:pdf},
isbn = {0-89791-985-8},
journal = {ACM/IEEE SC 1997 Conference (SC'97)},
pages = {1--10},
title = {{Evaluating the Performance Limitations of MPMD Communication}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.8068{\&}rep=rep1{\&}type=pdf},
year = {1997}
}
@incollection{K.ManiChandy2005,
abstract = {There is a class of sparse matrix computations, such as direct solvers of systems of linear equations, that change the fill-in (nonzero entries) of the coefficient matrix, and involve row and column operations (pivoting). This paper addresses the problem of the parallelization of these sparse computations from the point of view of the parallel language and the compiler. Dynamic data structures for sparse matrix storage are analyzed, permitting to efficiently deal with fill-in and pivoting issues. Any of the data representations considered enforces the handling of indirections for data accesses, pointer referencing and dynamic data creation. All of these elements go beyond current data-parallel compilation technology. We propose a small set of new extensions to HPF-2 to parallelize these codes, supporting part of the new capabilities on a runtime library. This approach has been evaluated on a Cray T3E, implementing, in particular, the sparse LU factorization.},
author = {Chandy, K. Mani and Kesselman, Carl},
booktitle = {Languages and Compilers for Parallel Computing},
doi = {10.1007/3-540-48319-5},
file = {:home/etn/Documents/PhD/Biblio/Chandy, Kesselman - 2005 - Compositional C Compositional parallel programming.pdf:pdf},
isbn = {978-3-540-66426-0},
issn = {0302-9743},
pages = {124--144},
title = {{Compositional C++: Compositional parallel programming}},
url = {http://authors.library.caltech.edu/26756/2/92-13.pdf http://www.springerlink.com/content/dd8uk1lp4pvmclta},
volume = {757},
year = {2005}
}
@article{Grimshaw1991,
author = {Grimshaw, Andrew S.},
month = {apr},
publisher = {University of Virginia},
title = {{An Introduction to Parallel Object-Oriented Programming with Mentat}},
url = {http://dl.acm.org/citation.cfm?id=900803},
year = {1991}
}
@article{Foster1995b,
abstract = {Fortran M is a small set of extensions to Fortran 77 that supports a modular approach to the design of message-passing programs. It has the following features. (1) Modularity. Programs are constructed by using explicitly-declared communication channels to plug together program modules called processes. A process can encapsulate common data, subprocesses, and internal communication. (2) Safety. Operations on channels are restricted so as to guarantee deterministic execution, even in dynamic computations that create and delete processes and channels. Channels are typed, so a compiler can check for correct usage. (3) Architecture Independence. The mapping of processes to processors can be specified with respect to a virtual computer with size and shape different from that of the target computer. Mapping is specified by annotations that influence performance but not correctness. (4) Efficiency. Fortran M can be compiled efficiently for uniprocessors, sharedmemory computers, distributed-memory computers, and networks of workstations. Because message passing is incorporated into the language, a compiler can optimize communication as well as computation.},
author = {Foster, I.T. and Chandy, K M},
doi = {10.1006/jpdc.1995.1044},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = {apr},
number = {1},
pages = {24--35},
title = {{Fortran M: A Language for Modular Parallel Programming}},
url = {http://www.sciencedirect.com/science/article/pii/S0743731585710441},
volume = {26},
year = {1995}
}
@article{Foster1996,
abstract = {Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems.},
author = {Foster, Ian and Kesselman, Carl and Tuecke, Steven},
doi = {10.1006/jpdc.1996.0108},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = {aug},
number = {1},
pages = {70--82},
title = {{The Nexus Approach to Integrating Multithreading and Communication}},
url = {http://www.sciencedirect.com/science/article/pii/S0743731596901082 http://linkinghub.elsevier.com/retrieve/pii/S0743731596901082},
volume = {37},
year = {1996}
}
@article{Culler,
author = {Culler, David E. and Dusseau, A. and Goldstein, Seth Copen and Krishnamurthy, Arvind and Lumetta, Steven and {Von Eicken}, Thorsten and Yelick, Katherine},
doi = {10.1109/SUPERC.1993.1263470},
file = {:home/etn/Documents/PhD/Biblio/Culler et al. - Unknown - Parallel programming in Split-C.pdf:pdf},
isbn = {0-8186-4340-4},
issn = {1063-9535},
keywords = {C language,Computer languages,Computer science,Cost function,Frequency,Message passing,Parallel processing,Parallel programming,Predictive models,Program processors,Signal processing,Split-C language,assignment operators,data parallel programming,distributed memory multiprocessors,global address space,high performance programming,language concepts,locality,message passing,parallel extension,parallel languages,parallel programming,performance results,program optimization,remote access,shared memory,software performance evaluation},
language = {English},
pages = {262--273},
publisher = {IEEE},
title = {{Parallel programming in Split-C}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1263470}
}
@article{Johnson1995,
author = {Johnson, Kirk L. and Kaashoek, M. Frans and Wallach, Deborah A.},
doi = {10.1145/224057.224073},
file = {:home/etn/Documents/PhD/Biblio/Johnson, Kaashoek, Wallach - 1995 - CRL High-Performance All-Software Distributed Shared Memory.pdf:pdf},
isbn = {0-89791-715-4},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
month = {dec},
number = {5},
pages = {213--226},
publisher = {ACM},
title = {{CRL: High-Performance All-Software Distributed Shared Memory}},
url = {http://dl.acm.org/citation.cfm?id=224057.224073},
volume = {29},
year = {1995}
}
@article{Walker1996,
author = {Walker, DW and Dongarra, JJ},
file = {:home/etn/Documents/PhD/Biblio/Walker, Dongarra - 1996 - MPI a standard message passing interface.ps:ps},
isbn = {0168-7875},
issn = {0168-7875},
journal = {Supercomputer},
pages = {56--68},
title = {{MPI: a standard message passing interface}},
url = {http://users.cs.cf.ac.uk/David.W.Walker/papers/supercomputer96.ps},
volume = {12},
year = {1996}
}
@article{Sunderam1994,
abstract = {The PVM system, a software framework for heterogeneous concurrent computing in networked environments, has evolved in the past several years into a viable technology for distributed and parallel processing in a variety of disciplines. PVM supports a straightforward but functionally complete message passing model, and is capable of harnessing the combined resources of typically heterogeneous networked computing platforms to deliver high levels of performance and functionality. In this paper, we describe the architecture of PVM system, and discuss its computing model, the programming interface it supports, auxiliary facilities for process groups and MPP support, and some of the internal implementation techniques employed. Performance issues, dealing primarily with communication overheads, are analyzed, and recent findings as well as experimental enhancements are presented. In order to demonstrate the viability of PVM for large scale scientific supercomputing, the paper includes representative case studies in materials science, environmental science, and climate modeling. We conclude with a discussion of related projects and future directions, and comment on near and long-term potential for network computing with the PVM system.},
author = {Sunderam, V.S and Geist, G.A and Dongarra, J and Manchek, R},
doi = {10.1016/0167-8191(94)90027-2},
file = {:home/etn/Documents/PhD/Biblio/Sunderam et al. - 1994 - The PVM concurrent computing system Evolution, experiences, and trends.pdf:pdf},
isbn = {0167-8191},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Case studies,Message passing,Networked computing platforms,PVM system,Performance},
month = {apr},
number = {4},
pages = {531--545},
title = {{The PVM concurrent computing system: Evolution, experiences, and trends}},
url = {http://www.sciencedirect.com/science/article/pii/0167819194900272},
volume = {20},
year = {1994}
}
@article{Thusoo2009,
author = {Thusoo, Ashish and Sarma, Joydeep Sen and Jain, Namit and Shao, Zheng and Chakka, Prasad and Anthony, Suresh and Liu, Hao and Wyckoff, Pete and Murthy, Raghotham},
doi = {10.14778/1687553.1687609},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
month = {aug},
number = {2},
pages = {1626--1629},
publisher = {VLDB Endowment},
title = {{Hive}},
url = {http://dl.acm.org/citation.cfm?id=1687553.1687609},
volume = {2},
year = {2009}
}
@article{Olston2008,
abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where inno- vation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively ex- pensive at this scale. Besides, many of the people who ana- lyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hard- ware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the develop- ment and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.},
address = {New York, New York, USA},
author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
doi = {10.1145/1376616.1376726},
file = {:home/etn/Documents/PhD/Biblio/Olston et al. - 2008 - Pig Latin A Not-So-Foreign Language for Data Processing.pdf:pdf},
isbn = {978-1-60558-102-6},
issn = {07308078},
journal = {Proceedings of the 2008 ACM SIGMOD international conference on Management of data - SIGMOD '08},
keywords = {dataflow language,pig latin},
month = {jun},
pages = {1099},
publisher = {ACM Press},
title = {{Pig Latin: A Not-So-Foreign Language for Data Processing}},
url = {http://dl.acm.org/citation.cfm?id=1376616.1376726},
year = {2008}
}
@article{Thies2002,
abstract = {We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly im- portant. The StreamIt language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreamIt compiler aims to im- prove the performance of streaming applications via stream-specific anal- yses and optimizations. In this paper, we motivate, describe and justify the language features of StreamIt, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax.},
author = {Thies, William and Karczmarek, Michal and Amarasinghe, Saman},
doi = {10.1007/3-540-45937-5},
file = {:home/etn/Documents/PhD/Biblio/Thies, Karczmarek, Amarasinghe - 2002 - StreamIt A language for streaming applications.ps:ps},
isbn = {3540433694},
issn = {0302-9743},
journal = {Compiler Construction},
pages = {179--196},
title = {{StreamIt: A language for streaming applications}},
url = {http://link.springer.com/chapter/10.1007/3-540-45937-5{\_}14 http://rtsys.informatik.uni-kiel.de/svn/teaching/sem/11ss-conc/dkr/11ss-conc-dkr-talk.pdf http://www.springerlink.com/index/LC5B77HWR8J2UBHK.pdf$\backslash$nhttp://groups.csail.mit.edu/commit/papers/02/stream},
volume = {LNCS 2304},
year = {2002}
}
@article{Ragan-Kelley2013,
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because},
author = {Ragan-Kelley, Jonathan and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'{e}}do and Barnes, Connelly and Amarasinghe, Saman},
doi = {10.1145/2491956.2462176},
file = {:home/etn/Documents/PhD/Biblio/Ragan-Kelley et al. - 2013 - Halide A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing.pdf:pdf},
isbn = {978-1-4503-2014-6},
issn = {03621340},
journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
keywords = {autotuning,compiler,domain specific language,gpu,image processing,locality,optimization,parallelism,redundant computation,vectorization},
pages = {519--530},
title = {{Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines}},
url = {http://people.csail.mit.edu/jrk/halide-pldi13.pdf http://doi.acm.org/10.1145/2491956.2462176},
year = {2013}
}
@article{Chen2008,
abstract = {This paper describes a general and robust loop transformation framework that enables compilers to generate efficient code on complex loop nests. Despite two decades of prior research on loop optimization, performance of compiler-generated code often falls short of manually optimized versions, even for some well-studied BLAS kernels. There are two primary reasons for this. First, today’s compilers employ fixed transformation strategies, making it difficult to adapt to different optimization requirements for different application codes. Second, code transformations are treated in isolation, not taking into account the interactions between different transformations. This paper addresses such limitations in a unified framework that supports a broad collection of transformations, (permutation, tiling, unroll-and-jam, data copying, iteration space splitting, fusion, distribution and others), which go beyond existing polyhedral transformation models. This framework is a key element of a compiler we are developing which performs empirical op- timization to evaluate a collection of alternative optimized variants of a code segment. A script interface to code generation and empirical search permits transformation parameters to be adjusted independently and tested; alterna- tive scripts are used to represent different code variants. By applying this framework to example codes,we showperformance results on automatically- generated code for the PentiumM and MIPS R10000 that are comparable to the best hand-tuned codes, and significantly better (up to a 14x speedup) than the native compilers.},
author = {Chen, Chun and Chame, Jacqueline and Hall, Mary},
doi = {10.1001/archneur.64.6.785},
file = {:home/etn/Documents/PhD/Biblio/Chen, Chame, Hall - 2008 - CHiLL A framework for composing high-level loop transformations.pdf:pdf},
issn = {00039942},
journal = {U. of Southern California, Tech. Rep},
pages = {1--28},
pmid = {17562926},
title = {{CHiLL: A framework for composing high-level loop transformations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8396{\&}rep=rep1{\&}type=pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:CHiLL+:+A+Framework+for+Composing+High-Level+Loop+Transformations{\#}0},
year = {2008}
}
@article{Catanzaro2009,
abstract = {Parallel programming must be accessible to domain experts without requiring them to become experts in parallel hardware architecture. While domain experts today prefer to use high-level “productivity” scripting languages with domain-appropriate abstractions, achieving high performance still requires expertise in lower-level “efficiency” lan- guages (CUDA, CILK, C with OpenMP) that expose hardware-level programming models directly. We bridge this gap through the use of embedded just-in-time specialization: domain experts write in high-level scripting languages, but at runtime, we specialize (generate, compile, and execute efficiency-language source code for) an application-specific and platform-specific subset of the productivity language. This enables invisible and selective optimization of only those application-level abstractions that enjoy a large performance advantage when expressed in an efficiency lan- guage on the available hardware and will be executed many times, amortizing the overhead of specialization. Because the specialization machinery is implemented in the productivity language, efficiency programmers can easily extend our system by adding new specializers for specific additional domain abstractions or new hardware, transparently to the productivity-language programmers. Our approach results in competitive performance on real applications with a fraction of the programming effort on the part of the domain expert. We argue that the separation of concerns enabled by embedded JIT specialization allows research to proceed in parallel on both the productivity and efficiency layers, and is therefore uniquely suited to the problem of making different parallel hardware architectures more accessible to domain-expert programmers with a fraction of the programmer time and effort.},
author = {Catanzaro, Bryan and Kamil, Shoaib and Lee, Yunsup},
doi = {10.1.1.212.6088},
file = {:home/etn/Documents/PhD/Biblio/Catanzaro, Kamil, Lee - 2009 - SEJITS Getting productivity and performance with selective embedded JIT specialization.pdf:pdf},
isbn = {978-0-9825442-3-5},
journal = {{\ldots} Models for Emerging {\ldots}},
pages = {1--10},
pmid = {8401072916273492945},
title = {{SEJITS: Getting productivity and performance with selective embedded JIT specialization}},
url = {http://www.eecs.berkeley.edu/{~}krste/papers/SEJITS-pmea2009.pdf http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1{\&}type=pdf{\&}doi=10.1.1.212.6088$\backslash$nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-23.pdf},
year = {2009}
}
@article{Bauer2012,
abstract = {Modern parallel architectures have both heterogeneous processors and deep, complex memory hierarchies. We present Legion, a programming model and runtime system for achieving high performance on these machines. Legion is organized around logical regions, which express both locality and independence of program data, and tasks, functions that perform computations on regions. We describe a runtime system that dynamically extracts parallelism from Legion programs, using a distributed, parallel scheduling algorithm that identifies both independent tasks and nested parallelism. Legion also enables explicit, programmer controlled movement of data through the memory hierarchy and placement of tasks based on locality information via a novel mapping interface. We evaluate our Legion implementation on three applications: fluid-flow on a regular grid, a three-level AMR code solving a heat diffusion equation, and a circuit simulation.},
author = {Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
doi = {10.1109/SC.2012.71},
file = {:home/etn/Documents/PhD/Biblio/Bauer et al. - 2012 - Legion Expressing Locality and Independence with Logical Regions.pdf:pdf},
isbn = {9781467308069},
issn = {21674329},
journal = {Proceedings of the International Conference on High Performance Computing Networking Storage and Analysis SC 12},
month = {nov},
pages = {1--11},
publisher = {IEEE Computer Society Press},
title = {{Legion: Expressing Locality and Independence with Logical Regions}},
url = {http://dl.acm.org/citation.cfm?id=2388996.2389086},
year = {2012}
}
@article{Slaughter2015,
author = {Slaughter, Elliott and Lee, Wonchan and Treichler, Sean and Bauer, Michael},
doi = {10.1145/2807591.2807629},
file = {:home/etn/Documents/PhD/Biblio/Slaughter et al. - 2015 - Regent A High-Productivity Programming Language for HPC with Logical Regions.pdf:pdf},
isbn = {9781450337236},
journal = {SC},
keywords = {legion,logical regions,regent,task-based runtimes},
title = {{Regent : A High-Productivity Programming Language for HPC with Logical Regions}},
url = {http://legion.stanford.edu/pdfs/regent2015.pdf},
year = {2015}
}
@phdthesis{Randall1998,
abstract = {This thesis describes Cilk, a parallel multithreaded language for$\backslash$nprogramming contemporary shared memory multiprocessors (SMP's). Cilk$\backslash$nis a simple extension of C which provides constructs for parallel$\backslash$ncontrol and synchronization. Cilk imposes very low overheads--- the$\backslash$ntypical cost of spawning a parallel thread is only between 2 and$\backslash$n6 times the cost of a C function call on a variety of contemporary$\backslash$nmachines. Many Cilk programs run on one processor with virtually$\backslash$nno degradation compared to equivalent C programs. We present the$\backslash$n{\&}quot;work-first {\&}quot; principle which guided the design of Cilk's$\backslash$nscheduler and two consequences of this principle, a novel {\&}quot;two-clone{\&}quot;$\backslash$ncompilation strategy and a Dijkstra-like mutual-exclusion protocol$\backslash$nfor implementing the ready queue in the work-stealing scheduler.$\backslash$nTo facilitate debugging of Cilk programs, Cilk provides a tool called$\backslash$nthe Nondeterminator-2 which finds nondeterministic bugs called {\&}quot;data$\backslash$nraces{\&}quot;. We present two algorithms, All-Sets and Brelly, used$\backslash$nby the Nondeterminator-2 for finding data races. The All-Sets algorithm$\backslash$nis exact but can sometimes have poor performance; the Brelly algorithm,$\backslash$nby imposing a locking discipline on the programmer, is guaranteed},
author = {Randall, K.H.},
booktitle = {Engineering},
file = {:home/etn/Documents/PhD/Biblio/Randall - 1998 - Cilk Efficient Multithreaded Computing.pdf:pdf},
title = {{Cilk: Efficient Multithreaded Computing}},
url = {http://supertech.csail.mit.edu/papers/randall-phdthesis.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.6664{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {30},
year = {1998}
}
@article{Frigo1998,
abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
doi = {10.1145/277652.277725},
eprint = {9809069v1},
file = {:home/etn/Documents/PhD/Biblio/Frigo, Leiserson, Randall - 1998 - The implementation of the Cilk-5 multithreaded language.pdf:pdf},
isbn = {0897919874},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {critical path,ming language,multithreading,parallel computing,program-,runtime system,work},
month = {may},
number = {5},
pages = {212--223},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
publisher = {ACM},
title = {{The implementation of the Cilk-5 multithreaded language}},
url = {http://dl.acm.org/citation.cfm?id=277652.277725},
volume = {33},
year = {1998}
}
@article{Leiserson2010,
abstract = {The availability of multicore processors across a wide range of computing platforms has created a strong demand for software frameworks that can harness these resources. This paper overviews the Cilk++ programming environment, which incorporates a compiler, a runtime system, and a race-detection tool. The Cilk++ runtime system guarantees to load-balance computations effectively. To cope with legacy codes containing global variables, Cilk++ provides a ldquohyperobjectrdquo library which allows races on nonlocal variables to be mitigated without lock contention or substantial code restructuring.},
author = {Leiserson, Charles E.},
doi = {10.1007/s11227-010-0405-3},
file = {:home/etn/Documents/PhD/Biblio/Leiserson - 2010 - The Cilk concurrency platform.pdf:pdf},
isbn = {978-1-6055-8497-3},
issn = {09208542},
journal = {Journal of Supercomputing},
keywords = {Amdahl's Law,Dag model,Hyperobject,Multicore programming,Multithreading,Parallel programming,Parallelism,Race detection,Reducer,Span,Speedup,Work},
month = {mar},
number = {3},
pages = {244--257},
title = {{The Cilk++ concurrency platform}},
url = {http://link.springer.com/10.1007/s11227-010-0405-3},
volume = {51},
year = {2010}
}
@article{Lee2013,
abstract = {Pipeline parallelism organizes a parallel program as a linear sequence of s stages. Each stage processes elements of a data stream, passing each processed data element to the next stage, and then taking on a new element before the subsequent stages have necessarily completed their processing. Pipeline parallelism is used especially in streaming applications that perform video, audio, and digital signal processing. Three out of 13 benchmarks in PARSEC, a popular software benchmark suite designed for shared-memory multiprocessors, can be expressed as pipeline parallelism. Whereas most concurrency platforms that support pipeline parallelism use a “construct-and-run” approach, this paper investigates “on-the-fly” pipeline parallelism, where the structure of the pipeline emerges as the program executes rather than being specified a priori. On-the-fly pipeline parallelism allows the number of stages to vary from iteration to iteration and dependencies to be data dependent. We propose simple linguistics for specifying on-the-fly pipeline parallelism and describe a provably efficient scheduling algorithm, the P IPER algorithm, which integrates pipeline parallelism into a work-stealing scheduler, allowing pipeline and fork-join parallelism to be arbitrarily nested. The PIPER algorithm automatically throttles the parallelism, precluding “runaway” pipelines. Given a pipeline computation with T{\_}1 work and T{\_}infty span (critical-path length), P IPER executes the computation on P processors in T P ≤ T 1 /P + O(T ∞ + lg P) expected time. PIPER also limits stack space, ensuring that it does not grow unboundedly with running time. We have incorporated on-the-fly pipeline parallelism into a Cilk-based work-stealing runtime system. Our prototype Cilk-P implementation exploits optimizations such as lazy enabling and dependency folding. We have ported the three PARSEC benchmarks that exhibit pipeline parallelism to run on Cilk-P. One of these, x264, cannot readily be executed by systems that support only construct-and-run pipeline parallelism. Benchmark results indicate that Cilk-P has low serial overhead and good scalability. On x264, for example, Cilk-P exhibits a speedup of 13.87 over its respective serial counterpart when running on 16 processors.},
author = {Lee, I-Ting Angelina and Leiserson, Charles E. and Schardl, Tao B. and Sukha, Jim and Zhang, Zhunping},
doi = {10.1145/2486159.2486174},
file = {:home/etn/Documents/PhD/Biblio/Lee et al. - 2013 - On-the-fly pipeline parallelism.pdf:pdf},
isbn = {9781450315722},
journal = {Proceedings of the 25th ACM symposium on Parallelism in algorithms and architectures},
keywords = {cilk,multicore,multithreading,on-the-fly pipelining,parallel programming,parallelism,pipeline,scheduling,work stealing},
pages = {140},
title = {{On-the-fly pipeline parallelism}},
url = {http://supertech.csail.mit.edu/papers/spaa030-lee.pdf http://dl.acm.org/citation.cfm?doid=2486159.2486174},
year = {2013}
}
@inproceedings{Zheng2014,
abstract = {Partitioned Global Address Space (PGAS) languages are convenient for expressing algorithms with large, random-access data, and they have proven to provide high performance and scalability through lightweight one-sided communication and locality control. While very convenient for moving data around the system, PGAS languages have taken different views on the model of computation, with the static Single Program Multiple Data (SPMD) model providing the best scalability. In this paper we present UPC++, a PGAS extension for C++ that has three main objectives: 1) to provide an object-oriented PGAS programming model in the context of the popular C++ language, 2) to add useful parallel programming idioms unavailable in UPC, such as asynchronous remote function invocation and multidimensional arrays, to support complex scientific applications, 3) to offer an easy on-ramp to PGAS programming through interoperability with other existing parallel programming systems (e.g., MPI, OpenMP, CUDA). We implement UPC++ with a "compiler-free" approach using C++ templates and runtime libraries. We borrow heavily from previous PGAS languages and describe the design decisions that led to this particular set of language features, providing significantly more expressiveness than UPC with very similar performance characteristics. We evaluate the programmability and performance of UPC++ using five benchmarks on two representative supercomputers, demonstrating that UPC++ can deliver excellent performance at large scale up to 32K cores while offering PGAS productivity features to C++ applications.},
author = {Zheng, Yili and Kamil, Amir and Driscoll, Michael B. and Shan, Hongzhang and Yelick, Katherine},
booktitle = {2014 IEEE 28th International Parallel and Distributed Processing Symposium},
doi = {10.1109/IPDPS.2014.115},
file = {:home/etn/Documents/PhD/Biblio/Zheng et al. - 2014 - UPC A PGAS Extension for C.pdf:pdf},
isbn = {978-1-4799-3800-1},
issn = {1530-2075},
keywords = {Arrays,C++ language,C++ templates,Electronics packaging,Instruction sets,Libraries,PGAS,PGAS language extension,Parallel Computing,Parallel Programming,Programming,Programming System,SPMD model,Syntactics,Titanium,UPC++,asynchronous remote function invocation,compiler-free approach,complex scientific applications,data models,large random-access data,lightweight one-sided communication,locality control,multidimensional arrays,object-oriented PGAS programming model,object-oriented programming,parallel programming,parallel programming systems,partitioned global address space languages,representative supercomputers,runtime libraries,static single program multiple data model},
pages = {1105--1114},
title = {{UPC++: A PGAS Extension for C++}},
url = {http://web.eecs.umich.edu/{~}akamil/papers/ipdps14.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6877339},
year = {2014}
}
@inproceedings{Edwards2012,
address = {New York, New York, USA},
author = {Edwards, H. Carter and Sunderland, Daniel},
booktitle = {Proceedings of the 2012 International Workshop on Programming Models and Applications for Multicores and Manycores - PMAM '12},
doi = {10.1145/2141702.2141703},
isbn = {9781450312110},
keywords = {GPU,manycore,mini-application,multicore,multidimensional array},
month = {feb},
pages = {1--10},
publisher = {ACM Press},
title = {{Kokkos Array performance-portable manycore programming model}},
url = {http://dl.acm.org/citation.cfm?id=2141702.2141703},
year = {2012}
}
@article{Hornung2014,
author = {Hornung, R D and Keasler, J A},
file = {:home/etn/Documents/PhD/Biblio/Hornung, Keasler - 2014 - The RAJA Portability Layer Overview and Status.pdf:pdf},
title = {{The RAJA Portability Layer : Overview and Status}},
url = {https://e-reports-ext.llnl.gov/pdf/782261.pdf},
year = {2014}
}
@misc{Kaiser2015,
author = {Kaiser, Hartmut and Heller, Thomas and Bourgeois, Daniel},
booktitle = {Proceedings of the First International Workshop on Extreme Scale Programming Models and Middleware - ESPM '15},
file = {:home/etn/Documents/PhD/Biblio/Kaiser, Heller, Bourgeois - 2015 - Higher-level Parallelization for Local and Distributed Asynchronous Task-Based Programming.pdf:pdf},
title = {{Higher-level Parallelization for Local and Distributed Asynchronous Task-Based Programming}},
url = {http://stellar.cct.lsu.edu/pubs/executors{\_}espm2{\_}2015.pdf},
urldate = {2015-11-24},
year = {2015}
}
@inproceedings{Ajima2015,
address = {New York, New York, USA},
author = {Ajima, Yuichiro and Nose, Takafumi and Saga, Kazushige and Shida, Naoyuki and Sumimoto, Shinji},
booktitle = {Proceedings of the First International Workshop on Extreme Scale Programming Models and Middleware - ESPM '15},
doi = {10.1145/2832241.2832242},
file = {:home/etn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ajima et al. - 2015 - ACPdl.pdf:pdf},
isbn = {9781450339964},
keywords = {PGAS,communication library,data structure,global memory allocator},
month = {nov},
pages = {11--18},
publisher = {ACM Press},
title = {{ACPdl}},
url = {http://dl.acm.org/citation.cfm?id=2832241.2832242},
year = {2015}
}
@article{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine a},
doi = {10.1145/1562764.1562783},
file = {:home/etn/Documents/PhD/Biblio/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
isbn = {UCB/EECS-2006-183},
issn = {00010782},
journal = {EECS Department University of California Berkeley Tech Rep UCBEECS2006183},
pages = {19},
pmid = {8429457},
title = {{The Landscape of Parallel Computing Research : A View from Berkeley}},
url = {http://rrsg.ee.uct.ac.za/courses/EEE4084F/Archive/2014/Assignments/Reading Assignments/R01+Berkeley+2006+-+Landscale+of+Parallel+Computing+Research.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8705{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {18},
year = {2006}
}
