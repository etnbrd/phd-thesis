Automatically generated by Mendeley Desktop 1.16-dev4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{McCool2010,
abstract = {Many-core processors target improved computational performance by making available various forms of architectural parallelism, including but not limited to multiple cores and vector instructions. However, approaches to parallel programming based on targeting these low-level parallel mechanisms directly leads to overly complex, non-portable, and often unscalable and unreliable code. A more structured approach to designing and implementing parallel algorithms is useful to reduce the complexity of developing software for such processors, and is particularly relevant for many-core processors with a large amount of parallelism and multiple parallelism mechanisms. In particular, efficient and reliable parallel programs can be designed around the composition of deterministic algorithmic skeletons, or patterns. While improving the productivity of experts, specific patterns and fused combinations of patterns can also guide relatively inexperienced users to developing efficient algorithm implementations that have good scalability. The approach to parallelism described in this document includes both collective "data-parallel" patterns such as map and reduce as well as structured "task-parallel" patterns such as pipelining and superscalar task graphs. The structured pattern based approach, like data-parallel models, addresses issues of both data access and parallel task distribution in a common framework. Optimization of data access is important for both many-core processors with shared memory systems and accelerators with their own memories not directly attached to the host processor. A catalog of useful structured serial and parallel patterns will be presented. Serial patterns are presented because structured parallel programming can be considered an extension of structured control flow in serial programming. We will emphasize deterministic patterns in order to support the development of systems that automatically avoid unsafe race conditions and deadlock.},
author = {{Michael D}, McCool},
file = {:home/etn/Documents/PhD/Biblio/Michael D - 2010 - Structured parallel programming with deterministic patterns.pdf:pdf},
journal = {HotPar '10, 2nd USENIX Workshop on Hot Topics in Parallelism},
keywords = {deterministic parallel computing,many-core computing,patterns,software engineering,structured programming},
pages = {14--15},
title = {{Structured parallel programming with deterministic patterns}},
url = {https://www.usenix.org/event/hotpar10/tech/full{\_}papers/McCool.pdf https://www.usenix.org/legacy/event/hotpar10/tech/full{\_}papers/McCool.pdf$\backslash$nhttp://software.intel.com/en-us/articles/structured-parallel-programming-with-deterministic-patterns},
year = {2010}
}
@article{Amini2012,
author = {Amini, M},
title = {{Transformations de programme automatiques et source-{\`{a}}-source pour acc{\'{e}}l{\'{e}}rateurs mat{\'{e}}riels de type GPU}},
url = {http://hal.archives-ouvertes.fr/pastel-00958033/},
year = {2012}
}
@article{Petit2009,
author = {Petit, E},
title = {{Vers un partitionnement automatique d'applications en codelets sp{\'{e}}culatifs pour les syst{\`{e}}mes h{\'{e}}t{\'{e}}rog{\`{e}}nes {\`{a}} m{\'{e}}moires distribu{\'{e}}es}},
url = {http://hal.inria.fr/tel-00445512/},
year = {2009}
}
@article{Amarasinghe1994,
author = {Amarasinghe, SP and Anderson, JM},
journal = {{\ldots}  and Compilers for Parallel {\ldots}},
title = {{An overview of a compiler for scalable parallel machines}},
url = {http://link.springer.com/chapter/10.1007/3-540-57659-2{\_}15},
year = {1994}
}
@article{Amarasinghe1995,
author = {Amarasinghe, SP and Anderson, JAM and Lam, MS and Tseng, CW},
file = {:home/etn/Documents/PhD/Biblio/Amarasinghe et al. - 1995 - An Overview of the SUIF Compiler for Scalable Parallel Machines.pdf:pdf},
journal = {PPSC},
title = {{An Overview of the SUIF Compiler for Scalable Parallel Machines.}},
url = {http://www.researchgate.net/publication/2646829{\_}Chapter{\_}1{\_}An{\_}Overview{\_}of{\_}the{\_}SUIF{\_}Compiler{\_}for{\_}Scalable{\_}Parallel{\_}Machines/file/60b7d5193bc4fef52b.pdf},
year = {1995}
}
@article{Buck2004,
author = {Buck, I and Foley, T and Horn, D},
file = {:home/etn/Documents/PhD/Biblio/Buck, Foley, Horn - 2004 - Brook for GPUs stream computing on graphics hardware.pdf:pdf},
journal = {{\ldots}  on Graphics (TOG)},
title = {{Brook for GPUs: stream computing on graphics hardware}},
url = {http://dl.acm.org/citation.cfm?id=1015800},
year = {2004}
}
@article{Mark2003,
author = {Mark, WR and Glanville, RS},
file = {:home/etn/Documents/PhD/Biblio/Mark, Glanville - 2003 - Cg A system for programming graphics hardware in a C-like language.pdf:pdf},
journal = {{\ldots}  Transactions on Graphics ( {\ldots}},
title = {{Cg: A system for programming graphics hardware in a C-like language}},
url = {http://dl.acm.org/citation.cfm?id=882362},
year = {2003}
}
@article{Rinard1996,
author = {Rinard, MC and Diniz, PC},
file = {:home/etn/Documents/PhD/Biblio/Rinard, Diniz - 1996 - Commutativity analysis A new analysis framework for parallelizing compilers.pdf:pdf},
journal = {ACM SIGPLAN Notices},
title = {{Commutativity analysis: A new analysis framework for parallelizing compilers}},
url = {http://dl.acm.org/citation.cfm?id=231390},
year = {1996}
}
@article{Fernandez2014a,
author = {Fernandez, Raul Castro and Migliavacca, Matteo and Kalyvianaki, Evangelia and Pietzuch, Peter},
file = {:home/etn/Documents/PhD/Biblio/Fernandez et al. - 2014 - Making state explicit for imperative big data processing.pdf:pdf},
journal = {USENIX ATC},
title = {{Making state explicit for imperative big data processing}},
url = {https://www.usenix.org/system/files/conference/atc14/atc14-paper-castro{\_}fernandez.pdf},
year = {2014}
}
@article{Suleman2009,
author = {Suleman, MA and Mutlu, O},
file = {:home/etn/Documents/PhD/Biblio/Suleman, Mutlu - 2009 - Accelerating critical section execution with asymmetric multi-core architectures.pdf:pdf},
journal = {ACM SIGARCH Computer  {\ldots}},
title = {{Accelerating critical section execution with asymmetric multi-core architectures}},
url = {http://dl.acm.org/citation.cfm?id=1508274},
year = {2009}
}
@article{Radoi2014,
author = {Radoi, C and Fink, SJ and Rabbah, R and Sridharan, M},
file = {:home/etn/Documents/PhD/Biblio/Radoi et al. - 2014 - Translating imperative code to MapReduce.pdf:pdf},
journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
title = {{Translating imperative code to MapReduce}},
url = {http://dl.acm.org/citation.cfm?id=2660228},
year = {2014}
}
@book{Banerjee2013,
author = {Banerjee, U},
title = {{Loop parallelization}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=9xXrBwAAQBAJ{\&}oi=fnd{\&}pg=PR13{\&}dq=loop+parallelization{\&}ots=flLbnvNcCM{\&}sig=a1VFNFLS3{\_}TRjBVdpPr8mrH-0DE},
year = {2013}
}
@article{Matsakis2012a,
abstract = {This paper presents a lightweight task framework and accompanying type system that statically guarantee deterministic execution. The framework is based on the familiar model of fork-join parallelism, but with two important twists. First, child tasks do not begin execution immediately upon creation, but rather they are both scheduled and joined as one atomic action; this change prevents the parent task from racing with its children. Second, the body of a child task is specified as a parallel closure. Parallel closures are a novel variation on traditional closures in which the data inherited from the environment is read-only. Parallel closures have the important property that they can be executed in parallel with one another without creating data races, even if they share the same environment. We also have a controlled means to grant mutable access to data in the environment where necessary. We have implemented a prototype of our framework in Java. The prototype includes a typechecker that enforces the constraint that parallel closures cannot modify their environment. The paper describes how the prototype has been used to implement a number of realistic examples and also explains how parallel closures can support the creation of structured parallel programming abstractions.},
author = {Matsakis, Nicholas D},
file = {:home/etn/Documents/PhD/Biblio/Matsakis - 2012 - Parallel Closures A new twist on an old idea.pdf:pdf},
journal = {HotPar'12 Proceedings of the 4th USENIX conference on Hot Topics in Parallelism},
pages = {5--5},
title = {{Parallel Closures A new twist on an old idea}},
url = {http://dl.acm.org/citation.cfm?id=2342793 https://www.usenix.org/system/files/conference/hotpar12/hotpar12-final5.pdf},
year = {2012}
}
@article{Jones2011,
author = {Hoffman, Karl Heinz and Meyer, Arnd},
title = {{Parallel Algorithms and Cluster Computing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.2094{\&}rep=rep1{\&}type=pdf{\#}page=9},
year = {2006}
}
@inproceedings{Kamruzzaman2013,
address = {New York, New York, USA},
author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on - SC '13},
doi = {10.1145/2503210.2503295},
file = {:home/etn/Documents/PhD/Biblio/Kamruzzaman, Swanson, Tullsen - 2013 - Load-balanced pipeline parallelism.pdf:pdf},
isbn = {9781450323789},
keywords = {Instruction sets,Load management,Pipeline processing,Pipelines,Synchronization,chip multiprocessors,compiler-based technique,compilers,data parallel fashion,inter-thread communication,load-balanced pipeline parallelism,load-balancing,locality,multiprocessing systems,parallel processing,parallel systems,pipeline parallelism,pipeline processing,pipeline stage automatic extraction,program compilers,resource allocation,sequential stages,sequential threads,single thread acceleration,synchronisation,token-based chunked synchronization},
language = {English},
pages = {1--12},
publisher = {ACM Press},
title = {{Load-balanced pipeline parallelism}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6877447},
year = {2013}
}
@inproceedings{Nugteren2012,
abstract = {Recent advances in multi-core and many-core processors re- quires programmers to exploit an increasing amount of par- allelism from their applications. Data parallel languages such as CUDA and OpenCL make it possible to take ad- vantage of such processors, but still require a large amount of effort from programmers. A number of parallelizing source-to-source compilers have recently been developed to ease programming of multi-core and many-core processors. This work presents and evalu- ates a number of such tools, focused in particular on C-to- CUDA transformations targeting GPUs. We compare these tools both qualitatively and quantitatively to each other and identify their strengths and weaknesses. In this paper, we address the weaknesses by presenting a new classification of algorithms. This classification is used in a new source-to-source compiler, which is based on the algo- rithmic skeletons technique. The compiler generates target code based on skeletons of parallel structures, which can be seen as parameterisable library implementations for a set of algorithm classes. We furthermore demonstrate that the presented compiler requires little modifications to the original sequential source code, generates readable code for further fine-tuning, and delivers superior performance compared to other tools for a set of 8 image processing kernels.},
address = {New York, New York, USA},
author = {Nugteren, Cedric and Corporaal, Henk},
booktitle = {Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units},
doi = {10.1145/2159430.2159431},
file = {:home/etn/Documents/PhD/Biblio/Nugteren, Corporaal - 2012 - Introducing 'Bones' a parallelizing source-to-source compiler based on algorithmic skeletons.pdf:pdf},
isbn = {9781450312332},
keywords = {Graphics Processing Units,Parallel Programming,Source-to-Source Compilation,algorithmic skeletons},
month = {mar},
pages = {1--10},
publisher = {ACM Press},
title = {{Introducing 'Bones': a parallelizing source-to-source compiler based on algorithmic skeletons}},
url = {http://dl.acm.org/citation.cfm?id=2159430.2159431 http://dl.acm.org/citation.cfm?id=2159431},
year = {2012}
}
@inproceedings{Hall1995,
address = {New York, New York, USA},
author = {Hall, Mary H. and Amarasinghe, Saman P. and Murphy, Brian R. and Liao, Shih-Wei and Lam, Monica S.},
booktitle = {Proceedings of the 1995 ACM/IEEE conference on Supercomputing (CDROM) - Supercomputing '95},
doi = {10.1145/224170.224337},
file = {:home/etn/Documents/PhD/Biblio/Hall et al. - 1995 - Detecting coarse-grain parallelism using an interprocedural parallelizing compiler.pdf:pdf},
isbn = {0897918169},
keywords = {compiler optimizations,interprocedural data-flow analysis,parallelizing compilers,shared memory multiprocessors},
month = {dec},
pages = {49--es},
publisher = {ACM Press},
title = {{Detecting coarse-grain parallelism using an interprocedural parallelizing compiler}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=224337{\&}type=html},
year = {1995}
}
@article{Beck1991,
author = {Beck, Micah and Johnson, Richard and Pingali, Keshav},
doi = {10.1016/0743-7315(91)90016-3},
file = {:home/etn/Documents/PhD/Biblio/Beck, Johnson, Pingali - 1991 - From control flow to dataflow.ps$\backslash$;jsessionid=D4194E1CBD3276E264A2D1FD1507F780:ps;jsessionid=D4194E1CBD3276E264A2D1FD1507F780},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
number = {2},
pages = {118--129},
title = {{From control flow to dataflow}},
volume = {12},
year = {1991}
}
@article{Li2012,
abstract = {This article presents a general algorithm for transforming sequential imperative programs into parallel data-flow programs. The algorithm operates on a program dependence graph in static-single-assignment form, extracting task, pipeline, and data parallelism from arbitrary control flow, and coarsening its granularity using a generalized form of typed fusion. A prototype based on GNU Compiler Collection (GCC) is applied to the automatic parallelization of recursive C programs.},
author = {Li, Feng and Pop, Antoniu and Cohen, Albert},
doi = {10.1109/MM.2012.49},
file = {:home/etn/Documents/PhD/Biblio/Li, Pop, Cohen - 2012 - Automatic Extraction of Coarse-Grained Data-Flow Threads from Imperative Programs.pdf:pdf},
issn = {0272-1732},
journal = {IEEE Micro},
keywords = {Instruction sets,Parallel processing,Pipeline processing,Radiation detectors,SSA form,Sequential analysis,Synchronization,automatic parallelization,data-flow model,loop fusion,program dependence graph,sequential imperative programs},
language = {English},
month = {jul},
number = {4},
pages = {19--31},
publisher = {IEEE Computer Society},
title = {{Automatic Extraction of Coarse-Grained Data-Flow Threads from Imperative Programs}},
url = {http://www.computer.org/csdl/mags/mi/2012/04/mmi2012040019.html},
volume = {32},
year = {2012}
}
@inproceedings{Vandierendonck2010a,
abstract = {Speeding up sequential programs on multicores is a challenging problem that is in urgent need of a solution. Automatic paral- lelization of irregular pointer-intensive codes, exemplified by the SPECint codes, is a very hard problem. This paper shows that, with a helping hand, such auto-parallelization is possible and fruitful. This paper makes the following contributions: (i) A compiler- framework for extracting pipeline-like parallelism from outer pro- gram loops is presented. (ii) Using a light-weight programming model based on annotations, the programmer helps the compiler to find thread-level parallelism. Each of the annotations specifies only a small piece of semantic information that compiler analy- sis misses, e.g. stating that a variable is dead at a certain program point. The annotations are designed such that correctness is eas- ily verified. Furthermore, we present a tool for suggesting annota- tions to the programmer. (iii) The methodology is applied to auto- parallelize several SPECint benchmarks. For the benchmark with most parallelism (hmmer), we obtain a scalable 7-fold speedup on an AMD quad-core dual processor. The annotations constitute a parallel programming model that relies extensively on a sequential program representation. Hereby, the complexity of debugging is not increased and it does not ob- scure the source code. These properties could prove valuable to increase the efficiency of parallel programming.},
address = {New York, New York, USA},
author = {Vandierendonck, Hans and Rul, Sean and {De Bosschere}, Koen},
booktitle = {Proceedings of the 19th international conference on Parallel architectures and compilation techniques},
doi = {10.1145/1854273.1854322},
file = {:home/etn/Documents/PhD/Biblio/Vandierendonck, Rul, De Bosschere - 2010 - The Paralax infrastructure.pdf:pdf},
isbn = {9781450301787},
keywords = {Semi-automatic parallelization,semantic annotations},
month = {sep},
pages = {389--399},
publisher = {ACM Press},
title = {{The Paralax infrastructure: automatic parallelization with a helping hand}},
url = {http://dl.acm.org/citation.cfm?id=1854273.1854322 http://portal.acm.org/citation.cfm?id=1854322},
year = {2010}
}
@inproceedings{Alvaro2014,
author = {Alvaro, Peter and Conway, Neil and Hellerstein, Joseph M. and Maier, David},
booktitle = {2014 IEEE 30th International Conference on Data Engineering},
doi = {10.1109/ICDE.2014.6816639},
file = {:home/etn/Documents/PhD/Biblio/Alvaro et al. - 2014 - Blazes Coordination analysis for distributed programs.pdf:pdf},
isbn = {978-1-4799-2555-1},
keywords = {BLAZES program,Bloom declarative language,Fault tolerance,Fault tolerant systems,Semantics,Servers,Storms,Topology,Twitter,Twitter Storm system,annotated programs,application-specific coordination code synthesis,coordination protocols,cross-platform program analysis framework,distributed consistency,distributed processing,distributed programs,distributed systems,program diagnostics,program location identification,scalable distributed architectures},
language = {English},
month = {mar},
pages = {52--63},
publisher = {IEEE},
title = {{Blazes: Coordination analysis for distributed programs}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6816639},
year = {2014}
}
@article{Rajopadhye2006,
abstract = {We present optimization techniques for high level equational programs that are generalizations of affine control loops (ACLs). Significant parts of the SpecFP and PerfectClub benchmarks are ACLs. They often contain reductions: associative and commutative operators applied to a collection of values. They also often exhibit reuse: intermediate values computed or used at different index points being identical. We develop various techniques to automatically exploit reuse to simplify the computational complexity of evaluating reductions. Finally, we present an algorithm for the optimal application of such simplifications resulting in an equivalent specification with minimum complexity.},
author = {Gautam and Rajopadhye, S.},
doi = {10.1145/1111320.1111041},
isbn = {1595930272},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {1,equational programming,i,incremen-,k,loop optimization,motivating example and introduction,polyhedral model,program transformation,s,tal computation},
month = {jan},
number = {1},
pages = {30--41},
publisher = {ACM},
title = {{Simplifying reductions}},
url = {http://dl.acm.org/citation.cfm?id=1111320.1111041},
volume = {41},
year = {2006}
}
@phdthesis{Mendis2015,
abstract = {Highly optimized programs are prone to bit rot, where performance quickly becomes sub- optimal in the face of new hardware and compiler techniques. In this paper we show how to automatically lift performance-critical stencil kernels from a stripped x86 binary and generate the corresponding code in the high-level domain-specific language Halide. Using Halide's state-of-the-art optimizations targeting current hardware, we show that new opti- mized versions of these kernels can replace the originals to rejuvenate the application for newer hardware. The original optimized code for kernels in stripped binaries is nearly impossible to analyze statically. Instead, we rely on dynamic traces to regenerate the kernels. We perform buffer structure reconstruction to identify input, intermediate and output buffer shapes. We ab- stract from a forest of concrete dependency trees which contain absolute memory addresses to symbolic trees suitable for high-level code generation. This is done by canonicalizing trees, clustering them based on structure, inferring higher-dimensional buffer accesses and finally by solving a set of linear equations based on buffer accesses to lift them up to simple, high-level expressions. Helium can handle highly optimized, complex stencil kernels with input-dependent con- ditionals. We lift seven kernels from Adobe Photoshop giving a 75{\%} performance improve- ment, four kernels from IrfanView, leading to 4.97× performance, and one stencil from the miniGMG multigrid benchmark netting a 4.25× improvement in performance. We manually rejuvenated Photoshop by replacing eleven of Photoshop's filters with our lifted implemen- tations, giving 1.12× speedup without affecting the user experience.},
author = {Mendis, Charith and Bosboom, Jeffrey and Wu, Kevin and Kamil, Shoaib and Ragan-kelley, Jonathan and Paris, Sylvain and Zhao, Qin and Amarasinghe, Saman and Mendis, Thirimadura Charith Yasendra},
file = {:home/etn/Documents/PhD/Biblio/Mendis et al. - 2015 - Helium Lifting High-Performance Stencil Kernels from Stripped x86 Binaries to Halide DSL Code.pdf:pdf},
isbn = {9781450334686},
keywords = {autotuning,binary instrumentation,dynamic analysis,helium,image processing,reverse engineering,stencil com-,x86},
pages = {100},
title = {{Helium: Lifting High-Performance Stencil Kernels from Stripped x86 Binaries to Halide DSL Code}},
url = {https://groups.csail.mit.edu/commit/papers/2015/mendis-pldi15-helium.pdf},
year = {2015}
}
@article{Yuki2013,
author = {Yuki, Tomofumi and Gupta, Gautam and Kim, Daegon and Pathan, Tanveer and Rajopadhye, Sanjay},
doi = {10.1007/978-3-642-37658-0_2},
file = {:home/etn/Documents/PhD/Biblio/Yuki, Gupta, Kim - 2013 - Alphaz A system for design space exploration in the polyhedral model.pdf:pdf},
isbn = {9783642376573},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {17--31},
title = {{AlphaZ: A system for design space exploration in the polyhedral model}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37658-0{\_}2},
volume = {7760 LNCS},
year = {2013}
}
@phdthesis{Mauras1989,
author = {Mauras, Christophe},
keywords = {en fran{\c{c}}ais},
month = {jan},
publisher = {Rennes 1},
title = {{Alpha : un langage equationnel pour la conception et la programmation d'architectures paralleles synchrones}},
url = {http://www.theses.fr/1989REN10116},
year = {1989}
}
@article{Herrmann2000,
author = {Herrmann, CA},
file = {:home/etn/Documents/PhD/Biblio/Herrmann - 2000 - The skeleton based parallelization of divide and conquer recursions.ps:ps},
title = {{The skeleton based parallelization of divide and conquer recursions}},
url = {https://scholar.google.com/scholar?q=The Skeleton-Based Parallelization of Divide-and-Conquer Recursions{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}0},
year = {2000}
}
@article{Harrison1989,
author = {Harrison, Williams Ludwell},
doi = {10.1007/BF01808954},
issn = {0892-4635},
journal = {Lisp and Symbolic Computation},
month = {oct},
number = {3-4},
pages = {179--396},
title = {{The interprocedural analysis and automatic parallelization of Scheme programs}},
url = {http://link.springer.com/10.1007/BF01808954},
volume = {2},
year = {1989}
}
@phdthesis{Nicolay2010,
author = {Nicolay, Jens},
booktitle = {Citeseer},
file = {:home/etn/Documents/PhD/Biblio/Nicolay - 2010 - Automatic Parallelization of Scheme Programs using Static Analysis.pdf:pdf},
title = {{Automatic Parallelization of Scheme Programs using Static Analysis}},
url = {http://prog.vub.ac.be/{~}cderoove/publications/master{\_}thesis{\_}jens{\_}nicolay.pdf},
year = {2010}
}
@article{Nvidia2007,
author = {Nvidia, C},
title = {{Compute unified device architecture programming guide}},
url = {https://scholar.google.com/scholar?q=CUDA{\&}btnG=Search{\&}as{\_}sdt=800000000001{\&}as{\_}sdtp=on{\#}1},
year = {2007}
}
@article{Dagum1998,
abstract = {At its most elemental level, OpenMP is a set of compiler$\backslash$ndirectives and callable runtime library routines that extend Fortran$\backslash$n(and separately, C and C++ to express shared memory parallelism. It$\backslash$nleaves the base language unspecified, and vendors can implement OpenMP$\backslash$nin any Fortran compiler. Naturally, to support pointers and$\backslash$nallocatables, Fortran 90 and Fortran 95 require the OpenMP$\backslash$nimplementation to include additional semantics over Fortran 77. OpenMP$\backslash$nleverages many of the X3H5 concepts while extending them to support$\backslash$ncoarse grain parallelism. The standard also includes a callable runtime$\backslash$nlibrary with accompanying environment variables},
author = {Dagum, L. and Menon, R.},
doi = {10.1109/99.660313},
isbn = {1070-9924 VO - 5},
issn = {1070-9924},
journal = {IEEE Computational Science and Engineering},
keywords = {ANSI standards,Coherence,Computer architecture,Fortran,Fortran 90,Fortran 95,Fortran compiler,Hardware,Message passing,OpenMP,Parallel processing,Parallel programming,Power system modeling,Scalability,Software systems,X3H5 concepts,allocatables,application program interfaces,callable runtime library,callable runtime library routines,coarse grain parallelism,compiler directives,environment variables,industry standard API,parallel programming,pointers,shared memory parallelism,shared memory programming,shared memory systems,software portability,software reviews,software standards},
language = {English},
number = {1},
pages = {46--55},
publisher = {IEEE},
title = {{OpenMP: an industry standard API for shared-memory programming}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=660313},
volume = {5},
year = {1998}
}
@article{Kuper2015,
author = {Kuper, Lindsey},
title = {{Prospect: Finding and Exploiting Parallelism in a Productivity Language for Scientific Computing - SPLASH 2015}},
url = {http://2015.splashcon.org/event/splash2015-splash-i-lindsey-kuper-talk},
year = {2015}
}
@article{Grosser2011,
abstract = {Various powerful polyhedral techniques exist to optimize computation intensive programs effectively. Applying these techniques on any non-trivial program is still surprisingly difficult and often not as effective as expected. Most polyhedral tools are limited to a specific programming language. Even for this language, relevant code needs to match specific syntax that rarely appears in existing code. It is therefore hard or even impossible to process existing programs automatically. In addition, most tools target C or OpenCL code, which prevents effective communication with compiler internal optimizers. As a result target architecture specific optimizations are either little effective or not approached at all. In this paper we present Polly, a project to enable polyhedral optimizations in LLVM. Polly automatically detects and transforms relevant programparts in a language-independent and syntactically transparent way. Therefore, it supports programs written in most common programming languages and constructs like C++ iterators, goto based loops and pointer arithmetic. Internally it provides a state-of-the-art polyhedral library with full support for Z-polyhedra, ad- vanced data dependency analysis and support for external optimizers. Polly includes integrated SIMD and OpenMP code generation. Through LLVM, machine code for CPUs and GPU accelerators, C source code and even hardware descriptions can be targeted.},
author = {Grosser, Tobias and Zheng, Hongbin and Aloor, Raghesh and Simb{\"{u}}rger, Andreas and Gr{\"{o}}{\ss}linger, Armin and Pouchet, Louis-No{\"{e}}l},
journal = {Proceedings of the First International Workshop on Polyhedral Compilation Techniques (IMPACT '11)},
pages = {None},
title = {{Polly - Polyhedral optimization in LLVM}},
url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-07.pdf},
year = {2011}
}
@misc{Trifunovic2010,
abstract = {Modern compilers are responsible for adapting the semantics of source programs into a form that makes efficient use of a highly complex, heterogeneous machine. This adaptation amounts to solve an optimization problem in a huge and unstructured search space, while predicting the performance outcome of complex sequences of program transformations. The polyhedral model of compilation is aimed at these challenges. Its geometrical, non-inductive semantics enables the construction of better-structured optimization problems and precise analytical models. Recent work demonstrated the scalability of the main polyhedral algorithms to real-world programs. Its integration into production compilers is under way, pioneered by the Graphite branch of the GNU Compiler Collection (GCC). Two years after the effective beginning of the project, this paper reports on original questions and innovative solutions that arose during the design and implementation of Graphite.},
author = {Trifunovic, Konrad and Cohen, Albert and Edelsohn, David and Li, Feng and Grosser, Tobias and Jagasia, Harsha and Ladelsky, Razya and Pop, Sebastian and Sj{\"{o}}din, Jan and Upadrasta, Ramakrishna},
booktitle = {GCC Research Opportunities Workshop (GROW'10)},
file = {:home/etn/Documents/PhD/Biblio/Trifunovic et al. - 2010 - GRAPHITE Two Years After First Lessons Learned From Real-World Polyhedral Compilation.pdf:pdf},
language = {en},
month = {jan},
title = {{GRAPHITE Two Years After: First Lessons Learned From Real-World Polyhedral Compilation}},
url = {https://hal.inria.fr/inria-00551516/},
year = {2010}
}
@article{Bastoul2004,
abstract = {Abstract. We seek to extend the scope and efficiency of iterative com- pilation techniques by searching not only for program transformation parameters but for the most appropriate transformations themselves. For that purpose, we need a generic way to express program transforma- ...},
address = {Berlin, Heidelberg},
author = {Bastoul, C{\'{e}}dric and Cohen, Albert and Girbal, Sylvain and Sharma, Saurabh and Temam, Olivier},
doi = {10.1007/b95707},
editor = {Rauchwerger, Lawrence},
isbn = {978-3-540-21199-0},
issn = {978-3-540-21199-0},
journal = {LCPC '04 Languages and Compilers for Parallel Computing},
number = {Chapter 14},
pages = {209--225},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Putting Polyhedral Loop Transformations to Work}},
url = {http://link.springer.com/10.1007/b95707 http://www.springerlink.com/index/10.1007/978-3-540-24644-2{\_}14$\backslash$npapers2://publication/doi/10.1007/978-3-540-24644-2{\_}14},
volume = {2958},
year = {2004}
}
@article{Ragan-Kelley2013,
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because},
author = {Ragan-Kelley, Jonathan and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'{e}}do and Barnes, Connelly and Amarasinghe, Saman},
doi = {10.1145/2491956.2462176},
file = {:home/etn/Documents/PhD/Biblio/Ragan-Kelley et al. - 2013 - Halide A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing.pdf:pdf},
isbn = {978-1-4503-2014-6},
issn = {03621340},
journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
keywords = {autotuning,compiler,domain specific language,gpu,image processing,locality,optimization,parallelism,redundant computation,vectorization},
pages = {519--530},
title = {{Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines}},
url = {http://people.csail.mit.edu/jrk/halide-pldi13.pdf http://doi.acm.org/10.1145/2491956.2462176},
year = {2013}
}
@article{Chen2008,
abstract = {This paper describes a general and robust loop transformation framework that enables compilers to generate efficient code on complex loop nests. Despite two decades of prior research on loop optimization, performance of compiler-generated code often falls short of manually optimized versions, even for some well-studied BLAS kernels. There are two primary reasons for this. First, today's compilers employ fixed transformation strategies, making it difficult to adapt to different optimization requirements for different application codes. Second, code transformations are treated in isolation, not taking into account the interactions between different transformations. This paper addresses such limitations in a unified framework that supports a broad collection of transformations, (permutation, tiling, unroll-and-jam, data copying, iteration space splitting, fusion, distribution and others), which go beyond existing polyhedral transformation models. This framework is a key element of a compiler we are developing which performs empirical op- timization to evaluate a collection of alternative optimized variants of a code segment. A script interface to code generation and empirical search permits transformation parameters to be adjusted independently and tested; alterna- tive scripts are used to represent different code variants. By applying this framework to example codes,we showperformance results on automatically- generated code for the PentiumM and MIPS R10000 that are comparable to the best hand-tuned codes, and significantly better (up to a 14x speedup) than the native compilers.},
author = {Chen, Chun and Chame, Jacqueline and Hall, Mary},
doi = {10.1001/archneur.64.6.785},
file = {:home/etn/Documents/PhD/Biblio/Chen, Chame, Hall - 2008 - CHiLL A framework for composing high-level loop transformations.pdf:pdf},
issn = {00039942},
journal = {U. of Southern California, Tech. Rep},
pages = {1--28},
pmid = {17562926},
title = {{CHiLL: A framework for composing high-level loop transformations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8396{\&}rep=rep1{\&}type=pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:CHiLL+:+A+Framework+for+Composing+High-Level+Loop+Transformations{\#}0},
year = {2008}
}
@article{Catanzaro2009,
abstract = {Parallel programming must be accessible to domain experts without requiring them to become experts in parallel hardware architecture. While domain experts today prefer to use high-level “productivity” scripting languages with domain-appropriate abstractions, achieving high performance still requires expertise in lower-level “efficiency” lan- guages (CUDA, CILK, C with OpenMP) that expose hardware-level programming models directly. We bridge this gap through the use of embedded just-in-time specialization: domain experts write in high-level scripting languages, but at runtime, we specialize (generate, compile, and execute efficiency-language source code for) an application-specific and platform-specific subset of the productivity language. This enables invisible and selective optimization of only those application-level abstractions that enjoy a large performance advantage when expressed in an efficiency lan- guage on the available hardware and will be executed many times, amortizing the overhead of specialization. Because the specialization machinery is implemented in the productivity language, efficiency programmers can easily extend our system by adding new specializers for specific additional domain abstractions or new hardware, transparently to the productivity-language programmers. Our approach results in competitive performance on real applications with a fraction of the programming effort on the part of the domain expert. We argue that the separation of concerns enabled by embedded JIT specialization allows research to proceed in parallel on both the productivity and efficiency layers, and is therefore uniquely suited to the problem of making different parallel hardware architectures more accessible to domain-expert programmers with a fraction of the programmer time and effort.},
author = {Catanzaro, Bryan and Kamil, Shoaib and Lee, Yunsup},
doi = {10.1.1.212.6088},
file = {:home/etn/Documents/PhD/Biblio/Catanzaro, Kamil, Lee - 2009 - SEJITS Getting productivity and performance with selective embedded JIT specialization.pdf:pdf},
isbn = {978-0-9825442-3-5},
journal = {Programming Models for Emerging Architectures},
pages = {1--10},
pmid = {8401072916273492945},
title = {{SEJITS: Getting productivity and performance with selective embedded JIT specialization}},
url = {http://www.eecs.berkeley.edu/{~}krste/papers/SEJITS-pmea2009.pdf http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1{\&}type=pdf{\&}doi=10.1.1.212.6088$\backslash$nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-23.pdf http://www.eecs.berkeley.edu/{~}krste/papers/SEJITS-pmea2009.pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1{\&}type=pdf{\&}doi=10.1.1.212.6088$\backslash$nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-23.pdf},
year = {2009}
}
@article{Lee2013,
abstract = {Pipeline parallelism organizes a parallel program as a linear sequence of s stages. Each stage processes elements of a data stream, passing each processed data element to the next stage, and then taking on a new element before the subsequent stages have necessarily completed their processing. Pipeline parallelism is used especially in streaming applications that perform video, audio, and digital signal processing. Three out of 13 benchmarks in PARSEC, a popular software benchmark suite designed for shared-memory multiprocessors, can be expressed as pipeline parallelism. Whereas most concurrency platforms that support pipeline parallelism use a “construct-and-run” approach, this paper investigates “on-the-fly” pipeline parallelism, where the structure of the pipeline emerges as the program executes rather than being specified a priori. On-the-fly pipeline parallelism allows the number of stages to vary from iteration to iteration and dependencies to be data dependent. We propose simple linguistics for specifying on-the-fly pipeline parallelism and describe a provably efficient scheduling algorithm, the P IPER algorithm, which integrates pipeline parallelism into a work-stealing scheduler, allowing pipeline and fork-join parallelism to be arbitrarily nested. The PIPER algorithm automatically throttles the parallelism, precluding “runaway” pipelines. Given a pipeline computation with T{\_}1 work and T{\_}infty span (critical-path length), P IPER executes the computation on P processors in T P ≤ T 1 /P + O(T ∞ + lg P) expected time. PIPER also limits stack space, ensuring that it does not grow unboundedly with running time. We have incorporated on-the-fly pipeline parallelism into a Cilk-based work-stealing runtime system. Our prototype Cilk-P implementation exploits optimizations such as lazy enabling and dependency folding. We have ported the three PARSEC benchmarks that exhibit pipeline parallelism to run on Cilk-P. One of these, x264, cannot readily be executed by systems that support only construct-and-run pipeline parallelism. Benchmark results indicate that Cilk-P has low serial overhead and good scalability. On x264, for example, Cilk-P exhibits a speedup of 13.87 over its respective serial counterpart when running on 16 processors.},
author = {Lee, I-Ting Angelina and Leiserson, Charles E. and Schardl, Tao B. and Sukha, Jim and Zhang, Zhunping},
doi = {10.1145/2486159.2486174},
file = {:home/etn/Documents/PhD/Biblio/Lee et al. - 2013 - On-the-fly pipeline parallelism.pdf:pdf},
isbn = {9781450315722},
journal = {Proceedings of the 25th ACM symposium on Parallelism in algorithms and architectures},
keywords = {cilk,multicore,multithreading,on-the-fly pipelining,parallel programming,parallelism,pipeline,scheduling,work stealing},
pages = {140},
title = {{On-the-fly pipeline parallelism}},
url = {http://supertech.csail.mit.edu/papers/spaa030-lee.pdf http://dl.acm.org/citation.cfm?doid=2486159.2486174},
year = {2013}
}
@article{Tarditi2006,
abstract = {GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls.We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50{\%} of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.},
author = {Tarditi, David and Puri, Sidd and Oglesby, Jose},
doi = {10.1145/1168918.1168898},
file = {:home/etn/Documents/PhD/Biblio/Tarditi, Puri, Oglesby - 2006 - Accelerator using data parallelism to program GPUs for general-purpose uses.pdf:pdf},
isbn = {1-59593-451-0},
issn = {03621340},
journal = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
keywords = {data parallelism,graphics processing units,just-in-time compilation},
month = {oct},
number = {5},
pages = {325--335},
publisher = {ACM},
title = {{Accelerator: using data parallelism to program GPUs for general-purpose uses}},
url = {http://dl.acm.org/citation.cfm?id=1168918.1168898 http://portal.acm.org/citation.cfm?id=1168898},
volume = {34},
year = {2006}
}
