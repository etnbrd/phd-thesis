\section{Highly concurrent web servers} \label{chapter2:highly-concurrent-web-servers}



\subsection{Concurrency}

The Internet allows communication at an unprecedented scale.
There is more than 16 billions connected devices, and it is growing fast\ftnt{http://blogs.cisco.com/news/cisco-connections-counter} \cite{Hilbert2011}.
% This massively interconnected network gives the ability for a web applications to be reached at the largest scale.
A large web application like google search receives about 40 000 requests per seconds\ftnt{http://www.internetlivestats.com/google-search-statistics/}.
Such a web application needs to be highly concurrent to manage this amount of simultaneous requests.
% Concurrency is the ability for an application to make progress on several tasks at the same time.
% For example to respond to several simultaneous requests, a task is a part in the response to a request. \nt{TOOD define more clearly what is a task}
%It represents an uninterrupted flow of requests, with a growing throughput.
In the 2000s, the limit to break was 10 thousands simultaneous connections with a single commodity machine\ftnt{http://www.kegel.com/c10k.html}.
Nowadays, in the 2010s, the limit is set at 10 millions simultaneous connections\ftnt{http://c10m.robertgraham.com/p/manifesto.html}.
With the growing number of connected devices on the internet, concurrency is a very important property in the design of web applications.
Moreover, the concurrency needs to be scalable to adapt to this growth of audience, as explained in the next paragraph.

\subsubsection{Scalability}

The traffic of a popular web application such as Google search remains stable because of its popularity.
The importance of the average traffic soften the occasional spikes.
%There is no apparent spikes in the traffic, because of the importance of the average traffic.
However, the traffic of a less popular web application is much more uncertain.
For example, it might become viral when it is efficiently relayed in the media.
% For example, when a web application appears in the evening news, it expects a huge spike in traffic.
% the number of simultaneous requests obviously  increases, and 
The load of the web application increases with the growth of audience.
The available resources needs to increase to meet this load.
This growth can be steady enough to plan the increase of resources ahead of time, or it might be erratic and challenging.
% The spikes of a less popular web application are unpredictable.
% Therefore, the concurrency needs to be expressed in a scalable fashion.
An application is scalable, if it is able to increasingly spread over resources to react to the increasing growth of audience.
And if the load on these resources is linearly proportional to the increasing growth, instead of exponentially proportional.
% For example, if a scalable application uses one resource to handle $n$ simultaneous requests, it will use $k$ resource to handle two times $n$ simultaneous requests.
% With $k$ being constant, for $n$ ranging from tens to millions of simultaneous requests.
% It assures that the load is increasing linearly, instead of exponentially.

\subsubsection{Time-slicing and Parallelism}

Concurrency is achieved differently on hardware with a single or several processing units.
On a single processing unit, the tasks are executed sequentially, interleaved in time, while on several processing units, the tasks are executed simultaneously, in parallel.
% Parallel executions reduce computing time over sequential execution, as it uses more processing units.
If the tasks are independent, they can be executed in parallel as well as sequentially.
This parallelism is scalable, as the independent tasks can stretch the computation on the resources so as to meet the required performance.
% This parallelism is used in operating system to execute several applications concurrently to allow multi-tasking.

However, the tasks within an application need to coordinate together to modify the application state.
This coordination limits the parallelism and impose to execute some tasks sequentially.
It limits the scalability.
The type of possible concurrency, sequential or parallel, is defined by the interdependencies of the tasks.
% Either the tasks are independents and they can be executed in parallel, or the tasks need to coordinate a common state, and they need to be executed sequentially to avoid conflicting accesses to the state.

% The same limitations are found in the two paradigms I study in this thesis : event-loop and multi-process.

\subsection{Interdependencies}

It is easy to understand the parallelism in a cooking recipe because the interdependencies between operations are trivial.
It seems obvious that melting chocolate is independent from whipping up egg whites.
% Because chocolate and egg whites are different ingredients.
This distinction between chocolate and egg whites is trivial.
% ... comes from the modifications to the state.
While the distinctions within the state of an application are more intricate.
This makes concurrent application more difficult to understand.

\subsubsection{State Coordination}

% The interdependencies between the tasks impose the coordination of the global application state.
The global state of an application impose the coordination between the tasks.
This coordination happens either by sending messages, or by modifying a shared memory.
If the tasks are independent enough, the coordinations can be done with message passing.
Each task sends messages to indicate the modifications of the state with consequences outside its scope.
% They pass the states from one task to another so as to always have an exclusive access on the state.
% As example, applications built around a pipeline architecture define independent tasks arranged to be executed one after the other.
% The tasks pass the result of their computation to the next.
% These tasks never share a state.
However, if the tasks are too dependent, the overhead of message passing tends to impact performances.
% If the tasks need concurrent accesses to a state, they cannot efficiently pass the state from one to the other repeatedly.
They need to share and coordinate their accesses to the state.
Each access needs to be exclusive to avoid corruption.
I address in the next paragraphs the different scheduling strategies, and how they assure this exclusivity.

\subsubsection{Task Scheduling}

There are two scheduling strategies to execute tasks sequentially on a single processing unit : preemptive scheduling and cooperative scheduling.
The coordination is different with the two scheduling strategies.

\illustration{feu rouge et rond point}

Preemptive scheduling is used to assure fairness between the tasks, such as in a multi-tasking operating system.
% in most execution environment in conjunction with multi-threading.
The scheduler allows a limited time of execution for each task, before preempting it.
% It is a fair and pessimistic scheduling, as it grant the same amount of computing time to each task.
However, as the preemption happens unexpectedly, the developer needs to assure exclusivity by locking the shared state before access.
Locking is known to be hard to manage by developers, and to impact performances, so it is set aside for the remaining of this chapter.
% This scheduling strategy should be avoided except when true concurrency is needed in concert with true shared state.
% Shared state could probably always be emulated with isolated memory and message passing.

On the other hand, in cooperative scheduling, a task is allowed to run until it yields the execution back to the scheduler.
Each task is an atomic execution : it has an exclusive access on the memory.
% It gives back to the developer the control over the preemption.
As the developer doesn't need to explicitly assure exclusivity, it is easier to write concurrent programs efficiently with this scheduling strategy.
% Indeed, I presented in the previous section the popularity of Javascript, which is often implemented on top of this scheduling strategy (DOM, Node.js).

\subsubsection{Invariance}

% The challenge introduced above is to assure to the developer an exclusive access to the state of its application.
In concurrent computation, it is important to assure the invariance of a state during its manipulation.
This assurance is given by the exclusive access of an atomic execution on the state.
% I call invariance the assurance given that the state accessible from a task will remain unchanged during its access to avoid corruption, and more generally to allow the developer to perform atomic modifications on the state.
It allows the developer to group operations inside this atomic execution, so as to avoid corruption of the state.
% so as to perform all the operations without interference from concurrent executions.
% The same concept is found in transactional memory.

When the tasks remains isolated and communicate by message passing, there is no risk of corrupted state.
The invariance is assured by the isolation of the state specified by the developer, and by the atomicity of the message processing.
On the other hand, in a cooperative scheduling application, each task is atomic, so the developer always has an exclusive access to the global state.
This atomicity assures the invariance.
% The invariance is assured by the atomicity of each task.
% , because any region in the memory can be accessed only by one task at a time.

% Between these two invariances, the locking mechanisms seems to be a promising compromise.
% The developer defines only the shared states, and these are locked only when needed.
% However, it increases the complexity of the possible locked combination, leading to unpredictable situations, such as deadlock, and so on.
% The locking mechanisms are known to be difficult to manage, and sub-optimal.
% Indeed, they are eventually as efficient as a queue to share resources.

% For the rest of this thesis, I focus only on the invariances provided by the multi-process paradigm and the cooperative scheduling.
% They two invariance are similar, because the developer defines sequence of instructions with atomic access to the memory.
% And in both paradigms, these sequences communicate by sending messages to each other.
The difference is that in the message passing paradigm, the developer defines the state isolation inducing the execution isolation, while with cooperative scheduling, the developer defines only the execution isolation.
% This difference seems to be crucial in the adoption by the developer community.
It is difficult for developer to isolate state, but it provides good performances through parallelism.
While it is easier to assure atomic execution with cooperative scheduling, but it is unable to provide parallelism.
On one hand, the state is distributed and isolated to improve performance scalability.
On the other hand, the code is organized logically to improve maintainability.
The impact of these two organizations on performance scalability and development scalability is at the heart of this thesis.

\subsection{Disrupted Development}

% \subsubsection{Power wall}

\subsubsection{Scalable Concurrency}

% Around 2004, the so-called Power Wall was reached.
% The clock of CPU is stuck at 3GHz because of the inability to dissipate the heat generated at higher frequencies.
% Additionally, the instruction-level parallelism is limited.

\illustration{heating chipset / parallel chipsets}

Around 2004, the speed of sequential execution on a processing unit plateaued\ftnt{https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/}.
Manufacturers started to arrange transistors into several processing units to keep increasing overall performance while avoiding overheating problems.
% Because of these limitations, a processor is limited in the number of instruction per second it can execute.
Therefore, the performance of the sequential execution required by the cooperative scheduling plateaued as well.
Isolating tasks is the only option to achieve high concurrency on this parallel hardware.
% Therefore, a coarser level of parallelism, like the task-level, multi-processes parallelism previously presented is the only option to achieve high concurrency and scalability.
But this isolation is in contradiction with the best practices of software development.
It implies a rupture between performance and maintainability.

\subsubsection{The Case for Modularity}

The best practices in software development advocate to gather features logically into distinct modules.
% The understanding of the whole application emerges from the interconnections between the different modules.
This modularity allows a developer to understand and contribute to an application one module at a time, instead of understanding the whole application.
It allows to develop and maintain a large code-base by a multitude of developers bringing small, independent contributions.


% Modularity advocates three principles : encapsulation, a module contains all the data, as well as the functions to manipulate this data ; separation of concerns, each module should have a clear scope of action, and this scope should not overlap with the scope of other modules ; and loose coupling, each module should require no, or as little knowledge as possible about the definition of other modules.
% The main goal followed by these principles, is to help the developer to develop and maintain a large code-base.

This modularity avoids a different problem than the isolation required by parallelism.
The former intends to structure code to improve maintainability, while the latter improve performance through parallel execution.
These two organizations are conflicting in the design of the application.
% Therefore, every language needs to provide a compromise between these two goals, and specialized in specific type of applications.
In the next paragraph, I argue this conflict disrupts the development of a web application.

% I argue that the more accessible, hence popular programming languages choose to provide modularity over isolation.
% They provide a global memory at the sacrifice of the performance provided by parallelism.
% On the other hand, the more efficient languages sacrifice the readability and maintainability, to provide a model closer to parallelism, to allow better performances.
% \nt{TODO instead of language, use a more generic term to refer to language or infrastructure}
% \nt{TODO justification and examples. What are modular application, or parallel applications ?}


\subsubsection{Technological shift}

Between the prototyping, and the maturation of a web application, the needs are radically different.
I argue that during the initiation of a web application project, the economical constraint holds on the pace of development.
The development reactivity is crucial to meet the market needs\ftnt{https://www.cbinsights.com/blog/startup-failure-post-mortem/}.
The development team opt for a popular and accessible language to leverage the advantage of its community.
It is only after a certain threshold of popularity that the  economical constraint on performance requirements exceed the one on development.
The development team shift to an organization providing parallelism.

% Between the early development, and the maturation of a web application, the development needs are radically different.
% In its early development, a web application needs to quickly iterate over feedback from its users.
% \textit{``Release early, release often''}, and \textit{``Fail fast''} are the punchlines of the web entrepreneurial community.
% The development team quickly releases a Minimum Viable Product as to get these feedbacks.
% The development reactivity is crucial.
% The first reason of startup failures is the lack of market need\ftnt{https://www.cbinsights.com/blog/startup-failure-post-mortem/}.
% Therefore, the development team opt for a popular, and accessible language.

% As the application matures and its audience grows, the focus shift from the development speed to the scalability of the application.
% % The ability of the application to handle a large amount of simultaneous request.
% The development team shift from a modular language, to a language providing parallelism.


This shift brings two risks.
The development team needs to rewrite the code base to adapt it to a completely different paradigm.
The application risks to fail because of this challenge.
% It is hard for the development team to find the time, hence the money, or the competences to deploy this new paradigm.
% Indeed, the number two and three reasons for startup failures are running out of cash, and missing the right competences.
And after this shift the development pace slow down.
% Parallel languages are incompatible with the commonly learned design principles.
The development team cannot react as quickly to user feedbacks to adapt the application to the market needs.
The application risks to fall in obsolescence.

% There is a performance problem with languages about concurrency.
% The used imperative programming languages are difficulty parallel.
% And the highly parallel programming languages are not largely used.

% There is a technological rupture between the two.

The risks implied by this rupture proves that there is economically a need for a solution that continuously follows the evolution of a web application.
We present in the next section the proposition of this thesis for such a solution.
It would allow developers to iterate continuously on the implementation focusing simultaneously on performance, and on maintainability.

% This technological rupture proves that there is economically a need for a more sustainable solution to follow the evolution of a web application.
% A paradigm that it is easy to develop with, as needed in the beginning of a web application development, and yet scalable, so as to be highly concurrent when the application matures.

% Such a paradigm would allow to develop with a global memory, so as to follow the best practices of software development, and be able to isolate the state of each concurrent tasks to parallelize them.

\endinput









