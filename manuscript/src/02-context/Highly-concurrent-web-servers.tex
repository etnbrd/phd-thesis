\section{Highly concurrent web servers} \label{chapter2:highly-concurrent-web-servers}

\nt{This section needs review}

\subsection{Concurrency}

The Internet allows interconnection at an unprecedented scale.
There is currently more than 16 billions devices connected to the internet, and it is growing exponentially\ftnt{http://blogs.cisco.com/news/cisco-connections-counter}.
This massively interconnected network gives the ability for a web applications to be reached at the largest scale.
A large web application like google search receives about 40 000 requests per seconds.
That is about 3.5 billions requests a day\ftnt{http://www.internetlivestats.com/google-search-statistics/}.
Such a web application needs to be highly concurrent to manage a large number of simultaneous request.
Concurrency is the ability for an application to make progress on several tasks at the same time.
For example to respond to several simultaneous requests, a task is a part in the response to a request. \nt{TOOD define more clearly what is a task}
%It represents an uninterrupted flow of requests, with a growing throughput.

In the 2000s, the limit to reach was to process 10 thousands simultaneous connections with a single commodity machine\ftnt{http://www.kegel.com/c10k.html}.
Nowadays, in the 2010s, the limit is set at 10 millions simultaneous connections at roughly the same price\ftnt{http://c10m.robertgraham.com/p/manifesto.html}.
With the growing number of connected devices on the internet, concurrency is a very important property in the design of web applications.

\subsubsection{Scalability}

The traffic of a popular web application such as Google search is huge, and it remains roughly stable because of this popularity.
There is no apparent spikes in the traffic, because of the importance of the average traffic.
However, the traffic of a less popular web application is much more uncertain.
If the web application fits the market need, it might become viral when it is efficiently relayed in the media.
For example, when a web application appears in the evening news, it expects a huge spike in traffic.
With the growth of audience, the number of simultaneous requests obviously  increases, and the load of the web application on the available resources increases  as well.
The available resources needs to increase to meet the load.
This growth might be steady and predictable enough to plan the increase of resources ahead of time, or it might be erratic and challenging.
The spikes of a less popular web application are unpredictable.
Therefore, the concurrency needs to be expressed in a scalable fashion.
An application is scalable, if the growth of its audience is proportional to the increase of its load on the resources.
For example, if a scalable application uses one resource to handle $n$ simultaneous requests, it will use $k$ resource to handle two times $n$ simultaneous requests.
With $k$ being constant, for $n$ ranging from tens to millions of simultaneous requests.
Scalability assures that the resource usage is not increasing exponentially in function of the audience increase ; it increases roughly linearly.


\subsubsection{Time-slicing and parallelism}

Concurrency can be achieved on hardware with either a single or several processing units.
On a single processing unit, the tasks are executed sequentially ; their executions are interleaved in time.
On several processing unit, the tasks are executed in parallel.
Parallel executions reduce computing time over sequential execution, as it uses more processing units.

If the tasks are completely independent, they can be executed in parallel as well as sequentially.
This parallelism is a form of scalable concurrency, as it allows to stretch the computation on available hardware to meet the required performance, at the required cost.
This parallelism is used in operating system to execute several applications concurrently to allow multi-tasking.

However, the tasks of an application are rarely independent.
The tasks need to coordinate their dependencies to modify the global state of the application.
This coordination limits the possible parallelism between the tasks, and might impose to execute them sequentially.
The type of possible concurrency, sequential or parallel, is defined by the required coordination between the tasks.
Either the tasks are independents and they can be executed in parallel, or the tasks need to coordinate a common state, and they need to be executed sequentially to avoid conflicting accesses to the state.

% The same limitations are found in the two paradigms I study in this thesis : event-loop and multi-process.

\subsection{Interdependencies}

It is easier to understand the possible parallelism of a cooking recipe than an application.
That is because the modifications to the state are trivial in the cooking recipe, hence the interdependencies between operations.
It is easy to understand that preheating the oven is independent from whipping up egg whites.
While the interdependencies are not immediately obvious in an application.
\nt{TODO is this metaphor useful here ? if yes, continue to a transition}

\subsubsection{State coordination}

The interdependencies between the tasks impose the coordination of the global application state.
This coordination happen either by sending events from one task to another, or by modifying a shared memory.

If the tasks are independent enough, they never need access to a state at the same time.
The coordination of the state of the application can be done with message passing.
They pass the states from one task to another so as to always have an exclusive access on the state.
As example, applications built around a pipeline architecture define independent tasks arranged to be executed one after the other.
The tasks pass the result of their computation to the next.
These tasks never share a state.

If the tasks need concurrent accesses to a state, they cannot efficiently pass the state from one to the other repeatedly.
They need to share and to coordinate their accesses to this state.
Each access needs to be exclusive to avoid corruption.
This exclusivity is assured differently depending on the scheduling strategy.

\subsubsection{Task scheduling}

There is roughly two main scheduling strategy to execute tasks sequentially on a single processing unit : preemptive scheduling and cooperative scheduling.
The state coordination presented previously is highly depending on the scheduling strategy.

Preemptive scheduling is used in most execution environment in conjunction with multi-threading.
The scheduler allows each task to execute for a limited time before preempting it to let another task execute.
It is a fair and pessimistic scheduling, as it grant the same amount of computing time to each task.
However, as the preemption might happen at any point in the execution, it is important for the developer to lock the shared state before access, so as to assure exclusivity.
This protection is known to be hard to manage.
% This scheduling strategy should be avoided except when true concurrency is needed in concert with true shared state.
% Shared state could probably always be emulated with isolated memory and message passing.

In cooperative scheduling, the scheduler allows a task to run until the task yield the execution back.
Each task is an atomic execution ; it is never be preempted, and have an exclusive access on the memory.
It gives back to the developer the control over the preemption.
It seems to be the easiest way for developers to write concurrent programs efficiently.
Indeed, I presented in the previous section the popularity of Javascript, which is often implemented on top of this scheduling strategy (DOM, Node.js).

As I explained the different paradigms for writing concurrent program in this subsection, it appears that the main problem is to assure to the developer for each task the exclusive access to the state of its application.
This assurance is called invariance.

\subsubsection{Invariance}

I call invariance the assurance given that the state accessible from a task will remain unchanged during its access to avoid corruption, and more generally to allow the developer to perform atomic modifications on the state.
This assurance allows the developer to regroup operations logically so as to perform all the operations without interference from concurrent executions.
The same concept is found in transactional memory.

In a multi-process application, there is no risk of corrupted state by simultaneous, conflicting accesses.
The invariance is made explicit by the developer as the memory needs to be isolated inside each process.
The invariance is assured at any point in time because the process remains isolated.

In a cooperative scheduling application, the developer is aware of the points in the code where the scheduler switches from one concurrent execution to the other, so it can manage its state in atomic modification.
The invariance is assured, because any region in the memory can be accessed only by one task at a time.

Between these two invariances, the locking mechanisms seems to be a promising compromise.
The developer defines only the shared states, and these are locked only when needed.
However, it increases the complexity of the possible locked combination, leading to unpredictable situations, such as deadlock, and so on.
The locking mechanisms are known to be difficult to manage, and sub-optimal.
Indeed, they are eventually as efficient as a queue to share resources.

For the rest of this thesis, I focus only on the invariances provided by the multi-process paradigm and the cooperative scheduling.
\nt{In the state of the art, we probably cannot reduce the analyze to these two paradigms.}
They are similar, because the developer defines sequence of instructions with atomic access to the memory.
And in both paradigms, these sequences communicate by sending messages to each other.
The difference is that in the multi-process paradigm, the developer defines the region and the isolated memory, while in the cooperative scheduling, the developer defines only the region, and the memory is isolated by the exclusivity in the execution.

This difference seems to be crucial in the adoption of the technology by the developer community.
As we will see in the next subsection, the parallelism of multi-process is difficult to develop, but provides good performances, while the sequentiality of the cooperative scheduling is easier to develop, but provides poor performances compared to parallelism.


\nt{TODO this paragraph needs review}

\subsection{Disrupted development}

% \subsubsection{Power wall}

\subsubsection{Scalable concurrency}

Around 2004, the so-called Power Wall was reached.
The clock of CPU is stuck at 3GHz because of the inability to dissipate the heat generated at higher frequencies.
Additionally, the instruction-level parallelism is limited.
Because of these limitations, a processor is limited in the number of instruction per second it can execute.
Therefore, a coarser level of parallelism, like the task-level, multi-processes parallelism previously presented is the only option to achieve high concurrency and scalability.
But as I presented previously, this parallelism requires the isolation of the memory of each independent task.
This isolation is in contradiction with the best practices of software development, hence, is difficult to develop for common developers.
It creates a rupture between performance and development accessibility.

\subsubsection{The case for global memory}

The best practices in software development advocate to design a software into isolated modules.
This modularity allows to understand each module by itself, without an understanding of the whole application.
The understanding of the whole application emerges from the interconnections between the different modules.
A developer need only to understand a few modules to contribute to an application of hundreds or thousands of modules.

Modularity advocates three principles : encapsulation, a module contains the data, as well as the functions to manipulate this data ; separation of concerns, each module should have a clear scope of action, and this scope should not overlap with the scope of other modules ; and loose coupling, each module should require no, or as little knowledge as possible about the definition of other modules.
The main goal followed by these principles, is to help the developer to develop and maintain a large code-base.

Modularity is intended to avoid a different problem than the isolation required by parallelism.
The former intends to avoid unintelligible spaghetti code ; while the latter avoids conflicting memory accesses resulting in corrupted state.
The two goals are overlapping in the design of the application.
\nt{TODO needs more explanations -> so it is hard for dev to do both ? Why exactly ?}
Therefore, every language needs to provide a compromise between these two goals, and specialized in specific type of applications.
I argue that the more accessible, hence popular programming languages choose to provide modularity over isolation.
They provide a global memory at the sacrifice of the performance provided by parallelism.
On the other hand, the more efficient languages sacrifice the readability and maintainability, to provide a model closer to parallelism, to allow better performances.
\nt{TODO instead of language, use a more generic term to refer to language or infrastructure}
\nt{TODO justification and examples. What are modular application, or parallel applications ?}


\subsubsection{Technological shift}

Between the early development, and the maturation of a web application, the development needs are radically different.
In its early development, a web application needs to quickly iterate over feedback from its users.
\textit{``Release early, release often''}, and \textit{``Fail fast''} are the punchlines of the web entrepreneurial community.
The development team quickly releases a Minimum Viable Product as to get these feedbacks.
The development reactivity is crucial.
The first reason of startup failures is the lack of market need\ftnt{https://www.cbinsights.com/blog/startup-failure-post-mortem/}.
Therefore, the development team opt for a popular, and accessible language.

As the application matures and its audience grows, the focus shift from the development speed to the scalability of the application.
% The ability of the application to handle a large amount of simultaneous request.
The development team shift from a modular language, to a language providing parallelism.


This shift brings two problems.
First, the development team needs to take a risk to be able to grow the application.
This risk usually implies for the development team to rewrite the code base to adapt it to a completely different paradigm, with imposed interfaces.
It is hard for the development team to find the time, hence the money, or the competences to deploy this new paradigm.
Indeed, the number two and three reasons for startup failures are running out of cash, and missing the right competences.
Second, after this shift the development pace is different.
Parallel languages are incompatible with the commonly learned design principles.
The development team cannot react as quickly to user feedbacks as with the first paradigm.

% There is a performance problem with languages about concurrency.
% The used imperative programming languages are difficulty parallel.
% And the highly parallel programming languages are not largely used.

% There is a technological rupture between the two.

This technological rupture proves that there is economically a need for a a more sustainable solution to follow the evolution of a web application.
A paradigm that it is easy to develop with, as needed in the beginning of a web application development, and yet scalable, so as to be highly concurrent when the application matures.

% Such a paradigm would allow to develop with a global memory, so as to follow the best practices of software development, and be able to isolate the state of each concurrent tasks to parallelize them.

\endinput









