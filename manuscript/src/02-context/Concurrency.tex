\section{Concurrency}

Javascript, like most programming languages, is synchronous and non-concurrent. The specification doesn't provide any mechanism to write asynchronous nor concurrent execution of Javascript.
There is no reference to \texttt{setTimeout} nor \texttt{setInterval}.
These two well-known instructions to asynchronously post-pone execution are provided by the DOM.
Indeed, like for many languages, concurrency is supported and provided by the execution engine.
For example the JVM in the case of Java, or the operating system in the case of C/C++.
We explore in this section, the different model to provide concurrency.
\comment{when there is the need for concurrency in a system (so the components are somewhat dependent on each others : causal relation, or state sharing), there is a need for synchronization.
If there is no need for synchronization, then there is two independent systems.
So concurrency is first a topology and synchronization (communication?) problem.}
% TODO continue this introduction

\subsection{Two concurrency model}

We distinguish two types of concurrent models, tailored for two types of applications, CPU bound applications, and I/O bound applications.
An example of CPU bound application is a scientific application, like a physics simulation. Such applications rely heavily on the CPU to do calculation on a fixed set of data.
CPU bound applications need concurrency to augment the computing power.
On the other hand, an example of I/O bound application is graphical user interface.
% Since user interfaces evolved from a simple and sequential user prompt to a full interactive graphical space, the user interacts in a non-sequential way.
It needs to display and react to multiple concurrent streams of interaction.
Another example is a web server, it needs to react to multiple concurrent requests.
I/O bound applications need concurrency to keep track of concurrent control flows in multiple contexts.
The difference between the two needs is thin enough for one to be mistaken with the other.
The two well-known models for concurrency are threads and events.
It is know broadly admitted that threads are the better solution for CPU bound applications, while events are the better solution for I/O bound applications.
However, this distinction is not as clearly defined as it seems.
In the next section I explain what characterizes these two models, what are the differences between the two, and how should they be used.

% Particularly, with the massive introduction of parallel architecture, things got a lot more confusing.
% In the next sections I explain the concurrency model for one core, and for multi-core.

% TODO transition

% time-slicing is actually concurrency on a single core, in certain cases, its called en event-loop, in other cases, its called multi-threading.
% Exactly, event-loop is cooperative task management, while threads is preemptive task management
% And multi-threading is actually concurrency on multiple core, in certain cases, its called multi-threading, in other cases, its called message passing.
% Exactly, multi-threading is lock synchronization, while message passing is share nothing.

\subsubsection{Threads and events}

% TODO transition

% This section should present clearly what are threads and event, historically and currently.
% It should be perfectly clear that threads and events are perpendicular to one or multi-core architecture, and that it is also perpendicular to I/O or CPU bound applications.
% both events, and threads can be used on single, or multi-core architecture, for I/O or CPU bound applications.
% The final transition should be about what exactly drive the choice between all this : preemptive task management (for long concurrent computations) versus cooperative task management (for a lot of small concurrent computations).

Threads-based system and event-based system evolved significantly over the last half century.
These evolutions were fueled by the long-running debate about which design is better.
% We try to succinctly and roughly retrace theses evolutions to understand the positions of each community.
% This demonstration show that thread and events are two faces of the same reality.

\paragraph{Thread}

A thread is the smallest representation/embodiment/...? of the execution of a sequence of instructions on a computing core.
It is characterized by a set of registers on this computing core representing the state of the computation.
It is the smallest ... manipulable by a scheduler.




\paragraph{Event}


% TODO 


\paragraph{Thread and Event are not opposed}

% TODO 


Lauer and Needham \cite{Lauer1979} presented an equivalence between Procedure-oriented Systems and a Message-oriented Systems.

Adya \textit{et. al.} analyzed this debate and presented fives categories through which to present the problem \cite{Adya2002}.
These two categories were often associated with thread-based systems and event-based systems.
% TODO not very clear
Their advantages and drawbacks were mistaken with those of thread and events.
Adya \textit{et. al.} explain in details two of these categories that are most representative, Task management and Stack management.
We paraphrase these explanations.


\subsubsection{Time-slicing}


Time-slicing is the technique used by both threads and event based system when they can dispose only of one computing core to achieve concurrency.

\paragraph{Task management}

Consider a task as an encapsulation of part of the logic of a complete application.
All the task access the same shared state.
The Task management is the strategy chosen to arrange the task executions in available space and time.

Preemptive task management executes each task concurrently.
Their executions interleave on a single core, or overlap on multiple cores.
It allows to leverage the parallelism of modern architectures.
This parallelism has a cost however, developers are responsible for the synchronization of the shared memory.
While accessing a memory cell, it must be locked so that no other task can modify it.
Synchronization mechanism impose the developer to be especially aware of race condition, and deadlocks.
These synchronization problems make concurrency hard to program with preemptive task management.

The opposite approach, Serial task management, executes each task to completion before starting the next.
The exclusivity of execution assures an exclusive access on the memory.
Therefore, it removes the need for synchronization mechanism.
However, this approach is ill-fitted for modern applications, where concurrency is needed.

A compromise approach, Cooperative task management, allows tasks to yield voluntarily.
A task may yield to avoid monopolizing the core for too long.
Typically, it yields to avoid waiting on long I/O operations.
It merges the concurrency of the preemptive task management, and the exclusive memory access.
Thus, it relieves the developer from synchronization problems.
But at the cost of dropping parallel execution.

Threads are associated with preemptive task management, and events with Cooperative task management.
For this reason, it is commonly believed that synchronization mechanisms make threads hard to program \cite{Ousterhout1996}.
While it is really Preemptive task management that is responsible for these synchronization problems \cite{Adya2002}.

% TODO define the two
\paragraph{Stack management}

Consider a task is composed of several subtasks interleaved with I/O operations.
Each I/O operation signal its completion with an event.
The task stops at each I/O operation, and must wait the event to continue the execution.
The stack management is the strategy chosen to express the sequentiality of the subtasks.

The automatic stack management is what is mostly used in imperative programming.
The execution seems to wait the end of the operations to continue with the next instruction.
The call stack is kept intact.
This is what is commonly called synchronous programming.

In the manual stack management, developers need to manually register the handlers to continue the execution after the operation.
The execution immediately continues with the next instruction, without waiting the completion of the operation.
It implies to rip the call stack in two functions; one to initiate the operation, and another to retrieve the result.
This is what is commonly called asynchronously programming.


\subsubsection{Multi-Threading}

Multi-threading is the technique used both by threads and events-based system to achieve concurrency when there is multiple computation core.

  \paragraph{Synchronization}

  \paragraph{Eventual Consistency}

  \paragraph{Event-based multi-threading}
  % Transition to the next subsection : 
  % The event-loop makes use of the two concurrency model : multi-threads for I/O and time-slicing for Computation


\subsection{Event-loop}






\subsection{Turn-based programming}
When an event-loop is used as the concurrency model for a partially functional language like Javascript, it creates what Douglas Crockford called turn-based programming\ftnt{https://youtu.be/dkZFtimgAcM?t=1852}.
% TODO then explain why Javascript is a good choice for this ()


\subsubsection{Promises}

% Douglas Crowford on Promises
% https://www.youtube.com/watch?v=dkZFtimgAcM

% The web is polluted with dumb, simple Promises tutorial for Javascript
% It is hard to find the relevant information about how Promises change the programmation paradigm.

\comment{TODO}

\subsubsection{Generators}

\comment{Generators are a way to implement synchronous programming on top of an event-loop (asynchronous programming), like fibers}











A concurrency model well adapted for such applications is the threading model.
It allows to run multiple executions simultaneously to leverage the potential of parallel architectures, and execute faster.
Execution engine provide thread libraries to allow concurrency, like pthreads\ftnt{https://computing.llnl.gov/tutorials/pthreads/} for POSIX operating systems, and the \texttt{Thread}\ftnt{https://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html} class for Java.
But, as we will see in a next chapter, threads are known to be very difficult to manipulate.
It is lucky that Javascript was not seen as a language to build CPU intensive applications, so it remained free of this concurrency model, and can adopt a different concurrency model.
% TODO reformulate this sentence : Javascript was mainly used to build graphical interfaces, and not CPU intensive applications.











% \paragraph{Fibers} -> TODO state of the art








As said above, most implementations of Javascript feature an event-based programming paradigm.
The DOM\ftnt{http://www.w3.org/DOM/} and node.js\ftnt{https://nodejs.org/} are the most famous examples.
% TODO Place this accordingly

Now the browser has a thread-like concurrency model as well, on the form of web workers\ftnt{http://www.w3.org/TR/workers/}.
% TODO place this accordingly, it might be only a footnote

% \subsubsection{Event-loop}

% As event-loop was never used in CPU intensive applications, there was no need to extend to the multi-thread.

An event-loop answer a particular need of concurrency.
It is designed to use a thread of execution, to several 
% TODO spearate well the two notions : thread of execution -> physical, and logical thread of execution -> a suite of instructions to execute.

The event-loop concurrency model is based around a queue of message, named the event-queue, and a thread of execution to process these messages, named the event-loop.
The event-loop process the messages queued on the event-queue one after the other, each during a turn.
A turn consists of the event-loop pulling a message from the event-queue, and executing the callback associated with this message until it yield the execution when finished.
The execution of the callback is never preempted.

An event-loop is single-threaded, callbacks are executed one after the other on the same CPU.
The concurrency is time-sliced.
During the execution of a callback, there is no other simultaneous execution.
Therefor, each callback has an exclusive access to the memory.

Because each callback has exclusive access to the memory, and is never preempted, it can consider the invariant.
% TODO I don't know yet what exactly is an invariant, so please check.
The event-loop provide concurrency without the need for synchronization on the form of mutual exclusions, and locks.
It is often said that the event-loop provide atomic execution / de facto exclusion ... whatever, because of this pattern.

However, there is a limitation to this concurrency model.
Because all callback are executed on the same core, if one takes two much time to complete, the other can't executes, and the program seems to freeze.
Therefore, each turn needs to be executed fast, so as to avoid the event-queue to congest on events waiting for their turn.

GUIs hardly ever congest the event-loop, because if the programmer is not so bad, event are fast, and there is usually not so much event : the graphical space is often limited to the attention of a single user.
However, in the case of web applications, there is a lot much more events to process.
When there is too many concurrent events to process, the event-loop become congested, and the latency of the application increase.
% TODO therefore, it is interesting to split into multiple event-queue.
% TODO only make a point here : start to build the problematic.

% TODO continue about how to program in an event-loop, and why it is so interesting to have HOF.

HOF allows Continuation Passing Style, which is particularly useful in a turn-based programming language such as node.js javascript.
% TODO continue to say a lot about CPS
% CPS leads all the way to Promises \o/






% Web pages are graphical environment offering multiple area of interaction for the user.
% Because of this multiplicity, the traditional linear programing model doesn't hold anymore.
% Graphical systems switched from this linear programming model to a different programming model focused on events.

% Javascript uses higher-order functions.
% It is the ability for a language to manipulate functions like any other value.
% This ability is used to register a function to trigger after an event occurred.
% An event might be the click on an element of the page, for example.

% Such a function is named a callback, a handler, a listener ...
% And it shift the programming paradigm from synchronous to asynchronous, which is a big deal.

% In synchronous programming, the computation step are executed sequentially, one after the other.
% The program execution follows perfectly the program layout written in a linear textual file.

% On the other hand, asynchronous programming allows a step back from this linearity.


% A multi-threaded system allows the developer to explicitly express the parallelism in the application.
% A GOTO statement allows the developer to explicitly express the control flow in the application.


% Asynchronous programming allows the program to manage the concurrency of the execution.
% Unlike a linear layout of an imperative program, it allows to express more finely the dependencies between instructions.

% James Coglan speak about Promises and the abstraction they allow on the control flow. very interesting, especially at the end.
% https://blog.jcoglan.com/2013/03/30/callbacks-are-imperative-promises-are-functional-nodes-biggest-missed-opportunity/

% >>> I want to say that Javascript is the first language broadly used with this asynchronous paradigm.
% Is asynchronous programming a step before declarative programming ?









% http://berb.github.io/diploma-thesis/original/043_threadsevents.html

% On the Duality of Operating System Structures
% http://tolstenko.net/blog/dados/Unicamp/2010.1/mc714/extras/201_recomendada_Lauer-Needham-78-Duality.pdf

% Threads vs. Events
% http://courses.cs.vt.edu/cs5204/fall09-kafura/Presentations/Threads-VS-Events.pdf

% Why Threads Are A Bad Idea (for most purposes)
% http://www.cs.utah.edu/~regehr/research/ouster.pdf

% Why Events Are A Bad Idea (for high-concurrency servers)
% http://static.usenix.org/publications/library/proceedings/hotos03/tech/full_papers/vonbehren/vonbehren_html/

% http://www.usingcsp.com/cspbook.pdf

% Threads Without the Pain
% http://queue.acm.org/detail.cfm?id=1105678


% Cooperative Task Management without Manual Stack Management or, Event-driven Programming is Not the Opposite of Threaded Programming 
% http://static.usenix.org/publications/library/proceedings/usenix02/full_papers/adyahowell/adyahowell_html/

% Retrospective on SEDA
% http://matt-welsh.blogspot.fr/2010/07/retrospective-on-seda.html

% Latency
% http://highscalability.com/latency-everywhere-and-it-costs-you-sales-how-crush-it
% http://blog.gigaspaces.com/amazon-found-every-100ms-of-latency-cost-them-1-in-sales/




% \subsection{About concurrent systems}


% TODO advantage and disadvantages of synchronous and asynchronous programming

What we argue is that synchronous is good because it is linear, it avoids stack ripping.
But asynchronous is good because it allows parallelism by default.


% synchronous programming is good to express linear execution. In parallel that means quite unrelated linear executions.
% asynchronous programming is good to express interleaved parallel execution. It is easy to express small parallel and sequential tasks.


Threads are associated with the automatic stack management, and events with manual stack management.
For this reason, it is commonly believed that threads are easier to program.
\cite{Thread systems allow programmers to express control flow and encapsulate state in a more natural manner} \cite{Behren2003}.
However, the automatic stack management is not exclusive to threads.
Fibers, presented by Adya \textit{et. al.} is an example of cooperative task management with automatic stack management \cite{Adya2002} .
Fibers present the advantage of cooperative task management, without the disadvantage of stack ripping.
That is the ease of programming because of the absence of synchronization, without the difficulty of stack ripping.

We argue that the advantages of manual stack management outweigh its drawbacks for web services.
Because of the numerous I/O operations, parallelism is 


% TODO I split the explenation in thread then events, instead, I should split in task management, then state management




But what is actually highlighted is the automatic state management provided by threads.
And with lighter context change, threads are a good choice which provide parallelism.

Historically, events-based system are associated with manual state management, while threads-based systems are associated with automatic state management.
Manual state management imposed stack ripping \cite{Adya2002}.
With closure, it is not the case anymore.
Events now have automatic state management as well \cite{Krohn2007}.

Now, there is implementation of thread model with cooperative management, with context-switch overhead improved enough to fill the gap with events model.
And there is implementation of event model with automatic state management filling the gap with thread model.
In this condition, we ask, what really is the difference between thread and events.
We argue there is none.
Except the isolation, versus sharing of the memory, which, again is not significant of either.
In the first case, the different execution threads exchange messages, while in the second, they use synchronization mechanism to assure invariants in their states % TODO disambiguation thread is a context for execution, what is a core of execution ?

For a single thread of execution, both model could avoid synchronization through cooperative task management, which assure invariants. % TODO disambiguation
Or avoid procedure slicing (if any) using synchronization.
These are the two ends of a design spectrum.
One end (cooperative task management) fits better for small processing with heavy use of shared resources.
While the other end (synchronization) fits better for long processing with small use of shared resources.
When one end of the design spectrum is used while the other should be used, one might expect unresponsiveness because of too heavy events, or performance fall due to interlocking.

Scalability is achieved through parallelism, which is itself achieved in our case (web servers) through cluster of commodity machines.
% TODO this links exactly to what I wrote on scalability, find a way to merge the two.

With distribution, this design spectrum gets a better contrast. % TODO that is really false, reformulate to get a correct articulation

The synchronization of distributed, shared resources is limited through the CAP theorem  \cite{Gilbert2002a}. % TODO needs to read this.
Partition tolerance is a requirement of a distributed system. %TODO define PA, and find reference http://codahale.com/you-cant-sacrifice-partition-tolerance/#ft2
One needs to choose good latency (availability) or consistency. % Define the CAP theorem, consistency and availability
The CAP theorem is generalized into a broader theroem about\cite{Gilbert2012} % TODO read, and merge within the argumentation (the paper seems really broad, be careful not to dive too deep)

The isolation of resources implies to split the architecture in different stages, like Ninja \cite{Gribble2001}, SEDA \cite{Welsh2000}, or Flash \cite{Pai1999}.
This splitting is difficult for the developer.
The splitting which is good for the machine, is not the same as the one good for the design in modules.

The two ends of this design spectrum presented map directly onto the two kinds of parallelism advocated for scalability.
That is pipeline parallelism, and data parallelism.

Pipeline parallelism is good for data locality, and important throughput. % TODO reference
But each stage adds an overhead in latency. % TODO reference

Data parallelism is good for latency, because one request is processed from beginning to the end without waiting in queues.
But it implies that the different machines share a common database.
Which is a shared resource, and is limited by the CAP theorem.

Both parallelism have advantages and drawbacks, and both could be combined, like in the SEDA architecture. % TODO find the exact reference that says : it is good to use both parallelism to scale
Ultimately, it would be possible to design a design spectrum to choose which kind of parallelism for a set of requirements.
But we leave this for future works.

Splitting an architecture in stages is a difficult process, which prevent future code refactoring, and module modifications.
We argue that the design for the technical architecture, and the design for the human minds should not be the same.
Threads belong to the mental model, the design granularity
Events belong to the execution model, the architecture granularity
  % TODO quote
  It is a mistake to attempt high concurrency without help from the compiler \cite{Behren2003}.
Through compilation, we want to transform an event-loop based program (cooperative task management, no synchronization) into a pipeline parallelism distributed system.
So, basically, we argue that it is possible to distribute one loop event onto multiple execution core.



% TODO Careful, you don't know fibers enough.
% And globally, there is much that you don't know.

% Il y à quelques années, la tendance chez les gros était de se concentrer sur la parallélisation par threads pour gagner en latence.
% Notamment avec Gigaspace.
% Donc en occultant complètement les events, et en utilisant des grosses bases de données comme Dynamo ou Cassandra qui scalent bien.
% Je n'ai pas vraiment trouvé la même tendance pour les events.
% Storm et Spark stream sont à usage plus spécifique.
% Mais ce déséquilibre est peut être simplement dû à l'héritage de l'architecture n-tier.


% MICROSERVICES
% Plus récemment, j'ai trouvé beaucoup de bruit sur les microservices en 2014.
% Ça me paraissait intéressant, en pensant y trouver des arguments pour favoriser les events, et nuancer la tendance précédente.
% Mais j'ai l'impression qu'il s'agit plus d'une question d'organisation humaine que de performance.
% Il me reste à trouver des arguments pour mettre à égalité les deux axes de parallélisation.





% TODO 
% Continue with the hybrid approach : multiple threads for events, and one threads for user code
% See Flash, SEDA, Node.js etc ...






/!\ WARNING
The paper Why events are a bad idea states that :
the control flow patterns used by these applications fell into three simple categories: call/return, parallel calls, and pipelines.
Indeed, it is no coincidence that common event patterns map cleanly onto the call/return mechanism of threads. Robust systems need acknowledgements for error handling, for storage deallocation, and for cleanup; thus, they need a “return” even in the event model.
>> Why is it completly false ?
	 It is crucial to find an answer.
Moreover, Ayda et. al. state that :
For the classes of applications we reference here [file servers and web servers], processing is often partitioned into stages.
Other system designers advocated non-threaded programming models because they observe that for a certain class of high-performance systems [...] substantial performance improvements can be optained by reducing context switching and carefully implementing application-specific cache-conscious task scheduling.


The paper Why events are a bad idea states that :
One could argue that instead of switching to thread systems, we should build tools or languages that address the problems with event systems (i.e., reply matching, live state management, and shared state management). However, such tools would effectively duplicate the syntax and run-time behavior of threads.
>> Well, yes ...
   With the exception of the stack junction.
   The paper on Duality had it right, their graph is correct, but for threads, it cannot be distributed because of stacks, while for events, it can.


Software evolution substantially magnifies the problem of function ripping: when a function evolves from being compute-only to potentially yielding, all functions, along every path from the function whose concurrency semantics have changed to the root of the call graph may potentially have to be ripped in two. (More precisely, all functions up a branch of the call graph will have to be ripped until a function is encountered that already makes its call in continuation-passing form.) We call this phenomenon ``stack ripping'' and see it as the primary drawback to manual stack management. Note that, as with all global evolutions, functions on the call graph may be maintained by different parties, making the change difficult. 
>> Stack ripping is what I am talking about.
   While the stack are joined, it is not possible to distribute.
   If they say that stack ripping is necessary, that means it is not possible to encapsulate asynchronous function into synchronous function.